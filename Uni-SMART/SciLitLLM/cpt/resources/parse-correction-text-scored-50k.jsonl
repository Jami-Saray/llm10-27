{"text": "of images before and after data augmentation. The techniques in- troduced here are also applicable for other online meth- ods [28, 51, 61]. Non-saturating loss. For classi\ufb01cation tasks, the loss functionLis usually de\ufb01ned as the cross-entropy loss, L(f\u0012(a (x)) =PK k=1\u0000yklogf\u0012(a (x))k, wherey2 f0;1gKandKdenote the one-hot ground truth label and the number of classes, respectively. In this case, the gra- dient of the \ufb01rst term in Eq. (1) often saturates in the maximization problem with respect to when the tar- get model\u2019s predictions are very con\ufb01dent. Thus, we usePK k=1yklog(1\u0000f\u0012(a (x))k)when updating the augmen- tation model rather thanPK k=1\u0000yklogf\u0012(a (x))k. This technique has been used in GANs [14]. The non-saturating loss is a key factor for TeachAug- ment; it improves error rates of WideResNet-28-10 [59] on CIFAR-100 [27] from 18.7% to 17.4% (a baseline\u2019s error rate is 18.4%). Thus, we basically use the non-saturating loss for updating augmentation models in our experiments. Experience replay. In reinforcement learning, experi- ence replay [32, 39] stores actions chosen by the actor in the past and reuses them to update the critic. We apply this technique to our method by storing the augmentation mod- els and prioritizing them in a manner similar to prioritized experience replay [46]. Then, the target network is updated using the augmentation model randomly sampled from the buffer following their priorities. Letpibe a priority of the i-th stored augmentation model. We compute the priority as pi= S\u0000i, where Sdenotes the number of augmentation models stored in the buffer, denotes a decay rate, and we set to 0.99. In our experiments, we stored the augmentation model ev- erynbuffer epochs. For image classi\ufb01cation on CIFAR-10, CIFAR-100, and semantic segmentation, nbuffer was set to 10, and for other tasks and datasets, it was set to 1. Label smoothing. Label smoothing is a technique that replaces the one-hot labels with ^yk= (1\u0000\u000f)yk+\u000f=K, where\u000f2[0;1)is a smoothing parameter. For our method, the label smoothing prevents gradients from ex- ploding when the target model\u2019s predictions are very con- \ufb01dent under the non-saturating loss. In particular, such a situation tends to occur for easy tasks or strong target mod- els. To mitigate exploding gradients, we used the smoothed labels for the \ufb01rst term in Eq. (1) when updating the aug- mentation model. Note that for a fair comparison, we didnot apply it when updating the target model. Color regularization. In practice, the color augmenta- tion model tends to transform the pixel colors outside the color distribution of the training data. As a result, the aug- mented images will be out-of-distribution data, which may hurt the recognition accuracy for the in-distribution sam- ples. To align the color distributions before and after aug- mentation, we regularized the color augmentation model by introducing sliced Wasserstein distance (SWD) [3] between pixel colors before and after the augmentation. SWD is a variant of Wasserstein distance that represents a distance between two distributions. We de\ufb01ne the color regularization term as follows: Lcolor(fxbgB b;f~xbgB b) =X iSWD(fxb igB b;f~xb igB b);(2) wherefxb igB bdenotes a set of i-th pixels of images in a mini-batch with a batch size of B, andf~xbgB bdenotes color- augmented images de\ufb01ned in Eq. (4). Because costs of computing SWD at each pixel position depends linearly on the image resolution, we can compute SWD for high- resolution images handled in semantic segmentation with low computational resources. Then, in the stochastic gradi- ent descent, the gradient with respect to in each iteration is represented as follows: @ @ 1 BBX b\u0002 L(f\u0012(a (xb)))\u0000L(f~\u0012(a (xb)))\u0003 \u0000\u0015Lcolor(fxbgB b;f~xbgB b);(3) where\u0015is a hyperparameter controlling the effect of the regularization that was set to 10 in our experiments. 4. Data augmentation using neural networks We propose data augmentation using neural networks pa- rameterized by , which consists of two models, a color augmentation model c cand a geometric augmentation modelg g. Thus,a is de\ufb01ned as the composite function ofc candg g,a =g g\u000ec c, and the parameter is a set of cand g, =f c; gg. Our augmentation model en- ables updating of its parameters by the gradient method and construction of the search space with only two functions. The augmentation procedure is illustrated in Fig. 3. Given an image x2RM\u00023, whereMdenotes the num- ber of pixels and 3 corresponds to the RGB color chan- nels, the color augmentation is applied with probability of pc2(0;1), and then the geometric augmentation is applied with probability of pg2(0;1). The color augmentation is de\ufb01ned as follows: ~xi=t( i xi+ i);( i; i) =c c(xi;z;c);(4) 4+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Geometric Augmenta/onColor Augmenta/on<latexit sha1_base64=\"qYeqgjfWvBJbxLa8LNQtyCdEv7E=\">AAACZnichVHLSsNAFD2Nr1q1rYoouCmWiqsykaLiqujGZR/2AbWUJI41NE1CkhZq8QcEt3bhSkFE/Aw3/oCL/oHisoIbF96mAdGi3mFmzpy5586ZGdnUVNthrOsTRkbHxif8k4Gp6ZlgKDw7l7eNhqXwnGJohlWUJZtrqs5zjupovGhaXKrLGi/Itd3+fqHJLVs19H2nZfJyXarq6pGqSA5RWbOiVMJRFmduRIaB6IEovEgZ4Vsc4BAGFDRQB4cOh7AGCTa1EkQwmMSV0SbOIqS6+xynCJC2QVmcMiRiazRWaVXyWJ3W/Zq2q1boFI26RcoIYuyJ3bEee2T37IV9/Fqr7dboe2nRLA+03KyEzpay7/+q6jQ7OP5S/enZwRG2XK8qeTddpn8LZaBvnnR62e1MrL3Krtkr+b9iXfZAN9Cbb8pNmmcuEaAPEH8+9zDIr8fFjXginYgmd7yv8GMZK1ij995EEntIIUfnVnGOC3R8z0JQWBAWB6mCz9PM41sIkU+5WYrP</latexit>pc <latexit sha1_base64=\"xChvi4QrwUortMDYt+qHa2V4lx4=\">AAACaHichVG7SgNBFD1Z3/GRqIWKTUhQbAyzElSsRBvLRM0DooTddYxD9sXuJhCDP2BjqWKlICJ+ho0/YJFPUMsINhbebBZERb3DzJw5c8+dMzOqrQvXY6wZkrq6e3r7+gfCg0PDI5Ho6FjOtaqOxrOapVtOQVVcrguTZz3h6bxgO1wxVJ3n1cp6ez9f444rLHPbq9t811DKptgXmuIRlZXn7ZJWiiZYkvkR+wnkACQQRNqK3mAHe7CgoQoDHCY8wjoUuNSKkMFgE7eLBnEOIeHvcxwhTNoqZXHKUIit0FimVTFgTVq3a7q+WqNTdOoOKWOYYY/slrXYA7tjT+z911oNv0bbS51mtaPldilyPLn19q/KoNnDwafqT88e9rHsexXk3faZ9i20jr52eNraWtmcacyyK/ZC/i9Zk93TDczaq3ad4ZsXCNMHyN+f+yfILSTlxWQqk0qsrgVf0Y9pxDFH772EVWwgjSydK3CCM5yHnqWoNCFNdVKlUKAZx5eQ4h/BfYtB</latexit>1\u0000pc<latexit sha1_base64=\"MU1xgK+yXkCy+0ehIZ0q2VW8AJM=\">AAACZnichVHLSsNAFD2Nr1q1rYoouCmWiqsykaLiqujGZR/2AbWUJI41NE1CkhZq8QcEt3bhSkFE/Aw3/oCL/oHisoIbF96mAdGi3mFmzpy5586ZGdnUVNthrOsTRkbHxif8k4Gp6ZlgKDw7l7eNhqXwnGJohlWUJZtrqs5zjupovGhaXKrLGi/Itd3+fqHJLVs19H2nZfJyXarq6pGqSA5RWbNSrYSjLM7ciAwD0QNReJEywrc4wCEMKGigDg4dDmENEmxqJYhgMIkro02cRUh19zlOESBtg7I4ZUjE1mis0qrksTqt+zVtV63QKRp1i5QRxNgTu2M99sju2Qv7+LVW263R99KiWR5ouVkJnS1l3/9V1Wl2cPyl+tOzgyNsuV5V8m66TP8WykDfPOn0stuZWHuVXbNX8n/FuuyBbqA335SbNM9cIkAfIP587mGQX4+LG/FEOhFN7nhf4ccyVrBG772JJPaQQo7OreIcF+j4noWgsCAsDlIFn6eZx7cQIp/BWYrT</latexit>pg Iden/ty<latexit sha1_base64=\"PYTO1lb9BGUL3dcMuXBUkmrHS0A=\">AAACaHichVG7SgNBFD1ZXzG+ohYqNpKg2BhmJahYBW0s8zAmECXsrmMc3Be7m0AM/oCNpYqVgoj4GTb+gEU+QS0j2Fh4s1kQFfUOM3PmzD13zsyoti5cj7FmSOrq7untC/dHBgaHhkeio2NbrlV1NJ7XLN1yiqricl2YPO8JT+dF2+GKoeq8oB6st/cLNe64wjI3vbrNdwylYoo9oSkeUXl5wS5XytE4SzA/Zn4COQBxBJG2ojfYxi4saKjCAIcJj7AOBS61EmQw2MTtoEGcQ0j4+xxHiJC2SlmcMhRiD2is0KoUsCat2zVdX63RKTp1h5QzmGWP7Ja12AO7Y0/s/ddaDb9G20udZrWj5XZ55Hgy9/avyqDZw/6n6k/PHvaw4nsV5N32mfYttI6+dnjayq1mZxtz7Iq9kP9L1mT3dAOz9qpdZ3j2AhH6APn7c/8EW4sJeSmRzCTjqbXgK8KYRgzz9N7LSGEDaeTpXIETnOE89CxFpQlpqpMqhQLNOL6EFPsAyX2LRQ==</latexit>1\u0000pg <latexit sha1_base64=\"MU1xgK+yXkCy+0ehIZ0q2VW8AJM=\">AAACZnichVHLSsNAFD2Nr1q1rYoouCmWiqsykaLiqujGZR/2AbWUJI41NE1CkhZq8QcEt3bhSkFE/Aw3/oCL/oHisoIbF96mAdGi3mFmzpy5586ZGdnUVNthrOsTRkbHxif8k4Gp6ZlgKDw7l7eNhqXwnGJohlWUJZtrqs5zjupovGhaXKrLGi/Itd3+fqHJLVs19H2nZfJyXarq6pGqSA5RWbNSrYSjLM7ciAwD0QNReJEywrc4wCEMKGigDg4dDmENEmxqJYhgMIkro02cRUh19zlOESBtg7I4ZUjE1mis0qrksTqt+zVtV63QKRp1i5QRxNgTu2M99sju2Qv7+LVW263R99KiWR5ouVkJnS1l3/9V1Wl2cPyl+tOzgyNsuV5V8m66TP8WykDfPOn0stuZWHuVXbNX8n/FuuyBbqA335SbNM9cIkAfIP587mGQX4+LG/FEOhFN7nhf4ccyVrBG772JJPaQQo7OreIcF+j4noWgsCAsDlIFn6eZx7cQIp/BWYrT</latexit>pg <latexit sha1_base64=\"PYTO1lb9BGUL3dcMuXBUkmrHS0A=\">AAACaHichVG7SgNBFD1ZXzG+ohYqNpKg2BhmJahYBW0s8zAmECXsrmMc3Be7m0AM/oCNpYqVgoj4GTb+gEU+QS0j2Fh4s1kQFfUOM3PmzD13zsyoti5cj7FmSOrq7untC/dHBgaHhkeio2NbrlV1NJ7XLN1yiqricl2YPO8JT+dF2+GKoeq8oB6st/cLNe64wjI3vbrNdwylYoo9oSkeUXl5wS5XytE4SzA/Zn4COQBxBJG2ojfYxi4saKjCAIcJj7AOBS61EmQw2MTtoEGcQ0j4+xxHiJC2SlmcMhRiD2is0KoUsCat2zVdX63RKTp1h5QzmGWP7Ja12AO7Y0/s/ddaDb9G20udZrWj5XZ55Hgy9/avyqDZw/6n6k/PHvaw4nsV5N32mfYttI6+dnjayq1mZxtz7Iq9kP9L1mT3dAOz9qpdZ3j2AhH6APn7c/8EW4sJeSmRzCTjqbXgK8KYRgzz9N7LSGEDaeTpXIETnOE89CxFpQlpqpMqhQLNOL6EFPsAyX2LRQ==</latexit>1\u0000pg<latexit sha1_base64=\"b8GfiUh/aXDTpi+7sr+jTIezZp8=\">AAACZHichVFNSwJBGH7cvswsLQmCICQxOskYUtFJ6tLRj/wAE9ndxlpcd5fdVTLpD9S16NCpICL6GV36Ax38A0F0NOjSodd1IUqqd5iZZ555n3eemZEMVbFsxjoeYWh4ZHTMO+6b8E9OBYLTM3lLb5gyz8m6qptFSbS4qmg8Zyu2youGycW6pPKCVNvq7Rea3LQUXduxWwYv18V9TakqsmgTlT6sBCMsxpwID4K4CyJwI6UHb7GLPeiQ0UAdHBpswipEWNRKiIPBIK6MNnEmIcXZ5ziGj7QNyuKUIRJbo3GfViWX1Wjdq2k5aplOUambpAwjyp7YHeuyR3bPXtjHr7XaTo2elxbNUl/LjUrgZC77/q+qTrONgy/Vn55tVLHueFXIu+EwvVvIfX3z6KKb3chE20vsmr2S/yvWYQ90A635Jt+keeYSPvqA+M/nHgT5lVh8NZZIJyLJTfcrvJjHIpbpvdeQxDZSyNG5HKc4w7nnWfALIWG2nyp4XE0I30JY+AT5OooB</latexit>x<latexit sha1_base64=\"jTj6MfMK2jTaMcaQze4iBXPJBOY=\">AAACanichVG7SgNBFD1ZXzG+ojaKjRgjVmFWRMUqaGPpKw/QEHbX0SzZF7uToC75ATsrwVQKIuJn2PgDFn6CaKdgY+HdzYJoUO8wM2fO3HPnzIzqGLonGHuMSR2dXd098d5EX//A4FByeCTv2TVX4znNNmy3qCoeN3SL54QuDF50XK6YqsELanU12C/UuevptrUtjhxeMpUDS9/XNUUQVditKMI/bJSTKZZhYUy2AzkCKUSxbievsYs92NBQgwkOC4KwAQUetR3IYHCIK8EnziWkh/scDSRIW6MsThkKsVUaD2i1E7EWrYOaXqjW6BSDukvKSaTZA7thr+ye3bIn9vFrLT+sEXg5olltablTHjoZ23r/V2XSLFD5Uv3pWWAfS6FXnbw7IRPcQmvp68dnr1vLm2l/hl2yZ/J/wR7ZHd3Aqr9pVxt8s4kEfYD887nbQX4uIy9k5jfmU9mV6CvimMAUZum9F5HFGtaRC92d4hzN2Is0Io1LE61UKRZpRvEtpOlPBH+Mzg==</latexit>\u02c6xIden/ty Iden/ty <latexit sha1_base64=\"9tNgfYSnk1KGBOHbuJlNrYcJHR0=\">AAACbXichVHLSgMxFD0dX7U+WhVBUEQsPlYllaLiqujGZX1Ui62UmZi2odOZYWZa0OIPuBZciIKCiPgZbvwBF36CuHCh4MaFt9MB0aLekOTk5J6bk0SzdOm4jD0GlLb2js6uYHeop7evPxwZGNxyzKrNRZqbumlnNNURujRE2pWuLjKWLdSKpottrbzS2N+uCduRprHp7ltit6IWDVmQXHWJ2uH5es4qyTw/zEeiLMa8mGgFcR9E4UfKjFwjhz2Y4KiiAgEDLmEdKhxqWcTBYBG3izpxNiHp7QscIkTaKmUJylCJLdNYpFXWZw1aN2o6nprTKTp1m5QTmGIP7Ia9snt2y57Yx6+16l6Nhpd9mrWmVlj58NHIxvu/qgrNLkpfqj89uyhg0fMqybvlMY1b8Ka+dnDyurG0PlWfZpfsmfxfsEd2Rzcwam/8ak2snyJEHxD/+dytYGsuFp+PJdYS0eSy/xVBjGISs/TeC0hiFSmk6VwDxzjDeeBFGVbGlPFmqhLwNUP4FsrMJ56Njfw=</latexit>c\u0000c<latexit sha1_base64=\"7QkVTqz1HWrQXUzHojfc6Qa3IVA=\">AAACbXichVHLSgMxFD0dX7U+WhVBUEQsVVclI0XFlejGpa+qqKXMjGkNTmeGmbRQS3/AteBCFBRExM9w4w+48BPEhQsFNy68Mx0QFfWGJCcn99ycJLpjCk8y9hBRmppbWtui7bGOzq7ueKKnd92zy67Bs4Zt2u6mrnncFBbPSiFNvum4XCvpJt/Q9xf8/Y0Kdz1hW2uy6vBcSStaoiAMTRK", "meta_data": {"split_id": 2}, "response": "This extract appears to be a technical paper on a machine learning topic, specifically data augmentation using neural networks. The content is dense and technical, with little to no relevance to primary school or grade school curriculum.\n\nThe text is filled with technical terms, mathematical equations, and specialized notation, making it challenging for non-experts to understand. The language is formal and concise, with no apparent attempt to simplify the concepts or provide explanations for non-technical readers.\n\nBased on this evaluation, I would give this extract a score of 0 out of 5. The content is too complex and specialized to be useful for teaching at primary school or grade school levels.\n\nEducational score: 0", "prediction": 0}
{"text": "transformation so that a more accurate histogram matching can be achieved. While EHM can be conducted with different strategies, we adopt the Sort-Matching algorithm [45] for its fast speed. Sort- Matching is based on the quicksort strategy [47], which is generally accepted as the fastest sort algorithm with com- plexity of O(nlogn). As stated by its name, Sort-Matching is implemented by matching two sorted vectors, whose in- dexes are illustrated in a one-line notation [2] as: x:\u03c4=\u0000\u03c41\u03c42\u03c43\u00b7\u00b7\u00b7\u03c4n\u0001 , y:\u03ba=\u0000 \u03ba1\u03ba2\u03ba3\u00b7\u00b7\u00b7\u03ban\u0001 ,(3) where {x\u03c4i}n i=1and{y\u03bai}n i=1are sorted values of xandy in ascending order. In other words, x\u03c41= min( x),x\u03c4n= max(x), and x\u03c4i\u2264x\u03c4jifi < j .y\u03baiis similarly defined. Based on the definition in Eq. (3), Sort-Matching outputs o with its \u03c4i-th element o\u03c4ias: o\u03c4i=y\u03bai. (4) Compared to AdaIN, HM and other EHM algorithms [5, 16], Sort-Matching additionally assumes that the two vec- tors to be matched are of the same size, i.e.m=n, which is satisfied in our focused applications of AST and DG. In other applications where the two vectors are of different sizes, interpolation or dropping elements can be conducted to make yandxthe same size. 3.2. EFDM for AST and DG In this section, we apply EFDM to tasks of AST and DG. We conduct the exact eCDFs matching by applying the EHM algorithm via Sort-Matching in the image fea- ture space. To enable the gradient back-propagation in deep models, we practically perform EFDM by modifying Eq. (4) as: EFDM (x,y) :o\u03c4i=x\u03c4i+y\u03bai\u2212 \u27e8x\u03c4i\u27e9, (5) where \u27e8\u00b7\u27e9represents the stop-gradient operation [4]. We stop the gradients to the style feature y\u03baifollowing [19,69]. Given the input data X\u2208RB\u00d7C\u00d7HWand the style data Y\u2208RB\u00d7C\u00d7HW, we apply EFDM in a channel-wise man- ner following [19, 69], where B, C, H, W indicate batch size, channel dimension, height, and width, respectively.The proposed EFDM does not introduce any parameters and can be used in a plug-and-play manner with few lines of codes and minimal cost, as summarized in Algorithm 1. Algorithm 1 PyTroch-like pseudo-code for EFDM. #x,y: input and target vectors of the same shape (n) , IndexX = torch.sort( x) # Sort xvalues SortedY , = torch.sort( y) # Sort yvalues InverseIndex = IndexX.argsort(-1) return x+ SortedY .gather(-1, InverseIndex) - x.detach() Input images 1st conv 1st block 2nd block 3rd block 4th block020406080100Percent of equivalent values ReLU PReLU Figure 3. An illustration of the percent of equivalent feature values (i.e.,number of equivalent values number of all values\u2217100) in ResNet18 feature maps with an input image of resolution 224\u00d7224 . \u20181st conv\u2019 represents the output of the first convolution layer. \u20181st block\u2019, \u20182nd block\u2019, \u20183rd block\u2019, and \u20184th block\u2019 indicate the outputs of the 1st, 2nd, 3rd, and 4th residual blocks, respectively. \u2018ReLU\u2019 and \u2018PReLU\u2019 indicate the vanilla ResNet18 with ReLU [40] and PReLU [17] activation functions, respectively. The percentage depends on the number of bits to represent the feature values and the size of feature maps. In the original image pixel space, the percentage is close to 100 percent since the pixels are quantized into 8-bits. The percentage decreases in the floating number (32-bits) feature space, as well as the depth of blocks since deeper blocks have smaller feature maps. In addition, compared to PReLU, there are generally more equivalent feature values for models with ReLU, since the ReLU function sets all negative values to zero. EFDM for AST. A simple encoder-decoder architecture is adopted, where we fix the encoder fas the first few lay- ers (up to relu41) of a pre-trained VGG-19 [49]. Given the content images Xand style images Y, we first encode them to the feature space and apply EFDM to get the style- transferred features as: S=EFDM (f(X), f(Y)). (6) Then, we train a randomly initialized decoder gto map S to the image space, resulting in the stylized images g(S). Following [8, 19], we train the decoder with the weighted combination of a content loss Lcand a style loss Ls, leading to the following objective: L=Lc+\u03c9Ls, (7) where \u03c9is a hyper-parameter balancing the two loss terms. Specifically, the content loss Lcis the Euclidean distance 4+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Visualization of high-order style statistics Figure 4. t-SNE [51] visualization of the third standardized moment-skewness [22, 58], which clearly shows that the style information can be represented by high-order statistics beyond mean and standard deviation. Besides skewness, style informa- tion can also be observed in the fourth standardized moment- kurtosis [22,57] and infinite norm (please refer to the supplemen- tary file for details). The visualized features are extracted from the 1st residual block of ResNet-18 [18] trained on the dataset of four domains [26]. between features of stylized images f(g(S))and the style- transferred features S: Lc=\u2225f(g(S))\u2212S\u22252. (8) The style loss measures the distribution divergence be- tween features of the stylized images g(S)and style im- agesY, which is instantiated as their divergence on mean and standard deviation in [19] based on the Gaussian prior. To measure the distribution divergence more exactly, we in- troduce the style loss as the sum of Euclidean distance be- tween features of the stylized images \u03d5i(g(S))and its style- transferred target EFDM (\u03d5i(g(S)), \u03d5i(Y)): Ls=XL i=1\u2225\u03d5i(g(S))\u2212EFDM (\u03d5i(g(S)), \u03d5i(Y))\u22252. (9) Following [19], we instantiate {\u03d5i}L i=1asrelu11,relu21, relu31, and relu41layers in VGG-19. EFDM for DG. Inspired by the studies that style informa- tion can be represented by the mean and standard deviation of image features [19, 31, 35], Zhou et al. [69] proposed to generate style-transferred and content-preserved feature augmentations for DG problems. As we discussed be- fore, distributions beyond Gaussian have high-order statis- tics other than mean and standard deviation, and hence the style information can be more accurately represented by us- ing high-order feature statistics. The visualization in Fig. 4 demonstrates that the third standardized moment-skewness [22,58] can well represent the four different domains of the same object. This motivates us to utilize high-order statis- tics for feature augmentations in DG. Since high-order feature statistics can be efficiently and implicitly matched via our proposed EFDM method, it is a natural idea to replace AdaIN with EFDM for cross- distribution feature augmentation in DG. To generate morediverse feature augmentations with mixed styles, follow- ing [69] we extend the EFDM in Eq. (5) by interpolating sorted vectors, resulting in the Exact Feature the latter one, which is generally faster and suitable for real-time applications. In both directions, transferring styles can be interpreted as a problem of feature distribution matching by assuming the image styles can be represented by feature distributions. Specifically, the seminal work in [10] adopts the second- order moments captured by the Gram matrix as the style representation. The loss introduced in [10] is rewritten as a Maximum Mean Discrepancy between image features in [32], bridging style transfer and feature distribution match-ing. Actually, many AST methods can be interpreted from the perspective of feature distribution matching. Based on the Gaussian prior assumption, feature distribution match- ing is conducted by matching mean and standard deviation in AdaIN [19]. Compared to AdaIN, WCT [31] additionally considers the covariance of feature channels via a pair of feature transforms, whitening and coloring. By additionally taking the content loss in [10] into the framework of WCT, a closed-form solution is presented in [30,35,39]. Besides the widely used first and second order feature statistics, high- order central moments and HM are introduced in [23] and [44] respectively for more exact distribution matching by relaxing the assumption of Gaussian feature distributions. However, computing high-order statistics explicitly intro- duces intensive computational overhead and the EFDM via HM is impeded by equivalent feature values. To this end, we, for the first time to our best knowledge, propose an ac- curate and efficient way for EFDM by exactly matching the eCDFs of image features, leading to more faithful AST re- sults (please refer to Fig. 5 for visual examples). Domain generalization (DG) aims to develop models that can generalize to unseen distributions. Typical DG meth- ods include learning domain-invariant feature representa- tions [13,29,38,63,64], meta-learning-based learning strate- gies [7,27], data augmentation [11,41,54,59,68,69] and so on [55, 66]. Among all above methods, the recent state- of-the-art [69] augments cross-distribution features based on the feature distribution matching technique [19], which is introduced in the above AST part. By utilizing high- order statistics implicitly via the proposed EFDM method, more diverse feature augmentations can be achieved and significant performance improvements have been observed (please refer to Tabs. 1 and 2 for details). Exact histogram matching (EHM) was proposed to match histograms of image pixels exactly. Compared to classical HM, EHM algorithms distinguish equivalent pixel values either randomly [45, 46] or according to their local mean [5, 16], leading to more accurate matching of histograms. The difference between outputs of EHM and HM in im- age pixel space is typically small, which is hardly percep- 2+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 0.00Input vector \ud835\udc99Target vector \ud835\udc9a0.100.000.200.10 0.100.000.400.300.50H(\ud835\udc65) = \" 0.10 \ud835\udc56\ud835\udc53 \ud835\udc65=0.00 0.40 \ud835\udc56\ud835\udc53 \ud835\udc65=0.10 0.50 \ud835\udc56\ud835\udc53 \ud835\udc65=0.20 Output vector \ud835\udc900.100.400.100.500.40Output vector \ud835\udc900.000.300.100.500.40Histogram Matching EHM via Sort-Matching\ud835\udc65!!\ud835\udc65!\"\ud835\udc65!#\ud835\udc65!$\ud835\udc65!%\ud835\udc66\"!\ud835\udc66\"\"\ud835\udc66\"#\ud835\udc66\"$\ud835\udc66\"%0.000.000.100.100.200.100.300.400.500.001325414352\ud835\udf0f!\ud835\udf0f\"\ud835\udf0f#\ud835\udf0f$\ud835\udf0f%\ud835\udf05!\ud835\udf05\"\ud835\udf05#\ud835\udf05$\ud835\udf05%o#=\ud835\udc66$o%=\ud835\udc66#o$=\ud835\udc66&o'=\ud835\udc66%o&=\ud835\udc66'\ud835\udf07\ud835\udf0e\ud835\udf07%1\ud835\udc3e\ud835\udc62\ud835\udc5f\ud835\udc61\u2113(\u22ef\ud835\udc90\ud835\udc9aEqual?0.260.26 \u2714 \u2714ValuesIndices0.210.21-0.11-0.110.970.970.500.50 \u2714 \u2714 \u2714\u22ef\u22ef\u22ef\ud835\udf07\ud835\udf0e\ud835\udf07%1\ud835\udc3e\ud835\udc62\ud835\udc5f\ud835\udc61\u2113(\u22ef\ud835\udc90\ud835\udc9aEqual?0.300.260.190.21-0.18-0.110.820.970.500.50\u2718\u22ef\u22ef\u22ef\u2718\u2718\u2718 \u2714Output vector \ud835\udc900.060.310.060.310.56Adaptive Instance Normalization\ud835\udf07\ud835\udf0e\ud835\udf07%1\ud835\udc3e\ud835\udc62\ud835\udc5f\ud835\udc61\u2113(\u22ef\ud835\udc90\ud835\udc9aEqual?0.260.260.210.210.25-0.111.180.970.560.50\u22ef\u22ef\u22ef\u2718\u2718 \u2714\ud835\udc90= \ud835\udc99\u2212\ud835\udf07 (\ud835\udc99)\ud835\udf0e (\ud835\udc99) \ud835\udf0e (\ud835\udc9a) + \ud835\udf07 (\ud835\udc9a) \u2714\u2718Figure 2. A comparison between AdaIN, HM and EHM via Sort-Matching using a toy example, where value precision is rounded to the level of 0.01. AdaIN only matches the mean and standard deviation between output vector oand target vector y. Although the eCDF ofois approximated to that of yby HM, they are not exactly matched, leading to the mismatched distributions and, consequently, the mismatched statistics. The EHM via Sort-Matching exactly matches the eCDFs of oandy, resulting in exactly matched distributions and, consequently, exactly matched statistics. Notations of \u00b5, \u03c3,f\u00b53,Kurt and\u2113\u221eindicate the mean, standard deviation, third standardized moment-skewness [22, 58], fourth standardized moment-kurtosis [22, 57], and infinite norm, respectively. tible to human eyes. However, this small difference can be amplified in the feature space of deep models, leading to clear divergence in feature distribution matching. We hence propose to perform EFDM by exactly matching the eCDFs of image features via EHM. While EHM can be conducted with different strategies, we empirically find that they yield similar results in our applications, and thus we promote the fast Sort-Matching [45] algorithm for EHM. 3. Methodology 3.1. AdaIN, HM and EHM Adaptive instance normalization (AdaIN) [19] transforms an input vector x\u2208Rn, which is sampled from a random variable X, into an output vector o\u2208Rn, whose mean and standard deviation match those of a target vector y\u2208Rm sampled from a random variable Y: o=x\u2212\u00b5(x) \u03c3(x)\u03c3(y) +\u00b5(y), (1) where \u00b5(\u00b7)and\u03c3(\u00b7)indicate the mean and standard devia- tion of referred data, respectively. By assuming that Xand Yfollow Gaussian distributions and nandmapproach in- finity, AdaIN can achieve EFDM by matching feature meanand standard deviation [30,35, 39]. However, feature distri- butions of real-world data usually deviate much from Gaus- sian, as can be seen from Fig. 1. Therefore, matching fea- ture distributions by AdaIN is less accurate. Histogram matching (HM) [14, 56] aims to transform an input vector xinto an output vector o, whose eCDF matches the target eCDF of a target vector y. The eCDFs of xand yare defined as: bFX(x) =1 nXn i=11xi\u2264x,bFY(y) =1 mXm i=11yi\u2264y, (2) where 1Ais the indicator of event Aandxi(oryi) is the i-th element of x(ory). For each element xiof the input vector x, we find the yjthat satisfies bFX(xi) =bFY(yj), resulting in the transformation function: H(xi) =yj. One may opt to match the explicit histograms as in discrete im- age space [14]. It is worth mentioning that matching eCDFs is equivalent to matching histograms with bins of infinites- imal width, which is however hard to achieve due to the finite number of bits to represent features. Ideally, HM could exactly match eCDFs of image fea- tures in the continuous case. Unfortunately, HM can only approximately match eCDFs when there exist equivalent feature values in inputs, since HM merges equivalent val- ues as a single point and applies a point-wise transforma- 3+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 tion (please refer to the toy example in Fig. 2). For fea- tures generated by deep models, equivalent feature values are common due to their dependency on discrete image pix- els and the use of activation functions, e.g., ReLU [40] and ReLU6 [24] (please refer to Fig. 3 for more details). All these facts impede the effectiveness of EFDM via HM. Exact Histogram Matching (EHM) [5, 16] was proposed to match histograms of image pixels exactly. Different from HM, EHM algorithms distinguish equivalent pixel val- ues and apply an element-wise Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization Yabin Zhang1Minghan Li1Ruihuang Li1Kui Jia2Lei Zhang1* 1Hong Kong Polytechnic University2South China University of Technology {csybzhang,csrhli,cslzhang }@comp.polyu.edu.hk, liminghan0330@gmail.com, kuijia@scut.edu.cn Abstract Arbitrary style transfer (AST) and domain generaliza- tion (DG) are important yet challenging visual learning tasks, which can be cast as a feature distribution match- ing problem. With the assumption of Gaussian feature dis- tribution, conventional feature distribution matching meth- ods usually match the mean and standard deviation of fea- tures. However, the feature distributions of real-world data are usually much more complicated than Gaussian, which cannot be accurately matched by using only the first-order and second-order statistics, while it is computationally pro- hibitive to use high-order statistics for distribution match- ing. In this work, we, for the first time to our best knowl- edge, propose to perform Exact Feature Distribution Match- ing (EFDM) by exactly matching the empirical Cumulative Distribution Functions (eCDFs) of image features, which could be implemented by applying the Exact Histogram Matching (EHM) in the image feature space. Particularly, a fast EHM algorithm, named Sort-Matching, is employed to perform EFDM in a plug-and-play manner with mini- mal cost. The effectiveness of our proposed EFDM method is verified on a variety of AST and DG tasks, demonstrat- ing new state-of-the-art results. Codes are available at https://github.com/YBZh/EFDM . 1. Introduction Distribution matching is a long-standing statistical learn- ing problem [37]. With the popularity of deep models [18, 25], matching the distribution of deep features has at- tracted growing interest for its effectiveness in solving com- plex vision tasks. For instance, in arbitrary style transfer (AST) [10, 19], image styles can be interpreted as feature distributions and style transfer can be achieved by cross- distribution feature matching [23, 32]. Furthermore, by us- ing style transfer techniques to augment training data, one can address the domain generalization (DG) tasks [11, 69], *Corresponding authorwhich target at generalizing the models learned in some source domains to other unseen domains. The most popular method of feature distribution matching is to match feature mean and standard deviation by assuming that features fol- low Gaussian distribution [19,30,35,39,69]. Unfortunately, the feature distributions of real-world data are usually too complicated to be modeled by Gaussian, as illustrated in Fig. 1. Therefore, feature distribution matching by using only mean and standard deviation is less accurate. It is de- sired to find more effective methods for more accurate and even Exact Feature Distribution Matching (EFDM). Intuitively, EFDM can be done by matching the high- order statistics of features. Actually, high-order central mo- ments have been explicitly introduced in [23, 61] to match distributions more precisely. However, considering high- order statistics in this way would introduce intensive com- putational overhead. Furthermore, the EFDM could only be theoretically achieved by matching central moments of infinite order [61], which is prohibitive in practice. Moti- vated by the Glivenko\u2013Cantelli theorem [52], which states that the empirical Cumulative Distribution Function (eCDF) asymptotically converges to the Cumulative Distribution Function when the number of samples approaches infinity, Risser et al. [44] introduce the classical Histogram Match- ing (HM) [14, 56] method as an auxiliary measurement to minimize the feature distribution divergence. Unfortu- nately, HM can only approximately match eCDFs when there are equivalent feature values in inputs, since HM merges equivalent values as a single point and applies a point-wise transformation. (A toy example is illustrated in Fig. 2). This commonly happens for digital images with dis- crete integer values ( e.g., 8-bits digital images). For features generated by deep models, equivalent feature values are also ineluctable due to their dependency on discrete image pix- els and the use of activation functions, e.g., ReLU [40] and ReLU6 [24] (please refer to Fig. 3 for more details). All these facts impede the effectiveness of EFDM via HM. To solve the above mentioned problem, we, for the first time to our best knowledge, propose to perform EFDM by exactly matching the eCDFs of image features, resulting 1+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 3 2 1 0 1 2 3 4 5 6 Values02468Numbers1e5(a) Art painting 3 2 1 0 1 2 3 4 5 6 Values02468Numbers1e5 (b) Cartoon 3 2 1 0 1 2 3 4 5 6 Values012345Numbers1e5 (c) Photo 3 2 1 0 1 2 3 4 5 6 Values0.00.51.01.52.02.53.03.5Numbers1e6 (d) Sketch Figure 1. Histograms of feature values in a randomly selected channel, where features are computed from the first residual block of a ResNet-18 [18] trained on the dataset of four domains [26]. We first normalize the mean and standard deviation of each channel to be 0 and1, respectively, and then collect feature values among all test samples in each domain for visualization. One can clearly see that the feature distributions of real-world data are usually too complicated to be modeled by Gaussian. in exactly matched feature distributions (when the number of samples approaches infinity) and consequently exactly matched mean, standard deviation, and high-order statis- tics (see the toy example in Fig. 2). The exact matching of eCDFs can be implemented by applying the Exact His- togram Matching (EHM) algorithm [5, 16] in the feature space. Specifically, by distinguishing the equivalent fea- ture values and applying an element-wise transformation, EHM conducts more fine-grained and more accurate match- ing of eCDFs than HM. In this paper, a fast EHM algorithm, named Sort-Matching [45], is adopted to perform EFDM in a plug-and-play manner with minimal cost. With EFDM, we conduct cross-distribution feature matching in one shot (cf. Eq. (6)) and propose a new style loss (cf. Eq. (9)) to more accurately measure distribution divergence, producing more stable style-transfer images in AST. Following [69], we extend EFDM to generate feature augmentations with mixed styles, leading to the Exact Fea- ture Distribution Mixing (EFDMix) (cf. Eq. (10)), which can provide more diverse feature augmentations for DG ap- plications. Our method achieves new state-of-the-arts on a variety of AST and DG tasks with high efficiency. 2. Related Work Arbitrary style transfer (AST) has been investigated in two conceptual directions: iterative optimization-based methods and feed-forward methods. The former [10,23,44] optimize image pixels in an iterative manner, whereas the latter [19,30,31,35,39] generate style-transferred output in one shot. Our method belongs to Distribution Mixing (EFDMix) as: EFDMix (x,y) :o\u03c4i=x\u03c4i+ (1\u2212\u03bb)y\u03bai\u2212(1\u2212\u03bb)\u27e8x\u03c4i\u27e9. (10) The instance-wise mixing weight \u03bbis adopted and we sam- ple\u03bbfrom the Beta-distribution: \u03bb\u223cBeta (\u03b1, \u03b1), where \u03b1\u2208(0,\u221e)is a hyper-parameter. We set \u03b1= 0.1un- less otherwise specified. Obviously, EFDMix degenerates to EFDM when \u03bb= 0. Given the input feature X, following [69] we adopt two strategies to mix with the style feature Y. When domain la- bels are given, we sample Yfrom a domain different from that of X, leading to EFDMix w/ domain label. Otherwise, Yis obtained by shuffling Xalong the batch dimension, re- sulting in EFDMix w/ random shuffle. We train the model solely with the cross-entropy loss. Following [69], we insert the EFDMix module to multiple lower-level layers, adopt a probability of 0.5to decide whether the EFDMix is acti- vated in the forward pass of training stage, and deactivate it in the testing stage. The advantage of utilizing high-order feature statistics could be intuitively clarified by the augmentation diversity. For example, given two different style features byandeywith the same mean and standard deviation and a specific mix- ing weight \u03bb, the same augmented feature will be obtained by only utilizing the mean and standard deviation [69]. On the contrary, our EFDMix could generate two different aug- mentations by implicitly utilizing high-order statistics, re- sulting in more diverse feature augmentations. 4. Experiments We perform experiments on AST and DG tasks to vali- date the effectiveness of EFDM. 4.1. Experiments on AST We closely follow [19] to conduct the experiments1on AST. Specifically, we adopt the adam optimizer, set the batch size as 8content-style image pairs, and set the hyper- parameter \u03c9= 10 . In training, the MS-COCO [33] and WikiArt [42] are adopted as the content and style images, respectively. We compare EFDM with state-of-the-arts in Fig. 5. One can see that our EFDM works stably across the style transfer (top two rows) and the more challeng- ing photo-realistic style transfer (bottom two rows) tasks. By conducting feature distribution matching more exactly, it preserves more faithfully the image structures and de- tails while transferring the style, and produces more photo- realistic results. In contrary, the competing methods may introduce many visual artifacts and image distortions. More visual results can be found in the supplementary file . 1https://github.com/naoto0804/pytorch-AdaIN 5+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 ContentimagesStyle imagesAdaINEFDM (ours)HM Gatys CMD (a) (b)Figure 5. Illustration of results on (a) style transfer [19] (top two rows) and (b) the more challenging photo-realistic style transfer [36] (bottom", "meta_data": {"split_id": 2}, "response": "This extract is relevant to educational topics, particularly in the fields of computer science, machine learning, and data analysis. However, it is not directly applicable to primary school or grade school levels, as it involves advanced concepts and terminology that may be challenging for younger students to understand.\n\nThe extract has some elements pertinent to education, such as the presentation of a research paper and the discussion of various algorithms and techniques. However, it does not align closely with educational standards, and the language and concepts used may be too complex for younger students.\n\nI would award a score of 2 out of 5, as the extract provides some basic information relevant to educational topics but is not directly applicable to primary school or grade school levels.\n\nEducational score: 2", "prediction": 2}
{"text": "developed in our open-set setting can be flexibly ap- plied to unknown objects without extra training with only a few labeled RGBD images, no matter they are within the trained categories or not. 2.2. Possible Few-Shot Pose Estimation Solutions Local Image Feature Matching. Local feature match- ing can establish the correspondence between two images for the few-shot pose estimation problem. Existing meth- ods can be categorized into detector-based [36\u201338, 46, 47] and detector-free [31, 34, 45, 50]. While these algorithms only leverage the grey-scale images, the performance drops on texture-less objects. Instead, we fully leverage both the appearance and the geometric information and generalize well in more scenarios. Point Cloud Registration. One line of point cloud reg- istration algorithms solve the problem by detecting 3D key- points [1, 30], extracting feature descriptors [7, 8, 14, 22, 43, 43] and estimating the relative transformation. Several end- to-end approaches [61] are also proposed. However, these algorithms heavily rely on fine point clouds and fail on ob- jects that are not captured by depth sensors, i.e., reflective ones. Instead, we fully leverage the complementary infor- mation in RGBD images for dense prototypes extraction and matching to retrieve better object pose parameters. 2.3. Metric learning on few-shot learning problems Metric learning techniques have been applied to sev- eral few-shot learning problems, including classification [13,48,56] and segmentation [10,35,53,65]. The represen- tative prototypical network [48] for classification map the support and query images into a global embedding space and then retrieve the class label of query image based on the support embedding, named prototype. The recent metric learning-based approaches in more challenging segmenta- tion areas utilize similar technique but output per-pixel pre- diction on the query images by matching per-pixel query features with global average prototypes [10, 53, 69] or part- level prototypes [35, 65]. While sparse support prototypes are enough to solve the above problems, few-shot pose esti- mation requires more dense correspondence exploration on pixel-level support prototypes and query features, which is more challenging.+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 3. Proposed Method 3.1. Problem Formulation We introduce the problem setting of the few-shot 6D ob- ject pose estimation and the derived domain generalization problem. The few-shot 6D object pose estimation. We formu- late the new open-set task, the few-shot 6D pose estima- tion as follows. Given ksupport RGBD patches P= {p1, p2, ..., p k}of a novel object with pose parameters as support frames, the inference task is to retrieve the 6D pose parameters of that novel object in the query novel scene im- ageI. Compared to current close-set setting, the proposed open-set one eliminates the reliance on precise CAD mod- els and focuses on the generalizability of trained models on unseen objects. Specifically, once the model is trained, we expect to apply it on novel scenes of novel objects by a few views without extra training. It bridges the gap between machine learning algorithms and flexible human visual sys- tems. Moreover, it enables real-world applications, i.e., fast registration of new objects for robotic manipulation and ser- vice home robots. The generalization requirement of the open-set problem also derive another interesting research question to study: Domain generalization. The domain generalization tar- gets to reduce the domain gaps between models trained on the synthesis and real-world data. It has been introduced to the 6D pose estimation field to deal with the lack of data [29,39,49,58]. However, this field is less explored as exist- ing real-world benchmarking datasets [23,24] for the close- setproblem has been well established: real-world training data for the object to be estimated is available. While exist- ing datasets are small with limited objects, in our few-shot open-set setting, diversity of shape and appearance are cru- cial to the generalizability of few-shot 6D object pose esti- mation algorithms. However, capturing and labeling such a large-scale real-world dataset is not practical due to the high cost (money and time). It is crucial to fully leverage the ge- ometry and appearance diversity in our large-scale photo- realistic datasets and generalize to the real world. The do- main generalization problem is thus an important problem to study for the few-shot 6D pose estimation. 3.2. Datasets The prior learned from large-scale datasets is crucial to the performance and generalizability of few-shot learning algorithms. ImageNet [9], for example, has been widely used for network pre-training in several few-shot learning tasks, i.e., object detection and segmentation. While 2D vi- sion tasks rely more on the semantic prior in RGB images, for the few-shot 6D object pose estimation, both shape and semantic prior are crucial for the generalizability of the net- work. However, existing datasets [23, 24, 62] for 6D ob-Dataset Modality NcatNobj Nimg LineMOD [20] RGBD - 15 18,273 YCB-V [4] RGBD - 21 133,936 TLESS [23] RGBD - 30 47,664 NOCS-REAL [59] RGBD 6 42 80,000 NOCS-CAMERA [59] RGBD 6 1,085 300,000 ShapeNet6D RGBD 51 12,490 800,000 Table 1. Statistics of Different Datasets. ShapeNet6D is diverse in shape and appearance, which is crucial to the generalizability of few-shot 6D pose algorithms. Ncat: number of category; Nobj: number of object instance. Nimg: number of images. ject pose estimation are small and lack diversity in shape and appearance to provide enough prior for the generaliza- tion capability. Therefore, we keep their role as real-world benchmark datasets and propose a new large-scale dataset, ShapeNet6D, with diverse shapes and appearances for prior learning. 3.2.1 ShapeNet6D The proposed ShapeNet6D is a large-scale photorealistic dataset containing RGBD scene images of more than 12K object instances from the ShapeNet [5] repository. Each scene image is labeled with ground truth information for the 6D pose estimation problem, including instance seman- tic segmentation and pose parameters of each object. As we demonstrate empirically, the diversity of shape and ap- pearance is crucial for the network to generalize. While it is not practical to collect and label such a large-scale, di- verse dataset in the real world due to the high cost (time and money), we instead generate photorealistic images by physically-based rendering. Our approach is inspired by the successful application of photorealistic datasets in [24, 67, 68] while improving the diversity of object shape and appearance. Specifically, we utilize the physically- based rendering engine, Blender1that simulates the flow of light energy by ray tracing to render realistic scene im- ages. To arrange a scene to render, we first randomly select several objects from ShapeNet, apply random material and texture, and drop them into a box with the PyBullet physics engine integrated into Blender. To enrich the variety of the background, we randomly selected physically-based ren- dering material from the HDRI Haven2and applied them to the wall of the box. Random environment lights are also added to generate diverse lighting conditions. Finally, the RGBD scene image is rendered from a random camera pose, and the ground truth instance semantic segmentation labels and pose parameters of each object are also obtained. Statistics about ShapeNet6D compared to existing 6D pose benchmark datasets are shown in Table 1. ShapeNet6D is 1https://www.blender.org 2https://hdrihaven.com/hdris+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Deformed ShapeOriginal Shape Real-World TextureOnline Texture BlendingRandom Background Random BackgroundOnline DeformationScene SceneOnline Texture BlendingFigure 2. Online data augmentation. The online texture blending augmentation generates texture by directly blending the real-world image to the object mesh model. No extra artificial simulation is applied, i.e., simulated lighting and the domain of real-world RGB images is preserved. Along with the online deformation augmentation [6], we can obtain data with diverse appearances and shapes at a low cost. on a larger scale and is more diverse in shape and appear- ance, which provides better prior to the few-shot pose esti- mation problem as we showed empirically. 3.2.2 Online texture blending As one of the crucial clues to solve the few-shot 6D pose estimation problem, the texture field is also essential to the performance of the few-shot 6D object pose estimation. However, it is labor-intensive and time-consuming to gen- erate textures and materials for objects that can be rendered to be photorealistic. The rendered RGB images tend to have more significant domain gaps between the real world as well. Moreover, to produce photorealistic images, time- and computation-consuming techniques like ray tracing are required. Therefore, the images should be pre-processed of- fline and stored before network training, which costs a lot of storage space for a large-scale dataset. On the other hand, real-world RGB images captured from various cameras are easy to access, i.e., ImageNet [9], and MS-COCO [33]. It motivates us to leverage efficient texture wrapping tech- niques to generate scenes of objects with rich real-world texture to serve as online data argumentation. Specifically, the mesh is first unwrapped to obtain a UV map. For each triangle, we get the UV coordinate of each vertex and then utilize it to determine the UV coordinate of each pixel by linear interpolation during rasterization. The UV coordi- nate is then applied to lookup the color value from a texture map randomly sampled from the real-world ImageNet [9], and MS-COCO [33]. Previous works [24,40] render images with artificial simulations, i.e., Beckmann model [2] , which change the domain and cause domain gaps. Instead, we ap- plied no simulation, so the composite images are kept in the real domain, i.e., the lighting condition, sensors noise of the real-world images are preserved. Moreover, such a simple blending strategy can be implemented fast to serve online . Moreover, we can combine it with online shape deforma- tion [6] to produce data with rich appearance and shape di-versity for training, as shown in Figure 2. 3.3. FS6D-DPM 3.3.1 Preliminaries Prototypes-based few-shot learning. We first briefly in- troduce the prototypes-based algorithms for few-shot learn- ing. It has been successfully applied to various few-shot 2D vision tasks, i.e., classification and semantic segmentation. Specifically, a pre-trained Siamese backbone is utilized for feature extraction from the support and the query images. Then, global average pooling is applied on the extracted support feature maps to obtain the support prototypes. This global average prototype is then applied to calculate the similarity between the global features (in classification) or dense pixel-wise features (in semantic segmentation) ex- tracted from the query image for prediction. However, these tasks\u2019 global-to-global or global-to-local correspondence is not enough to recover 6D object pose parameters. This work, instead, proposes a dense prototypes extraction mod- ule to establish the local-to-local correspondence between the support RGBD images and the query scene patch for pose estimation. Transformer [55]. Transformers networks are first in- troduced in Natural Language Processing and are brought into many vision tasks. The multi-head attention mecha- nism enables it to capture the long-term dependency even on an unordered set. Specifically, given three vectors as in- puts, namely query Q, key K, and value V. The attention mechanism is to retrieve information Ifrom the value s.t. the similarity between QandK, denoted as: Iretrieved =softmax (QKT)V. (1) Gifted with the capability of capturing long-term depen- dency, the Transformer networks [55, 60] have been suc- cessfully applied to aggregate contextual information in the local feature matching [47, 50] and point cloud registra- tion [25] field. In this work, we further extend it to dense+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Umeyama AlgorithmFull Flow Bidirectional Fusion Network Dense Prototypes Matching Pose Estimation Support View Support ViewQuery Scene Query Scene Correspondence MatrixCNN Encoder CNN Decoder Point Cloud DecoderPoint Cloud EncoderQuery FeatureSupport PrototypesLinear TransformerFigure 3. Overview of our pipeline. A Siamese full flow bidirectional fusion network [17] is utilized to extract rich appearance and geometric features from the support view and the query scene patch, respectively. The extracted features are then fed into self- and cross-attention modules to obtain dense support prototypes and query features for correspondence reasoning. Finally, the Umeyama algorithm [54] is applied to recover the pose parameters of novel objects in the query scene patch. \uff082\uff09\uff081\uff09 Figure 4. Complementary information in RGBD images for few-shot 6D pose estimation. (1) Texture information in RGB images is crucial cue for objects with smooth surface. (2) Geo- metric information in depth images is important cue for texture- less objects. RGBD prototypes matching for few-shot 6D pose estima- tion. 3.3.2 Overview To build a few-shot pose estimation algorithm that can gen- eralize well to novel objects, it is crucial to fully explore the semantic and geometric relationship between the given support views and the query scene patch, as shown in Fig- ure 4. In this section, we introduce our dense prototypes matching framework to tackle this challenging problem. As shown FS6D: Few-Shot 6D Pose Estimation of Novel Objects Yisheng He1Yao Wang2Haoqiang Fan2Jian Sun2Qifeng Chen1 1Hong Kong University of Science and Technology2Megvii Technology Abstract 6D object pose estimation networks are limited in their capability to scale to large numbers of object instances due to the close-set assumption and their reliance on high- fidelity object CAD models. In this work, we study a new open set problem; the few-shot 6D object poses estimation: estimating the 6D pose of an unknown object by a few sup- port views without extra training. To tackle the problem, we point out the importance of fully exploring the appear- ance and geometric relationship between the given support views and query scene patches and propose a dense pro- totypes matching framework by extracting and matching dense RGBD prototypes with transformers. Moreover, we show that the priors from diverse appearances and shapes are crucial to the generalization capability under the prob- lem setting and thus propose a large-scale RGBD photore- alistic dataset (ShapeNet6D) for network pre-training. A simple and effective online texture blending approach is also introduced to eliminate the domain gap from the syn- thesis dataset, which enriches appearance diversity at a low cost. Finally, we discuss possible solutions to this problem and establish benchmarks on popular datasets to facilitate future research. [project page] 1. Introduction 6D object pose estimation aims to predict a rigid trans- formation from the object coordinate system to the cam- era coordinate system, which benefits various applica- tions, including robotic manipulation, augmented reality, autonomous driving, etc. The explosive development of deep learning has brought significant improvement to this problem. With recent works [17, 18] reaching nearly 99% recall accuracy on existing benchmarks [20, 24, 62], one may get the impression that the 6D object pose problem has been solved, which is not the case. We argue that the current problem has been simplified with strict restrictions. They are under the close-set assumption that the training and testing data are drawn from the same object space, which, however, does not adhere to the real dynamic worlds. Moreover, extravagant high-fidelity CAD models and large- A Few Views Query Scene Patch Few Shot Pose Estimation Network Estimated Pose Figure 1. The few-shot 6D pose estimation problem . Given a few RGBD views of a novel objects with pose labels. The few-shot pose estimation network aims to estimate 6D pose of that object in a novel query scene without extra training. No precise CAD models are required as well. scale datasets are required for training to obtain good per- formance on new objects under the current instance-level pose estimation setting. The recently proposed category-level pose estimation task [59] loosens the restriction with generalizability to novel objects within the same categories. However, it is still limited in the close-set assumption of predefined categories. Instead, in this work, we study a new open-set problem, the few-shot 6D object pose estimation: estimating 6D pose of unknown objects by only a few views of the objects without extra training. As shown in Figure 1, in our setting, only a few labeled RGBD images of novel objects are provided, and no high-fidelity CAD models are required. The goal of the problem is to bridge the capability gap between machine learning algorithms and flexible human visual systems that can locate and estimate the pose of a novel object given only several views of it. Besides, it has a wide range of+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 real-world applications in robotic vision systems, i.e., fast registration of novel objects for robotic manipulation and home robots. Under the observation that human beings utilize both ap- pearance and geometric information to match and locate a new object, we propose a dense RGBD prototypes match- ing framework to tackle the problem. Specifically, trans- formers are utilized to fully explore the semantic and geo- metric relationship between the query scene patch and the support views of novel objects. Moreover, we point out that large-scale datasets\u2019 diverse shape and appearance priors are essential to empower networks to generalize on novel objects. Therefore, we introduce a large-scale photorealistic dataset (ShapeNet6D) with diverse shapes and appearances for prior learning. To our knowledge, ours (800K images of 12K objects) is the largest and most diverse dataset for 6D pose algorithms. To bridge the domain gap between ren- dered RGB images and real-world scenes, we introduce a simple and effective online texture blending augmentation, which further enriches the appearance diversity and facili- tates network performance at a low cost. To summarize, the contributions of this work are: \u2022 We introduce a challenging open-set problem, the few- shot 6D object pose estimation, and establish a bench- mark to study it. \u2022 We formulate the problem by dense RGBD proto- types matching and introduce FS6D-DPM, which fully leverage appearance and geometric information to tackle the problem. \u2022 Datasets: We introduce ShapeNet6D, a large-scale photorealistic dataset with diverse shapes and appear- ances for prior learning of few-shot 6D pose estima- tion algorithms. We also introduce an online texture blending augmentation to obtain scenes of texture-rich objects without domain gaps at a low cost. 2. Related Work 2.1. 6D Object Pose Estimation in Close-Set Setting Instance-level pose estimation retrieves pose parameters ofknown object instances. Matching based approaches [15, 19, 26, 51, 63] requires precise CAD models to render thousands of templates and establish hand-craft or learned codebook for matching. Learning-based approaches in- cludes direct pose regression [57,62], dense correspondence exploration [32] and recent keypoint-based approaches [17, 18, 41], which improve the performance by large margins. Despite compelling results, these approaches can only deal with scenarios of known objects with high-fidelity CAD models. Instead, the recent category-level pose estimation [59] improves the generalizability by estimating unseen ob-ject instances within the known categories . Normalized Ob- ject Coordinate Space (NOCS) [59] or shape deformation based [16,52] approaches are proposed. However, both tra- ditional instance- and category-level pose estimation prob- lems are under the close-set setting, assuming that the train- ing and testing data are within the same predefined instance or category spaces. While such close-set setting does not adhere to the real dynamic world, we instead define a new open-set problem, the few-shot 6D pose estimation. Algo- rithms in Figure 3, our framework consists of three main parts. Firstly, a Siamese RGBD feature extraction backbone is utilized to extract rich semantic and geometric features for each pixel/point. Then, a dense prototypes extraction network based on transformers is applied to extract denseRGBD prototypes from the support view and point-wise lo- cal features from the query scene patch for similarity cal- culation. Finally, after the correspondence between dense prototypes and scene features is established, the Umeyama algorithm [54] is leveraged to estimate the 6D pose param- eters. 3.3.3 Feature Extraction Backbone The first step is to extract rich semantic and geometric fea- tures from the given RGBD images. As a fundamental prob- lem, many works [17, 57, 64] have studied this representa- tion learning task. Recently, FFB6D [17] introduce a full flow bidirectional fusion network for 6D pose estimation and significantly improve the performance of close-set pose estimation. Specifically, bidirectional local feature fusion blocks are added into each encoding and decoding layer to bridge the information gap and improve the quality of ex- tracted semantic and geometric features (see [17] for de- tails). In this work, we leverage FFB6D to build a Siamese network for feature extraction from the support images and the query scenes. 3.3.4 Dense Prototypes Extraction and Matching Now we have obtained dense features from the Siamese feature extraction backbone. We then extract dense sup- port prototypes and query features to calculate the similar- ity and establish the correspondence. To extract descriptive and representative dense RGBD prototypes from the sup- port views and dense query features from query scenes, it is crucial to fully leverage the structural geometric infor- mation residing in point clouds and semantic information abiding in RGB images. Besides, contextual information+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 between the support shot and the query patch is also essen- tial to improve the precision of similarity calculation and correspondence exploration. Considering the power of transformers on long-term de- pendency capturing, we utilize the optimized Linear Trans- formers [60] to serve the above two purposes. As shown in the middle part of Figure 3, we first establish self-attention on the extracted feature maps to strengthen the geometric and semantic information residing in the extracted dense prototypes and dense query features. We regard the ex- tracted features as query, key, and value and fed them into the Linear Transformer networks to enhance the seman- tic and geometric features. Meanwhile, a cross-attention module is also applied to explore the contextual informa- tion between the support prototypes and the query scene features. Precisely, to extract contextual information from the support prototypes to the query scene features, we took each scene feature as a query and the dense prototypes as keys and values to the Linear Transformers. Contextual information from query scene features to support proto- types is enhanced similarly. With extracted contextual in- formation, another self-attention modules are applied to en- hance the geometric and semantic features further. In this way, we obtain dense support prototypes and query fea- tures with rich semantic, geometric and contextual infor- mation. Unlike prototype-based few-shot classification and segmentation algorithms that calculate the similarity by co- sine distance, we follow local feature matching pipelines [47] to establish the dense correspondence by calculating C(i, j) =\u27e8P(i), Q(j)\u27e9withP(i)theithprototype, Q(j) thejthquery feature and \u27e8\u00b7,\u00b7\u27e9the inner product. The Sinkhorn Algorithm [42] is applied for differentiable op- timization as well. 3.3.5 Pose Parameters Estimation After the correspondence between the dense prototypes and the query scene features is established, we utilize the Umeyama [54] algorithms to recover the pose parame- ters. Specifically, given a set of matched pairs M= {(pi, qi),1\u2264i\u2264N}withpi,qithe 3D coordinate of matched prototypes and queries, the Umeyama algorithms estimate the rotation Rand translation Tby minimizing: Llsq=NX i=1||qi\u2212(Rpi+T)||2 2. (2) To eliminate the influence of outliers. The RANSAC algo- rithms are also applied. Given Ksupport views of a novel object, we can obtain Kpredicted pose parameters along with their losses. We select the one with minimum loss as our final prediction.4. Experiments 4.1. Benchmark Datasets The LineMOD [20] and the YCB-Video [4] are two pop- ular datasets for 6D object pose estimation. The LineMOD dataset contains 13 videos of 13 low-textured objects, while the YCB-Video dataset consists of 92 RGBD videos of 21 YCB objects. For the few-shot pose estimation problem, we select 16 shots for each object for pose estimation. We also follow the strategy of other well-established few-shot prob- lems, i.e., segmentation, and split the dataset into different groups. Specifically, we split the objects into three groups for each dataset and select one for testing and the remaining two for training each time (see the supplementary material for details). 4.2. Evaluation Metrics The average distance metrics ADD and ADDS are widely used for performance evaluation of 6D pose esti- mation. For an object Oconsists of vertexes v, the ADD of asymmetric objects with the predicted pose R,Tand ground truth pose R\u2217,T\u2217is calculated by: ADD =1 mX v\u2208O||(Rv+T)\u2212(R\u2217v+T\u2217)||. (3) For symmetric objects, the ADDS based on the closest point distance is defined as: ADDS =1 mX v1\u2208Omin v2\u2208O||(Rv1+T)\u2212(R\u2217v2+T\u2217)||. (4) In the YCB-Video dataset, the area under the accuracy- threshold curve obtained by varying the distance threshold (ADDS and ADD AUC) is reported following [17, 18, 62]. In the LineMOD datasets, we report the distance less than 10% objects diameter recall (ADD-0.1d) as in [21, 41]. 4.3. Baselines Possible solutions to the few-shot 6D object pose estima- tion problem include local image feature matching, point cloud registration, and template matching. We select the state-of-the-art solution in each direction as our baseline. LoFTR [50] is a detector-free deep learning architec- ture for local image feature matching. It uses the self- and cross-attention layers in Transformers to obtain high-quality matches. PREDATOR [25] is a neural architecture for pairwise 3D point cloud registration with deep attention to the over- lap region. It learns to detect the overlap region between two unregistered scans and focus on that region when sam- pling feature points. Template Matching. Template matching approaches [15, 19, 26] discrete pose estimation problem into classifi- cation problem. These approaches rely on CAD models to+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 FS6D-DPMLoFTRPREDATOR GT LineMOD YCB-VideoFigure 5. Qualitative results on the YCB-Video (left) and the LineMOD (right) datasets. We visualize the results of PREDATOR [25], LoFTR [50] and the proposed FS6D-DPM. The ground truths are also visualized in the first row. Group ObjectPREDATOR [25] LoFTR [50] TP-UB FS6D-DPM ADDS ADD ADDS ADD ADDS ADD ADDS ADD 0002 master chef can 73.0 17.4 87.2 50.6 62.2 21.4 92.6 36.8 003 cracker box 41.7 8.3 71.8 25.5 65.6 5.0 83.9 24.5 004 sugar box 53.7 15.3 63.9 13.4 66.7 21.5 95.1 43.9 005 tomato soup can 81.2 44.4 77.1 52.9 75.2 43.1 93.0 54.2 006 mustard bottle 35.5 5.0 84.5 59.0 47.1 4.0 97.0 71.1 007 tuna fish can 78.2 34.2 72.6 55.7 72.8 38.4 94.5 53.9 008 pudding box 73.5 24.2 86.5 68.1 86.3 18.4 94.9 79.6 1009 gelatin box 81.4 37.5 71.6 45.2 90.9 43.2 98.3 32.1 010 potted meat can 62.0 20.9 67.4 45.1 59.8 28.9 87.6 54.9 011 banana 57.7 9.9 24.2 1.6 79.2 54.5 94.0 69.1 019 pitcher base 83.7 18.1 58.7 22.3 17.5 0.7 91.1 40.4 021 bleach cleanser 88.3 48.1 36.9 16.7 20.3 0.6 89.4 44.1 024 bowl 73.2 17.4 32.7 1.4 30.7 0.0 74.7 0.9 025 mug 84.8 29.5 47.3 23.6", "meta_data": {"split_id": 2}, "response": "The text has a low educational value and is not suitable for teaching from primary school to grade school levels. Here's why:\n\n1. The text is highly technical and specialized, using complex vocabulary and concepts that are not easily understandable by students at these levels.\n2. The text assumes a high level of background knowledge in computer vision, machine learning, and robotics, which may not be familiar to primary school or grade school students.\n3. The text is focused on a specific problem and solution, with no broader educational value or relevance to students' everyday lives.\n4. The text is dense and lacks illustrations, diagrams, or other visual aids that could help make the concepts more accessible and engaging for students.\n\nTherefore, I would score the text as having a low educational value and being unsuitable for teaching from primary school to grade school levels.\n\nEducational score: 1", "prediction": 1}
{"text": "", "meta_data": {"split_id": 1}, "response": "I apologize, but you didn't provide the extract. Please paste the extract, and I'll be happy to evaluate its educational value and assign a score using the additive 5-point scoring system.\n\nOnce I receive the extract, I'll justify my total score in 100 words or less and conclude with the score in the format \"Educational score: <total points>\".", "prediction": -1}
{"text": "non-deep learning-based works [2, 17, 18, 26, 27] are constructed based on statistical models. In [17], an attempt is made to perform the single- image deraining based on image decomposition using mor- phological component analysis. In a successive work [27], a mutual exclusivity property is incorporated into a sparsity model to form a discriminative sparse coding that can better separate the rain/background layers from their non-linear composite. Recently, deep-learning based deraining methods [9, 22, 41, 50] have been proposed to obtain superior rain removal results. More recent works [10, 20, 23, 34, 48] also focus on advanced deep networks. Others [7, 8, 16, 21, 33, 40, 43\u201345] make use of effective priors for deraining. Some studies focus on the generalization capability of deraining models. In [37], Wei et al. make use of real rainy images in net- work training by formulating the residual between an in- put rainy image and its expected deraining result as a mix- ture of Gaussians. In [46], a Gaussian process-based semi- supervised learning is proposed to allow the network learn with synthetic data and generalize deraining performance with unlabeled real rainy images. These methods can pro- vide better deraining results in terms of less remaining rain streaks and rich background textures. However, there is lit- tle work on the vulnerability of these deep networks against adversarial attacks. This work aims to fill this gap. 3. Benchmarking Adversarial Robustness of Deraining Models 3.1. Attack Framework Adversarial attacks aim to deteriorate the output of the deraining methods by adding a small amount of visually unperceivable perturbations to the input rainy images. To generate the adversarial perturbations, we develop an attack method based on one optimization-based method PGD [28], which has been extensively used and considered as a pow- erful attack method so far for evaluating the robustness of classification models [6]. We consider a deraining model f(\u00b7|\u03b8)parametrized by \u03b8. We denote Xas the input image, Yas the groundtruth image, \u03b4as the perturbations, and Das the metric to mea- sure the degradation. The objective of adversarial attacks is to maximize the deviation of the output from the target, e.g.+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 the original output: \u03b4= arg max \u03b4,\u2225\u03b4\u2225p\u2264\u03f5D(f(X|\u03b8), f(X+\u03b4|\u03b8))). (1) To solve the maximization problem with \u2113p-norm bound constraint (usually \u2113\u221e-norm), we use the PGD approach to obtain the perturbations iteratively: \u03c9t+1=\u03b4t+\u03b1sgn(\u2207\u03b4D(f(X|\u03b8), f(X+\u03b4|\u03b8)))),(2) \u03b4t+1=clip[\u2212\u03f5,\u03f5]\u2229[\u2212X,1\u2212X](\u03c9t+1), (3) where \u2207represents the gradient operation, sgn extracts the sign of gradients, and the clip operation guarantees that the perturbations are within [\u2212\u03f5, \u03f5]and the perturbed input is within [0,1]. The term \u03b1controls the step length each iter- ation, and \u03f5represents the maximum perturbation allowed for each pixel value. The initial \u03b40is sampled from the uniform distribution U(\u2212\u03f5, \u03f5), and the final adversarial per- turbations \u03b4Tis obtained after Titerations. Based on different attack objectives, we define two types of corresponding metrics for D: \u2022Restoration (Human Vision ): per-pixel measures, e.g. \u21132Euclidean distance: \u03b4= arg max \u03b4,\u2225\u03b4\u2225\u221e\u2264\u03f5\u2225f(X+\u03b4|\u03b8)\u2212f(X|\u03b8)\u22252.(4) \u2022Downstream CV Tasks (Machine Vision ): the dis- tance of feature extracted from pretrained models, e.g. Learned Perceptual Image Patch Similarity (LPIPS) [52]: \u03b4= arg max \u03b4,\u2225\u03b4\u2225\u221e\u2264\u03f5\u2113pips(f(X+\u03b4|\u03b8), f(X|\u03b8)).(5) We denote the first one as LMSE attack and the second as LPIPS attack. For each attack type and perturbation bound, \u03b4D,\u03f5(X)is obtained as Eqs. (2\u20133), and the general form of adversarial robustness for deraining model fis given by RD,\u03f5 f=EX\u223cPdata[P(Y, f(X+\u03b4D,\u03f5(X)))], (6) where the samples follow the distribution Pdata, and Pis the evaluation metric. While it is impossible to evalaute the robustness by considering all values of \u03f5, we estimate it using finite \u03f5and limited test samples. With the estimation, we use mean-Adversarial-Performance (mAP) to evaluate the adversarial robustness against each type of attack D: mAPD=1 n(E)X \u03f5\u2208E1 n(Dt)X X\u2208DtP(Y, f(X+\u03b4D,\u03f5(X))), (7) where Eis the set of \u03f5to be evaluated, Dtis the test dataset, andn(\u00b7)counts the numbers. Note that Pis usually mea- sured in terms of PSNR and SSIM.Table 1. PSNR, parameter and module comparisons on two datasets. AT denotes attention module, RB recurrent blocks, SI side information and DD diverse dilations, and RC RainCityscape. Model Para. ATRB SIDD Rain100H RC JORDER-E [41] 4.17M \u2713\u2713\u2713 29.75 32.51 RCDNet [33] 3.17M \u2713 29.65 31.44 MPRNet [48] 3.64M \u2713 30.56 36.31 PReNet [30] 1.50M \u2713 29.58 33.09 UMRL [45] 984K 26.05 30.15 RESCAN [25] 1.04M \u2713\u2713 \u2713 28.90 34.90 3.2. Evaluation Metrics In our benchmark, we consider two types of performance evaluation metrics: \u2022 The commonly used quality measures including peak signal to noise ratio (PSNR) and structural index simi- larity (SSIM) [36], for evaluating signal fidelity. \u2022 Task-driven metrics that evaluate the performance of down-stream machine analysis tasks. Specifically, we evaluate the performance of object detection and semantic segmentation, i.e. mean Intersection over Union (mIoU) for semantic segmentation and Average Precision (AP) for pedestrian detection, with the ap- proaches SSeg [53] and Pedestron [14] on the RainCi- tyscape dataset [13]. 3.3. Implementation Details 1) Datasets. We employ two synthetic deraining datasets (Rain100H [42] and RainCityscape [13]) and some real im- ages [37]. Rain100H dataset is one of the most commonly used datasets consisting of 1800 paired rain/non-rain im- ages of size 480\u00d7320for training and 100 paired images of the same size for testing. PSNR and SSIM of input test- ing set are 12.05 and 0.3623. RainCityscape dataset con- sists of 2875 paired images of size 512\u00d7256for training and 100 paired images for testing. PSNR and SSIM of in- put testing set are 19.71 and 0.7087. Besides paired im- ages, RainCityscape dataset also provides additional labels for object detection and semantic segmentation, which al- low us to evaluate the performance of down-stream tasks. In our experiment, the model evaluated on the testing set of Rain100H [42] and RainCityscape [13] is trained on their respective training sets. 2) Deraining Methods. We consider six state-of-the-art deep learning-based deraining models as shown in Table 1. 3) Perturbation Levels. The adversarial attacks are im- plemented based on PGD [28]. We also set \u03f5\u2208E= {1/255,2/255,4/255,8/255}regarding to \u2113\u221e-norm, \u03b1= \u03f5/4, and T= 20 . 4) Results and Remarks. Figure 2 and Table 2 show the robustness results of six deraining models against adversar- ial attacks. For the RainCityscape dataset, we also consider+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 (a)Rain100H, PSNR (b)Rain100H, SSIM (c)RC, PSNR (d)RC, SSIM (e)RC, mIoU (f)RC, Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond Yi Yu1,2Wenhan Yang1* Yap-Peng Tan1Alex C. Kot1 1School of Electrical and Electronic Engineering, Nanyang Technological University 2ROSE Lab, Interdisciplinary Graduate Programme, Nanyang Technological University yuyi0010@e.ntu.edu.sg {wenhan.yang,eyptan,eackot }@ntu.edu.sg Abstract Rain removal aims to remove rain streaks from im- ages/videos and reduce the disruptive effects caused by rain. It not only enhances image/video visibility but also allows many computer vision algorithms to function prop- erly. This paper makes the first attempt to conduct a com- prehensive study on the robustness of deep learning-based rain removal methods against adversarial attacks. Our study shows that, when the image/video is highly degraded, rain removal methods are more vulnerable to the adversar- ial attacks as small distortions/perturbations become less noticeable or detectable. In this paper, we first present a comprehensive empirical evaluation of various methods at different levels of attacks and with various losses/targets to generate the perturbations from the perspective of hu- man perception and machine analysis tasks. A system- atic evaluation of key modules in existing methods is per- formed in terms of their robustness against adversarial at- tacks. From the insights of our analysis, we construct a more robust deraining method by integrating these effec- tive modules. Finally, we examine various types of adver- sarial attacks that are specific to deraining problems and their effects on both human and machine vision tasks, in- cluding 1) rain region attacks, adding perturbations only in the rain regions to make the perturbations in the at- tacked rain images less visible; 2) object-sensitive attacks, adding perturbations only in regions near the given objects. Code is available at https://github.com/yuyi- sd/Robust_Rain_Removal . 1. Introduction Rain removal methods aim to remove the disruptive ef- fects caused by rain streaks to restore a clean version of the image. It not only largely improves the visibility of the rainy image but can also improve the performance of many *Corresponding author. Figure 1. Left is from corresponding patch of clean input/output, and right is from adversarial input/output with perturbation bound \u03f5= 4/255. The semantic segmentation overlap of output is shown in the last column. The testing image is from RainCityscape [13]. subsequent (downstream) computer vision applications. Early approaches are mainly model-driven and address the deraining problem based on the statistical properties of rain streaks and background scenes, e.g. image decom- position [17], sparse coding [27], and Gaussian mixture model [26]. These methods can well handle light rain. However, they fall short in cases of handle heavy rain and often blur the background scenes. Recently, deep-learning based deraining methods [9, 41] have become the main- stream. These methods have the capacity to model more complicated mappings from rain images to clean images and offer a better performance in terms of less remaining rain streaks and better preserved background scene. They also enhanced the performance of practical applications such as video surveillance. While many deep learning-based deraining methods have been introduced, it isn\u2019t a thorough study on the ro- bustness of these methods against adversarial attacks. It is also a real concern of deraining methods that some un- noticeable perturbations can alter the predicted results of deep networks [12, 31]. As the rainy images can be usu- ally highly degraded by intensive rain streaks, perturbations can be easily and noticeably hidden in such images. These adversarially generated outputs can also compromise the re- liability and stability of the subsequent applications such as video surveillance and autonomous driving, that adopt rain removal methods as a pre-processing module.+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 In this paper, we make the first attempt to investigate, improve, and evaluate the robustness of deep learning-based rain removal methods against adversarial attacks. Our main contributions are summarized below. \u2022 A thorough analysis of existing deraining methods at different levels of robustness against adversarial at- tacks and with various losses/targets to generate the perturbations, from both the perspectives of human and machine vision tasks. \u2022 A systematic evaluation of the modules in existing methods at both the model side (recurrence, attention, receptive field, and side information) and loss side (ad- versarial loss) in terms of their impact on robustness. \u2022 A more robust deraining method constructed by inte- grating the effective modules which are identified from our analysis. \u2022 Investigation of various adversarial attack types and scenarios specific to rain removal degradation in terms of both human and machine vision tasks. 2. Related Work 2.1. Adversarial Attacks Deep neural networks [32] are vulnerable to adversar- ial examples that are visually indistinguishable from natu- ral images but can compromise the performance of mod- els. From the attack method aspect, Szegedy et al. [32] developed an optimization-based approach to find adver- sarial examples within specific amounts of input perturba- tion. Goodfellow et al. [12] proposed the Fast Gradient Sign Method (FGSM) and Kurakin et al. [19] developed multi- ple variations. Madry et al. [28] proposed a more power- ful multi-step variant, which is essentially Projected Gra- dient Descent (PGD). There are also studies on the effects of adversarial attacks on various computer vision applica- tions and scenarios, e.g.human pose estimation [5], object detection [39], and semantic segmentation [39]. There are also works on adversarial attacks and ro- bustness analysis in low-level processing tasks. In [29], super-resolution is adopted as the tool to mitigate the adversarial attacks for deep image classification models. In [47], adversarial attacks are exploited on super-resolution model to attack the down-stream computer vision tasks, e.g.image classification, style transfer, and image caption- ing. In [3], the robustness of deep learning-based super- resolution methods against adversarial attacks is examined. In [11, 49], rain streak/haze is considered as one type of adversarial noise. In [4], the robustness of deep image-to- image models against adversarial attacks is evaluated from the perspective of human vision. However, there is no work investigating adversarial attacks and robustness analysis inrain removal scenarios. It is also absent to consider the adversarial attacks from the perspective of human and ma- chine vision tasks. 2.2. Single-image rain removal It is non-trivial to separate rain streaks and background scenes from their mixed versions. Methods based on sig- nal separation and texture classification have been proposed to address the challenge. Early AP (g)Rain100H, PSNR (h)Rain100H, SSIM (i)RC, PSNR (j)RC, SSIM (k)RC, mIoU (l)RC, AP Figure 2. Adversarial Robustness of six deraining models and our proposed method evaluated by PSNR, SSIM, mIoU of Semantic Segmentation and AP of Pedestrian Detection on the Rain100H [42] and RainCityscape [13] datasets. Subfigures in the first row denote the performance against LMSE attack, and subfigures in the second row denote the LPIPS attack. RC denotes RainCityscape. Note that \u03f5= 0/255denotes the performance of output to input without perturbations. (a) Deraining Images against adversarial attacks in real images [37]. The models are trained on Rain100H [42]. (b) Deraining Images against adversarial attacks in RainCityscape [13] and its corresponding Semantic Segmentation overlap. Figure 3. Visual comparison of the deraining outputs for input with perturbations based on LMSE attack. In each image other than input, patches on the top-left, top-right, bottom-left and bottom-right are corresponding patches from the output to input with perturbation bound \u03f5={0/255,1/255,2/255,4/255}. Best view by zooming in. More visualization results can be found in the supplementary material. the performance of pedestrian detection and semantic seg- mentation. From the benchmark results, we can have the following: \u2022 Even very small perturbations ( \u03f5= 1 ) can heavily degrade the deraining performance of existing meth- ods. Larger perturbations \u03f5lead to significantly per- formance drops in image quality. \u2022 On the Rain100H dataset, MPRNet [48] is the most ro- bust model in terms of PSNR and SSIM, which might be attributed to its use of attention modules to suppress the effects of adversarial noise at the feature level.\u2022 On the RainCityscape dataset, the gaps of PSNR and SSIM values against adversarial attacks with identical \u03f5are closer for all the models, while MPRNet is most robust for the down-stream tasks in terms of mIoU and AP. \u2022 When attacking with LPIPS, it is observed that pertur- bations from 0 to 1 have the largest performance drop in terms of PSNR and SSIM while larger perturba- tions lead to smaller performance gaps. Especially on RainCityscape, perturbations from 1 to 8 bring about the same degradation in terms of PSNR, while SSIM, which considers more structural information, contin-+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Table 2. Adversarial Robustness (mean-Adversarial-Performance) of six deraining models on two datasets. Datasets Rain100H RainCityscape Metrics PSNR SSIM PSNR SSIM mIoU AP Restoration (Human Vision): MSE Loss JORDER-E 10.91 0.3383 12.89 0.5920 0.2799 0.6511 RCDNet 12.29 0.3744 13.21 0.5386 0.1924 0.6307 MPRNet 13.47 0.5253 12.76 0.5693 0.3945 0.7217 PReNet 8.82 0.2107 13.12 0.4864 0.1151 0.6056 UMRL 12.94 0.4297 12.02 0.5512 0.1886 0.6086 RESCAN 11.40 0.3528 11.13 0.5088 0.2168 0.6065 Downstream CV Tasks (Machine Vision): LPIPS Loss JORDER-E 16.22 0.5423 20.18 0.6692 0.2354 0.6014 RCDNet 18.54 0.6086 20.39 0.6498 0.1001 0.5427 MPRNet 22.70 0.6878 20.10 0.6815 0.2451 0.6753 PReNet 11.22 0.3628 20.48 0.6598 0.2291 0.6398 UMRL 21.44 0.6594 19.03 0.6420 0.1140 0.4548 RESCAN 17.54 0.5640 20.44 0.6681 0.1652 0.6376 ues to drop. And it\u2019s not surprising to find that LMSE attack brings more drop on PSNR and SSIM, while LPIPS attack brings more on mIoU and AP. \u2022 It is interesting to note that the deraining results are vulnerable in terms of semantic segmentation perfor- mance. The attack with the perturbation ( \u03f5= 2) has already crashed the segmentation performance. \u2022 Existing methods are more robust for object detection than semantic segmentation. 4. Towards Robust Deraining Model In this section, we perform a comprehensive analysis on the modules and losses to evaluate their adversarial robust- ness. 4.1. Module Analysis The state-of-the-art deraining methods usually consist of recurrent blocks, several attention modules, side informa- tion injection and larger receptive field. We perform an ablation study of these modules based on the baseline of RCDNet [33] and JOEDER-E [42]. 1) Recurrent Blocks. As there might be different rain streak layers overlapping with each other and it is not easy to remove all streaks in one stage, many deraining mod- els incorporate a recurrent structure to perform rain removal into multiple stages. The process can be formulated as: O0=O, [Rt, Bt] =ft(Ot\u22121, Rt\u22121),1\u2264t\u2264T, Ot=O\u2212Rt=Bt,1\u2264t\u2264T,(8) where Ois the input, Tis the number of recurrent blocks, Rtis the output rain streaks at the t-th stage, Btis the output background at the t-th stage, and ftis the t-th CNN block. In each stage, RtandBtare updated, and the final output is BT. Figure 4. Recurrent Blocks: Adversarial Robustness for RCDNet with various stage numbers. Red curves correspond to the left y- axis, and blue curves correspond to the right y-axis. Table 3. Attention Module: Adversarial Robustness of RCDNet with various attention module. Clean represents the performance for input without perturbations in terms of SSIM. Datasets Rain100H RainCityscape Metrics Clean PSNR SSIM Clean PSNR SSIM mIoU AP Restoration (Human Vision): MSE Loss RCDNet 0.8867 12.29 0.3744 0.9517 13.21 0.5386 0.1924 0.6307 + SE (MUL) 0.8834 16.10 0.5325 0.9626 13.43 0.5768 0.2874 0.6425 + SE (ADD) 0.8859 15.63 0.5246 0.9697 13.83 0.6100 0.3502 0.6672 + CBAM 0.8777 14.47 0.4930 0.9553 13.55 0.6039 0.2699 0.6465 + GC (MUL) 0.8797 15.83 0.5371 0.9617 13.75 0.5959 0.2869 0.6666 + GC (ADD) 0.8694 15.81 0.5595 0.9644 10.99 0.5648 0.2947 0.6347 + SK 0.8813 15.29 0.5260 0.9553 12.77 0.5574 0.3478 0.6341 Downstream CV Tasks (Machine Vision): LPIPS Loss RCDNet 0.8867 18.54 0.6086 0.9517 20.39 0.6498 0.1001 0.5427 + SE (MUL) 0.8834 24.00 0.7108 0.9626 20.23 0.6405 0.1976 0.6838 + SE (ADD) 0.8859 23.68 0.7011 0.9697 20.44 0.6867 0.2461 0.6730 + CBAM 0.8777 22.33 0.6552 0.9553 19.25 0.6783 0.1373 0.6603 + GC (MUL) 0.8797 24.50 0.7176 0.9617 20.02 0.6574 0.2242 0.6725 + GC (ADD) 0.8694 23.81 0.7159 0.9644 19.43 0.6114 0.1710 0.6516 + SK 0.8813 23.58 0.7048 0.9553 20.48 0.7109 0.2389 0.5603 We examine the adversarial robustness of RCDNet with various stage numbers {1,2,3,5,8,12,17}. For a fair com- parison, the models with fewer stages have wider feature maps to keep the number of parameters around 3M. Insight : As shown in Fig. 4, the models with one or two re- current blocks are more robust in terms of PSNR and SSIM, while the model with three stages is the most robust con- sidering the mIoU and AP for down-stream tasks and the second most robust in terms of PSNR and SSIM . 2) Attention Module. RESCAN [25] and MPRNet [48] both consist of attention modules to enhance its deraining performance. We look into their effects in terms of robust- ness. We consider several attention modules based on fea- ture re-calibration in CNN: \u2022SENet [15]: Features are first passed through an avg- pooling squeeze operator across spatial dimension and+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Table 4. Side Information: Effect of side information on the adver- sarial robustness of JORDER-E. Clean represents the performance for input without perturbations in terms of SSIM. Datasets Rain100H RainCityscape Metrics Clean PSNR SSIM Clean PSNR SSIM mIoU AP Restoration (Human Vision): MSE Loss JORDER-E 0.8921 10.91 0.3383 0.9600 12.89 0.5920 0.2799 0.6511 NO Mask 0.8809 15.37 0.4991 0.9626 14.89 0.6143 0.2955 0.6202 Downstream CV Tasks (Machine Vision): LPIPS Loss JORDER-E 0.8921 16.22 0.5423 0.9600 20.18 0.6692 0.2354 0.6014 NO Mask 0.8809 22.71 0.6947 0.9626 20.01 0.6707 0.2574 0.6176 then produce the channel-wise activations by the sub- sequent excitation operation. Note that the activations can be added or multiplied to the feature maps. \u2022GCNet [1]: GCNet differs from SENet in the squeeze operation, and adopts the structures of Non-Local Net- works [35] for spatial modeling. \u2022CBAM [38]: CBAM has two sequential sub-modules: channel and spatial. The subsequent module concate- nates the output of Max and Avg pooling (across chan- nel), and uses a Conv layer to produce the spatial at- tention map. \u2022SKNet [24]: \u201d Selective Kernel \u201d (SK) block fuses the features in multiple branches with different kernel sizes via softmax attention. Different channel atten- tions on these branches are performed as well. Insight : By inserting the attention module into the same location of RCDNet (after each conv block), we find that all attention modules indeed improve the robustness and SE with the addition is the most robust considering the down- stream tasks; it also performs well in terms of PSNR and SSIM, as can be seen from Table 3", "meta_data": {"split_id": 2}, "response": "Based on the extract, I would evaluate the educational value as follows:\n\n* Provides some basic information relevant to educational topics (1 point): The extract discusses deep learning-based deraining methods, adversarial attacks, and robustness analysis, which is relevant to educational topics in computer vision and machine learning.\n* Does not align closely with educational standards (0 points): The extract assumes a high level of prior knowledge in deep learning and computer vision, which may not be suitable for primary school or grade school levels.\n* Coherent though not comprehensive overview of key concepts (2 points): The extract presents a clear overview of the concepts of deep learning-based deraining methods, adversarial attacks, and robustness analysis, but it does not provide a comprehensive treatment of the subject.\n* Writing style is professional, but not easy to follow (1 point): The extract is well-written and professional, but it assumes a high level of technical expertise, which may make it challenging for non-experts to follow.\n\nTotal score: 4 points\n\nEducational score: 4", "prediction": 4}
{"text": "vectors are concatenated with position embeddings. Also, this set of vectors includes an additional \u201cclassi\ufb01cation\u201d token. Note that in our case, this token is used to obtain the image embedding, but we do not train a standard classi\ufb01er as in [7]. For consistency with previous literature, we name this token [class] . The set of resulting vectors is fed into a standard transformer en- coder [56]. It consists of several layers with multiheaded self-attention (MSA) and MLP blocks, with a LayerNorm before and a residual connection after each block. The out- put for the transformer encoder for the [class] token is used as the \ufb01nal image representation. For more details, we refer to [7]. ViT-S [48] is a smaller version of ViT with 6 heads in MSA (base version uses 12 heads). This architecture is similar to ResNet-50 [18] in terms of number of parame- ters (22M for ViT-S and 23M for ResNet-50) and computa- tional requirements (8.4 FLOPS for ViT-S and 8.3 FLOPS for ResNet-50). This similarity makes it possible to fairly compare with previous works based on ResNet-50 encoder, for this reason, we employ this con\ufb01guration for our exper- iments. A more thorough description is available in [48]. Vision transformers, compared to CNNs, require more training signal. One solution, as proposed in [7], is to use a large dataset. ImageNet-21k [6] contains approxi- mately 14M images classi\ufb01ed into 21K categories. ViT- S, pretrained on ImageNet-21k, is publicly available [48]; we include it in our experiments. Another solution, DeiT- S[53], is based on the same (ViT-S) architecture and is trained on a smaller ImageNet-1k dataset [41] (a subset of ImageNet-21k consisting of about 1.3M training im-ages and 1K categories). An additional training signal is provided by teacher-student distillation, with a CNN-based teacher [53]. The third solution used in our experiments, DINO [3], is based on self-supervised training. In this case, the model ViT-S is trained on the ImageNet-1k dataset [41] without labels. The encoder must produce consistent output for dif- ferent parts of an image, obtained using augmentations (ran- dom crop, color jitter, and others). This training scheme is in line with the image retrieval task; in both cases, the en- coder is explicitly trained to produce similar output for se- mantically similar input. However, the goal of these tasks is different: self-supervised learning provides pretrained fea- tures, which are then used for other downstream tasks, while for image retrieval resulting features are directly used for the evaluation. 3. Experiments We follow a widely adopted training and evaluation pro- tocol [23] and compare several versions of our method with current state-of-the-art on four benchmark datasets for category-level retrieval. We include technical details of datasets, our implementation and training details, and \ufb01- nally, present empirical results. There are two types of ex- periments, \ufb01rst, we compare with the state-of-the-art, and then we investigate the impact of hyperparameters (encoder patch size, manifold curvature, embedding size and batch size). 3.1. Datasets CUB-200-2011 (CUB) [61] includes 11,788 images with 200 categories of bird breeds. The training set corresponds to the \ufb01rst 100 classes with 5,864 images, and the remain- ing 100 classes with 5,924 images are used for testing. The images are very similar; some breeds can only be distin- guished by minor details, making this dataset challenging and, at the same time, informative for the image retrieval task. Cars-196 (Cars) [25] consists of 16,185 images repre- senting 196 car models. First 98 classes (8,054 images) are used for training and the other 98 classes (8,131 images) are held out for testing. Stanford Online Product (SOP) [47] consists of 120,053 images of 22,634 products downloaded from eBay.com. We use the standard split: 11,318 classes (59,551 images) for training and remaining 11,316 classes (60,502 images) for testing. In-shop Clothes Retrieval (In- Shop) [28] consists of 7,986 categories of clothing items. First 3,997 categories (25,882 images) are for training, the remaining 3,985 categories are used for testing partitioned into a query set (14,218 images) and a gallery set (12,612 images).+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 3.2. Implementation Details We use ViT-S [48] as an encoder with three types of pre- training (ViT-S, DeiT-S and DINO), details are presented in Section 2.5. The linear projection for patch embeddings as a \ufb01rst basic operation presumably corresponds to low- level feature extraction, so we freeze it during \ufb01ne-tuning. The encoder outputs a representation of dimensionality 384, which is further plugged into a head linearly projecting the features to the space of dimension 128. We initialize the bi- ases of the head with constant 0and weights with a (semi) orthogonal matrix [43]. We include two versions of the head: with a projection to a hyperbolic space (\u201cHyp-\u201d) and with projection to a unit hypersphere (\u201cSph-\u201d). In the \ufb01rst case, we use curvature parameter c= 0.1(in Section 3.4 we investigate how it affects the method\u2019s performance), temperature \u03c4= 0.2and clipping radius (de\ufb01ned in Sec- tion 2.4)r= 2.3. For spherical embeddings, we use tem- perature\u03c4= 0.1. To evaluate the model performance, for the encoder, we compute the Recall@K metric for the output with distance Dcos(Eq. (4)); for the head, we use Dcosfor \u201cSph-\u201d ver- sion and hyperbolic distance Dhyp(Eq. (2)) for \u201cHyp-\u201d ver- sion. We resize the test images to 224(256for CUB) on the smaller side and take one 224\u00d7224center crop. Note that some methods use images of higher resolution for train- ing and evaluations, e.g., ProxyNCA++ [52] use 256\u00d7256 crops indicating that smaller 227\u00d7227crops degrade the performance by 4.3%on CUB. However, 224\u00d7224is the default size for encoders considered in our work; moreover, some recent methods, such as IRT R[8], use this size for experiments. We use the AdamW optimizer [29] with a learning rate value 1\u00d710\u22125for DINO and 3\u00d710\u22125for ViT-S and DeiT- S. The weight decay value is 0.01, and the batch size equals 900. The number of optimizer steps depends on the dataset: 200for CUB, 600for Cars, 25000 for SOP, 2200 for In- Shop. The gradient is clipped by norm 3for a greater sta- bility. We apply commonly used data augmentations: ran- dom crop resizing the image to 224\u00d7224 using bicubic interpolation combined with a random horizontal \ufb02ip. We train with Automatic Mixed Precision in O2mode1. All experiments Hyperbolic Vision Transformers: Combining Improvements in Metric Learning Aleksandr Ermolov University of Trento, Italy aleksandr.ermolov@unitn.itLeyla Mirvakhabova Skoltech*, Russia leyla.mirvakhabova@skoltech.ruValentin Khrulkov Yandex & Skoltech*, Russia khrulkov.v@gmail.com Nicu Sebe University of Trento, Italy niculae.sebe@unitn.itIvan Oseledets Skoltech*, Russia I.Oseledets@skoltech.ru Abstract Metric learning aims to learn a highly discriminative model encouraging the embeddings of similar classes to be close in the chosen metrics and pushed apart for dis- similar ones. The common recipe is to use an encoder to extract embeddings and a distance-based loss function to match the representations \u2013 usually, the Euclidean dis- tance is utilized. An emerging interest in learning hyper- bolic data embeddings suggests that hyperbolic geometry can be bene\ufb01cial for natural data. Following this line of work, we propose a new hyperbolic-based model for metric learning. At the core of our method is a vision transformer with output embeddings mapped to hyperbolic space. These embeddings are directly optimized using modi\ufb01ed pairwise cross-entropy loss. We evaluate the proposed model with six different formulations on four datasets achieving the new state-of-the-art performance. The source code is available athttps://github.com/htdt/hyp_metric . 1. Introduction Metric learning task formulation is general and intu- itive: the obtained distances between data embeddings must represent semantic similarity. It is a typical cogni- tive task to generalize similarity for new objects given some examples of similar and dissimilar pairs. Metric learn- ing algorithms are widely applied in various computer vi- sion tasks: content-based image retrieval [32, 46, 47], near- duplicate detection [65], face recognition [27, 44], person re-identi\ufb01cation [5, 63], as a part of zero-shot [47] or few- shot learning [38, 45, 50]. Modern image retrieval methods can be decomposed into roughly two components: the encoder mapping the image to its compact representation and the loss function govern- *Skolkovo Institute of Science and Technologying the training process. Encoders with backbones based on transformer architecture have been recently proposed as a competitive alternative to previously used convolu- tional neural networks (CNNs). Transformers lack some of CNN\u2019s inductive biases, e.g., translation equivariance, re- quiring more training data to achieve a fair generalization. On the other hand, it allows transformers to produce more general features, which presumably can be more bene\ufb01cial for image retrieval [3,8], as this task requires generalization to unseen classes of images. To alleviate the issue above, several training schemes have been proposed: using a large dataset [7], heavily augmenting training dataset and using distillation [53], using self-supervised learning scenario [3]. The choice of the embedding space directly in\ufb02uences the metrics used for comparing representations. Typically, embeddings are arranged on a hypersphere, i.e. the output of the encoder is L2normalized, resulting in using cosine similarity as a distance. In this work, we propose to con- sider the hyperbolic spaces. Their distinctive property is the exponential volume growth with respect to the radius, un- like Euclidean spaces with polynomial growth. This feature makes hyperbolic space especially suitable for embedding tree-like data due to increased representation power. The paper [42] shows that a tree can be embedded to Poincar \u00b4e disk with an arbitrarily low distortion. Most of the natu- ral data is intrinsically hierarchical, and hyperbolic spaces suit well for its representation. Another desirable property of hyperbolic spaces is the ability to use low-dimensional manifolds for embeddings without sacri\ufb01cing the model ac- curacy and its representation power [34]. The goal of the loss function is straightforward: we want to group the representations of similar objects in the embed- ding space while pulling away representations of dissimilar objects. Most loss functions can be divided into two cat- egories: proxy-based and pair-based [23]. Additionally to the network parameters, the \ufb01rst type of losses trains prox- ies, which represent subsets of the dataset [32]. This proce-arXiv:2203.10833v1 [cs.CV] 21 Mar 2022+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 ViTExponential mapping Pairwise Cross-EntropyPoincar\u00e9 disk 384d 128dFCFigure 1. Overview of the proposed method. Two images representing one class (positives) are encoded with the vision transformer, projected into a space of a lower dimension with a fully connected (FC) layer, and then mapped to a hyperbolic space. Blue stars depict the resulting embeddings. Poincar \u00b4e disk is shown with uniform triangle tiling on the background to illustrate the manifold curvature. Gray circles represent other samples from the batch (negatives). Finally, arrows in the disk represent distances used in the pairwise cross-entropy loss. Positives are pushed closer to each other, negative are pulled far apart. dure can be seen from a perspective of a simple classi\ufb01ca- tion task: we train matching embeddings, which would clas- sify each subset [33]. At the same time, pair-based losses operate directly on the embeddings. The advantage of pair- based losses is that they can account for the \ufb01ne-grained in- teractions of individual samples. Such losses do not require data labels: it is suf\ufb01cient to have pair-based relationships. This property is crucial for a widely used pairwise cross- entropy loss in self-supervised learning scenario [4, 17, 55]. Instead of labels, the supervision comes from a pretext task, which de\ufb01nes positive and negative pairs. Inspired by these works, we adopt pairwise cross-entropy loss for our experi- ments. The main contributions of our paper are the following: \u2022 We propose to project embeddings to the Poincar \u00b4e ball and to use the pairwise cross-entropy loss with hy- perbolic distances. Through extensive experiments, we demonstrate that the hyperbolic counterpart outper- forms the Euclidean setting. \u2022 We show that the joint usage of vision transformers, hyperbolic embeddings, and pairwise cross-entropy loss provides the best performance for the image re- trieval task. 2. Method We propose a new metric learning loss that combines representative expressiveness of the hyperbolic space and the simplicity and generality of the cross-entropy loss. The suggested loss operates in the hyperbolic space encourag- ing the representatives of one class ( positives ) to be closer while pushing the samples from other categories ( negatives ) away.The schematic overview of the proposed method is de- picted at Figure 1. The remainder of the section is organized as follows. We start with providing the necessary prelimi- naries on hyperbolic spaces in Section 2.1, then we discuss the loss function in Section 2.2 and, \ufb01nally, we brie\ufb02y de- scribe the architecture and discuss pretraining schemes in Section 2.5. 2.1. Hyperbolic Embeddings Formally, the n-dimensional hyperbolic space Hnis a Riemannian manifold of constant negative curvature. There exist several isometric models of hyperbolic space, in our work we stick to the Poincar \u00b4e ball model (Dn c,gD)with the curvature parameter c(the actual curvature value is then \u2212c2). This model is realized as a pair of an n-dimensinal ballDn={x\u2208Rn:c/bardblx/bardbl2<", "meta_data": {"split_id": 2}, "response": "I evaluated the text based on the additive 5-point scoring system, and here's my assessment:\n\n* The text provides basic information relevant to educational topics, but it is heavily focused on specific research in the field of computer vision and metric learning. The content is dense and technical, making it challenging for primary school or grade school students to understand. (Score: 1 point)\n* The text addresses certain elements pertinent to education, such as the concept of metric learning and the use of hyperbolic spaces, but it does not align closely with educational standards. The writing style is not coherent, and the text is more suitable for advanced students or professionals in the field. (Score: 2 points)\n* The text is not appropriate for educational use at the primary or grade school level due to its complexity and technical nature. It may be suitable for upper-level high school students or college students with a background in computer science or mathematics. (Score: 2 points)\n* The text is highly relevant and beneficial for educational purposes at the college or university level, but it may not be suitable for grade school or primary school students due to its complexity. (Score: 3 points)\n* The text is not outstanding in its educational value, as it is highly specialized and technical. While it may be beneficial for students or professionals in the field, it is not a comprehensive or accessible resource for primary or grade school students. (Score: 0 points)\n\nTotal score: 8 points\n\nEducational score: 8", "prediction": 8}
{"text": "work by construction, and their likelihood-based training objective ensures accurate reconstructions. Vector-quantized variational autoencoders (VQ- V AE) [2, 49], which discretize the latent space, have been found to offer both good reconstructions as well as com- pelling samples. In particular, VQ-GAN [15, 53] further improves reconstructions by including an adversarial loss term to train the autoencoder. In our work, we adopt the VQ-GAN autoencoder, and edit images in its latent space. Latent space manipulation. The introduction of \u201cstyle- based\u201d GANs, such as StyleGAN [28\u201330] significantly im- proved the disentanglement of the latent space, leading to a surge of research into its interpretation and manipula- tion. By using an auxiliary classifier, a simple approach consists in finding linear boundaries in the latent space sep- arating binary attributes [18, 42, 59], which allows to edit attributes by \u201cwalking\u201d in the orthogonal latent direction. StyleFlow [1] proposes a non-linear approach by learning the latent transformations using normalizing flows. Other methods [23,50] operate without a pre-trained classifier and find the transformations in an unsupervised manner, requir- ing a manual labelling process to interpret and annotate the \u201cdiscovered\u201d transformations. Rather such restricted sets of possible edit dimensions, we target more general transfor- mations described by free-text. Semantic alignment with CLIP. To align images and text, CLIP [41] learns encoders that map both modalities to a shared latent space in which they can easily be compared and combined. Vision encoders are based on ResNets [20] and Vision Transformers [13]. CLIP, trained on 400M web-crawled image/text pairs with a simple contrastive InfoNCE loss [48], can provide a robust differentiable signal for image synthesis and editing, used in conjunction with diffusion models [32], and vec- tor strokes generators [16]. Similarly to us, StyleCLIP [40] transforms images based on text queries via alignment in 2+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Latent variable Image Encoder Embedding loss Image query I0 Source text S Target text T Cat Tiger Multimodal EncoderImage Decoder Multimodal Target point Perceptual loss Transformation Query 2: Optimization Loop 1: Initialization Optimized via gradient descent Latent loss Figure 2. FlexIT optimization framework: components involving the multimodal latent space colored in green; those involving the image latent space in yellow; those involving the LPIPS distance in pink. Given a transformation query (I0, S, T ), we first compute a target point Pin the multimodal embedding space, and we encode I0in the image latent space to get z0. Then, for a fixed number of steps, we update the latent variable z(initialized with z0) to get closer to the target point P. We add two regularization terms: the LPIPS perceptual distance between the input image and the output image, and a latent distance between zandz0. All networks are frozen, only zis updated. CLIP\u2019s latent space. However it relies on the latent space of StyleGAN2 to optimize the image, which requires training a separate generative and latent space inference model per application domain. 3. FlexIT framework for semantic editing An overview of our image transformation approach is depicted in Figure 2. It relies on three pre-trained com- ponents. First, we edit the input image in a latent space, with the requirement that a wide range of images can be encoded and decoded back to an RGB image with minimal distortion. We chose the VQGAN autoencoder [15] for that purpose. Second, we embed the text query and input image in a multimodal embedding space, to define the optimiza- tion target for the modified image. We use the CLIP [41] multimodal embedding spaces. Finally, to ensure that the modified image remains similar to the input, we control its distance to the input image with the LPIPS perceptual dis- tance [56] computed with a VGG [44] backbone. Optimization scheme. The core idea of the FlexIT method is to edit the input image in a latent space, guided by a high-level semantic objective defined in the multimodal em- bedding space. Let Ebe the image encoder, Dthe image decoder and (Ct, Ci)the multimodal encoders for text and image respectively. Given an input image I0and a textual transformation S\u2192T, we first initialize FlexIT by com- puting the initial latent image representation as z0=E(I0) and the target multimodal point Pas P=Ct(T) +\u03bbICi(I0)\u2212\u03bbSCt(S). (1) We choose to use a multimodal embedding space since it allows text and image modalities to be combined together in a meaningful way: semantic transformations defined by textual embeddings can be applied to images with linear op- erations [25]. In this context, our target point Pcan be seen as an image embedding that has been semantically modi- fied with textual embeddings, by removing the source classinformation ( \u2212\u03bbSEt(S)) and adding the target class infor- mation ( +Et(T)). Since we don\u2019t know what is the optimal linear combination of image and text embeddings, we con- sider \u03bbIand\u03bbSas parameters which will be validated on our development set. To find an output image which, when encoded in the multimodal embedding space, gets as close as possible to the target point, we optimize the embedding loss: Lemb(z) =\u2225Ci(D(z))\u2212P\u22252 2. (2) We add two regularization terms to the embedding loss, to encourage that only the content related to the transforma- tion query is changed. Without regularization, the optimiza- tion scheme can alter any part of the image if this helps in getting closer to the multimodal target point, which we have found to yield unnatural artifacts. The distance to the input image I0is controlled with a LPIPS distance: Lperc(z) =dLPIPS (D(z), I0). (3) To enforce staying in parts of the latent space that are well decoded by our image decoder, we use a regularization term with respect to the initial latent code z0. We use a \u21132 norm at each spatial position iof the latent code, and sum these norms across spatial positions to obtain the loss: Llatent (z) =X i\u2225zi\u2212zi 0\u22252. (4) This\u21132,1loss encourages sparse zichanges, i.e.limiting changes in spatial locations, which is aligned with our ob- jective to transform a localized part of the input image. Finally, note that \u03bbIin Eq. (1) also acts as a regulariza- tion parameter, by encouraging the input and output image to be close in the multi-modal embedding space. The total loss we optimize can be written as: Ltotal(z) =Lemb(z) +\u03bbpLperc(z) +\u03bbzLlatent (z).(5) 3+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Data Augmentations Concat Encode in embedding spaces CLIP ViT-B/32 CLIP RN50 CLIP RN50x4 Figure 3. Architecture of our robust CLIP-based image encoder, which combines three different encoders by concatenation. After initialization, the latent image variable zis updated via gradient descent with a fixed learning rate \u00b5for a fixed number of steps N, while keeping all network weights frozen. Following the implementation of the Fast Gradient Method [12], we normalize the gradient before the update. Image optimization space. The distance to the multi- modal target point is a differentiable loss that can be op- timized via gradient descent. A straightforward approach consists in performing gradient descent directly in the pixel- space. However, this type of image representation lacks a prior on low-level image statistics. By optimizing over a la- tent variable instead, the image is obtained as the output of a neural-network based decoder. Choosing an autoencoder, like that of VQGAN, lets us (i) make use of the decoder\u2019s low-level priors, which guides the optimization problem to- wards images that exhibit at least low-level consistency; and (ii) encode and decode images in its latent space with lit- tle distortion. The spatial dimensions in the VQGAN la- tent space allows to edit specific parts of the image inde- pendently, contrary to GANs which typically rely on more global latent variables. Although GANs generate realistic images with stronger priors, it is problematic to optimize their latent space for two reasons: first, GANs work well on narrow distributions (such as human faces), but do not work as well when trained on a much wider distribution; second, even with a GAN trained on a wide distribution such as that of ImageNet, it is hard to faithfully reconstruct an image using its latent space. We report on experiments with optimization over raw pixels and GAN latent spaces in Section 4.3. Implementation details. In FlexIT, we run the optimiza- tion loop for N= 160 steps, which we found enough to transform most images. We use a resolution of 288 for en- coding images with VQGAN, which compresses the images in a latent space with dimensions (256, 18, 18). We take advantage of various pre-trained CLIP mod- els, and combine their embeddings with concatenation, asshown in Figure 3. By default, we use three image embed- ding networks with different ResNet and ViT architectures, which implement complementary inductive biases. To en- code an image with a single CLIP network, we average the embeddings of multiple augmentations of the input image (8 by default). We have empirically observed that using multiple augmentations per network stabilizes optimization in the early stages. For the regularization coefficients, we use \u03bbz= 0.05, \u03bbp= 0.15, \u03bbS= 0.4, \u03bbI= 0.2as our default values. These coefficients are set using our ImageNet-based devel- opment set, and are fixed for all experiments. These implementation choices are analysed in Sec. 4.4. 4. Experiments Below, we first describe our evaluation protocol in detail. We then present qualitative and quantitative results, and an in-depth analysis of various components of our approach. 4.1. Evaluation Protocol Evaluation dataset. We did not find a satisfying evaluation framework to study the problem of semantic image translation: existing dataset and metrics focus on narrow image domains, or ran- dom text transformation queries [37,40]. To overcome this, we have decided to build upon the ImageNet dataset [9] for its diversity and its high number of classes: by defin- ing which class labels can be changed into one another (like cat\u2192tiger ), we can build a set of sensible object-centric transformation queries. We have selected a subset of the 273 ImageNet labels that we manually split into 47 clusters according to their semantic similarity. For instance, there is a cluster containing all kinds of vegetables. Details on the subset selection and grouping are presented in the ap- pendix. We only consider transformations S\u2192Twhere SandTare in the same cluster, in order to avoid nonsen- sical transformations between unrelated objects, e.g. laptop \u2192butterfly. For each target label Twe construct eight transforma- tion queries by randomly sampling eight other classes {Si} within the same cluster, and sample a random image from eachSifrom the ImageNet validation set. This gives a total of 2,184 transformation queries that we split into a devel- opment set and a test set of equal size. We use the develop- ment set to tune various hyper-parameters of our approach, and report evaluation metrics on the test set. Metrics. We evaluate the success of the transformation by means of the Accuracy of an image classifier, which is possible since we use ImageNet class labels as the transformation targets. We use a DeiT [46] classifier, which has an ImageNet 4+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Input Image Output Image Dataset Image Lifeboat \u2192 Fireboat Beagle \u2192 Otterhound Soccer ball \u2192 Ping pong ball Broccoli \u2192 Spaghetti squash Indigo bunting \u2192 Junco Monarch \u2192 Sulphur butterfly Tiger beetle \u2192 Ladybug Text Query Steel drum \u2192 Marimba (a) (b) (c) (d) (e) (f) (g) (h)Figure 4. Transformation examples with FlexIT on ImageNet images. Columns (a)-(e) show examples of successful transformations. Column (f) shows an interesting behavior where another object has been added in the image to add more context (a table tennis racket in the hand of the person). The last two columns show the most frequent modes of failure: only part of the input object is transformed (g), or parts of the input object that should be changed are not changed: in column h, the transformed images still has a broccoli shape with green parts instead of an orange and round spaghetti squash). validation accuracy of 85.2%. We judge a transformation successful if, for the transformed image, class Thas the highest probability among the 273 selected classes. To assess naturalness of transformed images, we use the Fr\u00b4echet Inception distance (FID) [21]. To avoid numeri- cal instability related to estimating the feature distribution with a small number of samples, we use the \u201cSimplified FID\u201d ( SFID ) [31] which does not take into account the off- diagonal terms in the feature covariance matrix. In addition to the SFID, we use a class-conditional SFID score ( CS- FID) which is an average of the SFID scores computed for each target class separately.1Because FlexIT: Towards Flexible Semantic Image Translation Guillaume Couairon Facebook AI Research gcouairon@fb.comAsya Grechka Meero asya.grechka@meero.comJakob Verbeek Facebook AI Research jjverbeek@fb.com Holger Schwenk Facebook AI Research schwenk@fb.comMatthieu Cord LIP6 matthieu.cord@lip6.fr Tiger \u2192 White wolf Standard schnauzer \u2192 Yorkshire terrier Black \u2192 Red Glass \u2192 Water jug Plastic bag \u2192 Backpack Sow\u2019s ear \u2192 Silk purse Figure 1. FlexIT transformation examples. From top to bottom: input image, transformed image, and text query. Abstract Deep generative models, like GANs, have considerably improved the state of the art in image synthesis, and are able to generate near photo-realistic images in structured domains such as human faces. Based on this success, recent work on image editing proceeds by projecting images to the GAN latent space and manipulating the latent vector. How- ever, these approaches are limited in that only images from a narrow domain can be transformed, and with only a limited number of editing operations. We propose FlexIT, a novel method which can take any input image and a user-defined text instruction for editing. Our method achieves flexible and natural editing, pushing the limits of semantic image translation. First, FlexIT combines the input image and text into a single target point in the CLIP multimodal embedding space. Via the latent space of an autoencoder, we iteratively transform the input image toward the target point, ensuring coherence and quality with a variety of novel regulariza- tion terms. We propose an evaluation protocol for semantic image translation, and thoroughly evaluate our method on ImageNet. Code will be made publicly available.1. Introduction The old saying goes: \u201cYou can\u2019t make a silk purse from a sow\u2019s ear. \u201d Or can you? Truly flexible and powerful se- mantic image editing is elusive, and current work is limited in terms of possible input images and edit operations. Re- search in deep generative image models has seen significant progress in recent years, with GANs in particular generating near photo-realistic samples in domains such as human and animal faces [27] or object-centric images [4]. Moreover, recent \u201cstyle-based\u201d GANs, like StyleGAN [28\u201330], have an impressively disentangled latent space, where perform- ing copy-pastes between two latent vectors transfers the cor- responding styles in the image space. Consequently, significant research efforts have been put into using pre-trained GANs for semantic image edition. Through specific latent-space manipulation, high-level at- tributes such as age or gender can be identified and edited in a realistic manner [1,23,42,59]. These approaches, how- ever, present several caveats. First, contrary to generated latents, inferred latent codes representing real images have been shown to react poorly to latent editing operations [19]. Although recent methods [19, 47, 57] improve editability, 1+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 input images are still highly limited to the distribution of the generative network. Moreover, edit operations are also limited to the semantics identified in the latent space via a pre-trained classifier [1,42,59] or through a semi-automatic manner [23, 50]. These semantics are specific to the single domain the GAN was trained on, such as age or apparent gender in the case of faces. Some flexibility w.r.t. the input images can be obtained by training a GAN to directly mod- ify the images, known as image-to-image translation. These methods learn a transformation between two domains, using paired data [24, 39, 51] or unpaired data [7, 58]. However, these models only learn a single transformation, or combi- nations thereof [52], specific to the training data, limiting the scope of their applicability. We tackle these challenges with a unified framework which modifies an input image based on a user-defined text query of the form (S\u2192T), like cat\u2192dog. For this se- mantic image translation task, the goal is to make mini- mal image modifications while transforming the image as requested. We leverage CLIP [41], which combines text and image representations in one powerful multimodal em- bedding space. This space is used to define our target point, based on the embeddings from the user input. We perform a per-image optimization procedure, using specific strategies to ensure image quality and relevance to the transformation query. Our method requires only fixed pre-trained compo- nents, and can thus be used off-the-shelf without requiring any training. The image is optimized in the latent space of an auto-encoder, rather than a GAN, which greatly enlarges the scope of possible input images. This allows for truly flexible image edits; as Fig. 1 shows, even a sow\u2019s ear can be changed into a silk purse. We propose an evaluation protocol for the task of seman- tic image translation. Evaluation is based on three crite- ria: (i) the transformed image should correctly correspond to the text query, (ii) the output image should look natural, and (iii) visual elements irrelevant to the text query should remain unchanged. We thoroughly evaluate our model on ImageNet, and demonstrate quantitatively and qualitatively the superiority of our method against baselines, broadening the horizon of text-driven image editing. 2. Related Work Image editing. Deep generative networks, like GANs, have given rise to numerous image editing applications, ranging from photography retouching [43], image inpaint- ing [54], object insertion [17], domain translation [55, 58], colorization [24], super-resolution [26, 36], among many others. Automatic user-driven image editing aims at provid- ing the user control to modify an image, by tweaking seg- mentation masks [38], scene graphs [10], or class labels [6]. Allowing the user to provide unstructured free-form textqueries is more challenging. Close to our objective, Mani- GAN [37] aims at performing text-based edits by training a model to refine the details of an image based on its tex- tual description. Their quantitative evaluation protocol uses transformation queries on the COCO dataset by consider- ing unaligned (image, caption) pairs, resulting in possibly incoherent transformation queries. We carefully design our evaluation protocol to avoid such cases. Image latent space. While GANs are highly effective as generative models, inference of the latent variable given an image is intractable. Even though joint learning of an in- ference network has been proposed, see e.g. [11, 14], the mode-seeking training dynamics of GANs are not suited for good reconstruction performance beyond the training distri- bution (or even within it, if modes are dropped). Variational autoencoders [33], on the other hand, offer an inference net- we compute these scores with a low number of examples for many classes, the CSFID score has a high bias, low variance profile on our dataset [8], and we have found it to be reliable and stable. The CSFID metric is a measure of both image quality and transformation accuracy, as it measures the feature distribu- tion distance between the transformed images and the refer- ence images from the target class in the training set. Editing should not change parts of the image that are irrelevant to the transformation defined in the text, e.g. the background. We use the LPIPS perceptual distance [56] to measure de- viation from the input image. It is a weighted \u21132distance of deep image features, and has been demonstrated to corre- late well with human perceptual similarity. During training, we used the LPIPS distance using VGG features rather than AlexNet, so as to reduce bias in the evaluation results. The LPIPS distance cannot differentiate between edits that are relevant to the text query, and those which are not; and we don\u2019t know the minimal LPIPS distance between an image and its closest successful transformation. Still, we argue 1Referred to as within-class FID in [3].that it should be as low as possible. More details on the metrics are presented in appendix. 4.2. Results Qualitative results of FlexIT transformations on Ima- geNet images are presented in Figure 4, including success- ful transformations as well as several failure cases. To demonstrate the generality of our approach, we also show examples of color transformations for images from the Stanford Cars dataset [34] in Figure 5. Semantic image translation is inherently a trade-off be- tween having the most relevant and natural output image (as measured by Accuracy, CSFID and SFID), while stay- ing as close as possible to the input image (as measured by LPIPS). We consider two extreme configurations as base- lines, which only optimize one of these two criteria: (i) The COPY baseline, which simply copies the input image with- out any modification, and (ii) the R ETRIEVE baseline that outputs a random validation image labelled with the target class T. We add the E NCODE baseline that simply passes the input image through the VQGAN autoencoder. We also evaluate StyleCLIP [40], the most relevant text- driven image transformation algorithm from the literature. We consider the version most similar to our method that embeds images with an ImageNet-trained StyleGAN2,2and iteratively updates the StyleGAN2 latent representation to maximize the similarity with a given text in the CLIP latent space. We have also trained ManiGAN [37] on ImageNet with the official implementation. 2We used the publicly available model from https://github. com/justinpinkney/awesome-pretrained-stylegan2 , and train our own e4e encoder [47] to embed images into this latent space. 5+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Yellow \u2192 Orange Grey \u2192 Blue Red \u2192 Orange Grey \u2192 Yellow Red \u2192 Grey Figure 5. Example transformations on the Cars dataset: input images (first row), FlexIT results (second row), StyleCLIP re- sults based on a StyleGAN2 backbone pre-trained on LSUN Cars. dataset (last row). Although GAN-based images have better de- tails like the wheels, they are farther away from the input images. LPIPS \u2193Acc.% \u2191CSFID \u2193SFID \u2193 COPY 0.0 0.45 106.0 0.20 ENCODE 17.5 1.6 107.5 2.99 RETRIEVE 72.4 90.6 27.2 0.23 ManiGAN [37] 21.7 2.0 123.8 17.0 StyleCLIP [40] 33.4 8.0 146.6 35.8 FlexIT (Ours) 24.7 51.3 57.9 6.8 Table 1. Evaluation of FlexIT and baselines on ImageNet images. Results are reported in Table 1. As expected, the copy baseline is ideal on LPIPS and SFID, but fails to adapt to the transformation target T, and thus fails on Accuracy and CSFID. The auto-encoding baseline fails on Accuracy and CSFID for the same reason, but demonstrates the non-trivial impact of using the VQGAN latent space on LPIPS and SFID. The R ETRIEVE baseline provides ideal metrics for Accuracy, CSFID and SFID, as it returns natural images of the target class. It fails on LPIPS, however, since the output image is unrelated to the input. Our FlexIT approach combines a low LPIPS (24.7 vs. 17.5 for E NCODE ) with an accuracy of 51.3% and a CS- FID of 57.9, which is closer to the CSFID of R ETRIEVE (27.2) than that of E NCODE (107.5). The StyleCLIP scores are poor, with high SFID and CSFID scores which was ex- pected as StyleCLIP has been designed to work well where GANs shine. The StyleGAN2 model we use, trained on Im- ageNet, is agnostic to class information and cannot synthe- size realistic images for all ImageNet classes. ManiGAN works well when trained on narrow domains with color change transformation requests, but we find that it does not produce convincing edits when trained on ImageNet. To provide insight into which transformations work well, and which less so, we group the ImageNet clusters into 13 bigger groups (see appendix for details) and report the av- erage CSFID and failure rate (1 - accuracy) scores for each 0 50 100 CSFIDbird fungus dog mammal vehicle insect sea life reptile edible music instr. container device object 0% 50% 100% Failure RateFigure 6. Groupwise CSFID and Failure Rate (1-Accuracy), lower is better for both metrics. Dark colors: best possible values ob- tained with R ETRIEVE baseline; medium colors: scores obtained with FlexIT; light colors: values obtained with C OPY baseline. 20 25 30 35 40 45 Mean LPIPS distance60708090100110CSFID 8 3264160 8 32 160160 3264 1608 32 160Without LPIPS (\u03bbp= 0.15) Latent (\u03bbz= 0.05) CLIP (\u03bbI= 0.2) All Figure 7. CSFID obtained without regularization, with individual LPIPS, Latent and CLIP regularizers, and using all. Each curve corresponds to 160 steps of optimization on the dev. set. group in Figure 6. Generally, transformations among natu- ral objects are more successful than transformations among man-made objects. We believe that this is mostly because the latter appear in a wider variety of shapes and contexts which leads to more difficult transformations. 4.3. Ablation studies Regularizers. In Figure 7, we show the evolution of CSFID along the optimization steps, where we consider our method without regularization, with each regulariza- tion scheme separately, and with all regularizers (default configuration). Compared to not using regularization, the LPIPS regularization substantially improves the CSFID score along the optimization path, while also reducing LPIPS as expected. The CLIP regularizer has a similar ef- fect, but is able to reduce CSFID further while the LPIPS distance is only slightly reduced compared to our method without any regularization. Using all regularizers allows us to obtain the lowest CSFID scores at low LPIPS. We be- lieve that these two regularizers are complementary: while 6+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Input Image No regularization Only LPIPS regularization Main Method Figure 8. Example transformations with different regularizers. Textual queries from top to bottom: Rottweiler \u2192German shep- herd, Electric guitar \u2192Banjo, Red wolf \u2192Grey fox. ViT RN ViT + RNViT + 2 RN2 ViT + 3 RN405060708090100CSFIDNo aug. 1 aug. 8 aug. 32 aug. Figure 9. CSFID for different CLIP networks combinations and number of data augmentations options. Default setting: ViT+2RN. the LPIPS loss mitigates image deviation for local features, the CLIP loss provides semantic guidance which helps to reconstruct recognizable objects. Corresponding qualitative examples are shown in Figure 8. CLIP embedding module. We study how different choices of CLIP image encoders impact the CSFID score. Our default configuration involves two ResNet-based net- works and one ViT-based network to embed the image in the CLIP space. We experiment with a single ViT or ResNet, a combination of ViT with a single ResNet, and also us- ing all available pre-trained CLIP networks, which com- prises a ViT-B/16, a ViT-B/32, a ResNet50, ResNet50x4 and ResNet50x16, see [41] for details on the modules. For each CLIP network configuration, we experiment with ei- ther not using data augmentation, or using d\u2208 {1,8,32} augmentations. We apply basic geometric augmentations that are commonly used to train image classification net- works (more details in appendix). Each of the NnetsCLIP 10 20 30 40 50 60 Mean LPIPS distance6080100120140CSFID 8 32 1608 321608 32 160832 160VQGAN ICGAN Pixels StyleGAN2Figure 10. CSFID and LPIPS scores across iterations, using dif- ferent latent spaces, or raw pixels, for optimization. networks sees a different augmentation in each of the Nsteps steps of the optimization process, resulting in a total of d\u00d7Nnets\u00d7Nstepsaugmentations of the input image. From the results in Figure 9, we see that while the ViT and ResNet embedding networks lead to similar results, they are complementary and combining them leads to a sub- stantial improvement. Adding additional networks leads to further improvements. Second, using data augmentation is very beneficial, and leads to a reduction in CSFID of 10 or more points for all network configurations. Using more than one augmentation does not improve results substantially: it suffices to a different augmentation for each network at each optimization step. In our other experiments we use the three smallest (and fastest) CLIP networks as our default setting. Image optimization space. We compare our choice of op- timizing in the VQGAN latent space with using the latent spaces of StyleGAN2 [30] and IC-GAN [6], as well as op- timizing directly in the pixel space. IC-GAN [6] generates images similar to an input image, and uses a latent variable to allow for variability in its output. As IC-GAN does not offer direct inference of the latents for a given image, we take 1,000 samples from the latent prior, and keep the one yielding minimal LPIPS distance to the input image. We found that optimization to further reduce the LPIPS w.r.t. the input image from this point on was not effective. For StyleGAN2 [30], we use the same network pre-trained on ImageNet as we used for StyleCLIP. To embed the evalu- ation images into this latent space, we first obtain an ini- tial prediction of the vector with the e4e encoder [47], as in StyleCLIP, and then perform an additional 1,000 optimiza- tion steps to better fit the input image, following the GAN inversion procedure described in [29]. The results in Figure 12 show that using the VQGAN la- tent space allows to substantially decrease the CSFID score along the iterations, while only slightly increasing LPIPS. Using the raw pixel space is not effective to decrease the CSFID. IC-GAN has relatively good image synthesis abil- ities but it is hard to faithfully encode images in its latent space, yielding high LPIPS scores above 50. The Style- GAN2 latent space ( W+) is bigger, allowing generated im- 7+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 22 23 24 25 26 27 28 Mean LPIPS distance54565860626466CSFID \u03bbp= 0.10.2\u03bbS= 0 0.5\u03bbS= 0.8 \u03bbI= 0 \u03bbI= 0.1\u03bbI= 0.4 \u03bbz= 0\u03bbz= 0.1\u03bbp \u03bbS \u03bbI \u03bbz 23 24 25 26 27 28 29 Mean LPIPS distance405060Accuracy \u03bbp= 0.05 \u03bbp= 0.1 \u03bbp= 0.2 \u03bbS= 0\u03bbS= 0.2\u03bbS= 0.5\u03bbI= 0 \u03bbI= 0.1 \u03bbI= 0.4\u03bbz= 0 \u03bbz= 0.1 \u03bbp \u03bbS \u03bbI \u03bbzFigure 11. Effect on CSFID and Accuracy of hyper-parameters; default settings represented by the black dot, where", "meta_data": {"split_id": 2}, "response": "Educational score: 32", "prediction": 32}
{"text": "", "meta_data": {"split_id": 1}, "response": "Since you didn't provide the extract, I won't be able to evaluate its educational value. Please provide the extract, and I'll be happy to help you assess its educational value and provide a score using the 5-point system.\n\nOnce you provide the extract, I'll evaluate it and justify my total score within the 100-word limit. I'll then conclude with the score in the format \"Educational score: <total points>\".", "prediction": -1}
{"text": "", "meta_data": {"split_id": 1}, "response": "I apologize, but there seems to be no extract provided. Please provide the extract, and I'll be happy to evaluate its educational value and score it using the additive 5-point scoring system.", "prediction": -1}
{"text": "", "meta_data": {"split_id": 1}, "response": "I apologize, but it seems that you didn't provide the extract. Please provide the text, and I'll be happy to evaluate its educational value and score it using the additive 5-point system.\n\nOnce you provide the extract, I'll assess its relevance, coherence, and suitability for educational use, and assign a score from 1 to 5. I'll then justify the total score in a brief statement and conclude with the final score in the format \"Educational score: <total points>\".", "prediction": -1}
{"text": "565 Real-Sim pairs 49.7ha Table 1: Benchmark datasets statistics. 4. Benchmark datasets We introduce two large-scale sim-to-real benchmark datasets to exemplify the utility of the TOPO-DataGen. Dataset statistics. Table 1 shows the statistics of the bench- mark datasets, which are distributed across two different landscapes: Urbanscape andNaturescape . For each land- scape, we provide data in three scenes: LHS-sim ,In-place andOut-of-place , all of which come with synthetic 2D im- ages, multimodal 3D labels and semantic map as depicted in Figure 2. Specially, the In-place andOut-of-place scenes include accurately geo-tagged real photos captured by a DJI drone equipped with the cm-level real-time kinematics (RTK) positioning sensors [2]. The In-place scene is highly overlapped with the LHS-sim scene, while the Out-of-place scene describes a neighboring but non-overlapping environ- ment w.r.t LHS-sim . Figure 3 shows the 3D textured models with camera position distributions of the datasets. In the proposed datasets, the data resolution for all modalities, including real and synthetic images, is set to 480 px in height and 720 px in width. The semantic labels have seven classes: sky, ground, vegetation, building, wa- ter, bridge, and others. Following [28], we evaluate the ac- curacy of the synthetic 3D labels by coordinate reprojecting error. The mean absolute reprojection error of the Urban- scape and Naturescape datasets are respectively 1.19 px and1.04 px, showing high accuracy for outdoor aerial datasets. More error analysis can be found in the supplement. Dataset splits. We randomly split the In-place andOut-of- place scene data into training (40%), validation (10%) and testing (50%) sections. As for the LHS-sim scene data, it is split into training (90%) and validation (10%) sets. We in- tentionally formulate a challenging visual localization task by using more real data for testing than for training to bet- ter study the real data scarcity mitigation. Please also note that the real image density indicated in Table 1 is lower than many available outdoor city street-based visual localization datasets such as Cambridge [31] and Aachen [48, 49]. 5. CrossLoc localization In this section, we present CrossLoc, an absolute lo- calization algorithm leveraging the cross-modal visual rep- resentations for enhanced robustness and data efficiency. CrossLoc learns to localize the query image by predicting its scene coordinates using a set of cross-modal encoders, followed by camera pose estimation using a PnP solver. Coordinate-depth-normal geometric hierarchy. For an image Iwith ground-truth scene coordinates Zand cam- era pose (R,P)\u2208SE(3)in the world coordinate system, it is straightforward to compute the Euclidean or z-buffer depthDvia homogeneous transformation. Subsequently, the surface normal Ncan be obtained from the depth map D[42, 73]. The geometrical information richness of the scene coordinates Zlabel is beyond that of depth Dand surface normal N, as one can convert the first to the latter without loss of accuracy. We hypothesize that if a neural network is capable of pre- dicting perfect scene coordinates Z, it possesses sufficient information to estimate high quality depth Dand surface normal N. Inspired by the natural geometric hierarchy, we propose to learn scene coordinate regression with auxiliary 4+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 self-supervision tasks on depth and surface normal estima- tion. Our method is modular and also extends to using any external labels such as semantics Sif they may be available. Consider an encoder-decoder network making predic- tionsTq\u2208 {Z q,Dq,Nq}from a query image Iq. Let fT(\u00b7), gT(\u00b7), hTdenote the encoder, decoder and the mid- way representations respectively: hT q=fT(Iq), \u02c6Tq=gT(hT q).(1) where \u02c6Tqis the network prediction. Based on the coordinate-depth-normal geometric hierarchy, we argue that the representations for scene coordinate hZ qmay en- code richer information than the others, e.g., hD q, hN q. To better ensure the hierarchical consistency, we propose to explicitly learn a cross-modal scene coordinate regres- sion representation hZ qwith those of lower-level tasks: hZ+ q=fZ+(hZ q, hD q, hN q). (2) where fZ+is a projection head for representations aggre- gation. Next, the cross-modal representation is fed into the primary task decoder for scene coordinate regression: \u02c6Z+ q=gZ(hZ+ q). (3) Afterward, the 6D camera pose could be computed using the coordinates prediction via a PnP solver, as per the stan- dard scene coordinate regression localization methods. Training objective. LetNbe the number of pixels in the image. We use azimuth \u03b8and elevation \u03d5to repre- sent the surface normal vectors as in [35], and therefore: Z \u2208RN\u00d73,D \u2208RN\u00d71,N= [N\u03b8,N\u03d5]\u2208RN\u00d72. The maximum likelihood estimation loss [30, 72] is used to sta- bilize the coordinate, depth and surface normal regression training. The predicted values come with isotropic noise estimation: [\u02c6T, sT] =gT(hT), where sT\u2208RN\u00d71 >0is the pixel-wise uncertainty map. We adopt a two-step training method to regularize the scene coordinate representations hZ+. First, the encoder-decoders fT, gT(T \u2208 {Z ,D,N}) are separately trained with their own loss functions: LZ=NX i=1 \u2225\u02c6Z(i)\u2212 Z(i)\u22252 2 2(sZ(i))2+ 3log(sZ(i))! , LD=NX i=1 \u2225\u02c6D(i)\u2212 D(i)\u22252 2 2(sD(i))2+log(sD(i))! , LN=NX i=1 \u2225Lsn(\u02c6N(i),N(i))\u22252 2 2(sN(i))2+ 2log(sN(i))! .(4) where Lsn(\u02c6N(i),N(i)) =\u2225\u02c6N\u03d5(i)\u2212 N\u03d5(i)\u22251+ 2min\u0010 \u2225\u02c6N\u03b8(i)\u2212 N\u03b8(i)\u22251,1\u2212 \u2225\u02c6N\u03b8(i)\u2212 N\u03b8(i)\u22251\u0011Encoders CrossLoc CrossLoc-SE CrossLoc-CO Coordinate \u2713 \u2713 \u2713 Depth \u2713 \u2713 Surface normal \u2713 \u2713 Semantics \u2713 Table 2: Variants of the proposed CrossLoc algorithm. is the circle loss defined in [35]. The subscript in Eq. (4) denotes the loss function for the corresponding task T. Fol- lowing [17], we also implement the reprojection loss for coordinate regression loss LZ. Lastly, we employ the non-coordinate encoders as frozen representation extractors to fine-tune the coordinate encoder-decoder fZ, gZand the projection head fZ+: Lcrossloc =LZ(fZ, gZ, fZ+). (5) The CrossLoc applies the multi-task learning by re-using the representations of geometrically-related tasks to im- prove the original objective of coordinate regression. Ta- ble 2 shows some variants of the proposed methods, among which the vanilla CrossLoc adopts the self-supervised cross-modal representation learning without any additional data. Our method is flexible with external labels such as semantics, e.g., the CrossLoc-SE. 6. Experiments Extensive experiments have been carried out to evalu- ate the performance of CrossLoc against the state-of-the-art approaches to scene coordinate regression. Specifically, we apply various CrossLoc architectures in Table 2 to validate the effectiveness of self-supervision. Systematic ablation studies about the efficacy of synthetic training data on real data scarcity mitigation are also conducted. Network architecture. Following [17], we adopt a fully convolutional network (FCN) query image pixels and the 3D coordi- nates of the scene model [13, 14, 15, 16, 17, 44, 45, 47, 57, 66, 69]. The camera pose can then be computed using a PnP solver with RANSAC optimization [17, 25, 34]. Compared with the 2D image-based approaches, the structure-based methods deliver more competitive performance [28, 45], but at the cost of attaining accurate 3D models for the underly- ing scenes, which is non-trivial and may require special- ized engineering efforts [45]. The conventional descrip- tor matching methods, such as Active Search [47], typi- cally create the 2D-3D correspondences via structure-from- motion (SfM) reconstruction and achieve state-of-the-art lo- calization accuracy. However, the descriptor detector is prone to fail in repetitive or texture-less scenes or complex environments with entangling structures [55, 62]. Recently, the learning-based scene coordinate regression methods have been proposed [13, 14, 15, 16, 17, 36, 72] to predict the 2D-3D matches using neural networks in an end-to-end manner. Thanks to the great capacity of the neu- ral network, it achieves state-of-the-art performance across various tasks [17]. The regressor learns from the ground- truth scene coordinates and can be theoretically separated from the SfM reconstruction. Most existing methods di- rectly learn the mapping from image to coordinates while ignoring the rich geometry information contained in the 3D coordinate labels, e.g., one could compute camera pose and depth from scene coordinates given camera intrinsics. 2+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Urbanscape Naturescape Scene Coordinates Depth Normals Semantics RGB - Real RGB - Synthetic Figure 2: Examples of synthetic modalities generated with TOPO-DataGen. Our workflow traverses between the real and virtual world seamlessly hinged on the camera pose: from the leftmost real geo-tagged photos, we can generate a rich set of matching synthetic modalities, including synthetic RGB image, scene coordinates, depth, surface normal, and semantics. Sim-to-real datasets have become increasingly important to tackle the issue of real data scarcity or to augment the real-world datasets during training. A set of synthetic datasets have been proposed and employed to facilitate model training for better robustness and efficiency [19, 22, 27, 41, 43, 51, 54, 60, 70]. [70] proposes the attend-remove- complete (ARC) method for sim-to-real depth prediction, which learns to identify, remove and fill in some challenging regions in real images as well as to translate the real images into the synthetic domain. Closer to our work, [22] pro- vides a toolkit for mid-level cues generation from 3D mod- els. However, it is not optimized for geodata and does not support synthetic data generation to match real geo-tagged photos. Likewise, most synthetic datasets or data genera- tion workflows only involve rendered results in the virtual world and cannot traverse well between reality and simu- lation. Our proposed TOPO-DataGen workflow not only generates multimodal virtual data but can also produce geo- referenced reality-matching data to intentionally augment the downstream learning tasks designed by the users. 3. TOPO-DataGen workflow In this section, we describe the TOPO-DataGen toolkit for synthetic data generation as shown in Figure 1 (left). Geodata preprocessing. In the first step, a high-fidelity 3D textured model is generated over the area of interest based on available geographic data, e.g., classified LiDAR point cloud, orthophoto, or digital terrain model. We prepro- cess the off-the-shelf geodata such that it is compatible with the open-source geospatial rendering engine CesiumJS [1], which is at the core of our data generation workflow. For example, one common practice is to convert the coordi- nate reference system into the global WGS84 from the local one. We argue that nowadays, the high-quality open geo- data from national agencies is more and more common [20] such as the swisstopo [6, 7, 8], making it easier to access the 3D models for locations of interest and employ our methodon a large scale. Moreover, the open geodata mostly comes from airborne devices such as satellites. Its accuracy and the top-down view are generally sufficient for aerial local- ization tasks as opposed to street-level localization. Synthetic data generation. The generated 3D textured model in the WGS84 reference system is the input to Ce- siumJS engine for synthetic data generation. Given the vir- tual camera viewpoint, our proposed TOPO-DataGen pro- vides a series of designer modalities related to visual lo- calization, which include: RGB image, scene coordinates in the WGS84 reference system ( i.e., earth-centered-earth- fixed coordinates), depth, surface normal, and semantics. Specifically, through ray tracing [10], our workflow pro- duces synthetic RGB images and the geo-referenced scene coordinates as raw output. Subsequently, we retrieve the semantic maps by matching each pixel to its closet point in the categorized geodata, i.e., the classified LiDAR point cloud. The PyTorch [40] framework is used to accelerate the matrix computation. Lastly, based on the scene co- ordinate labels, we generate the other 3D modalities, i.e., depth and surface normal. Following [68], we provide the z-buffer depth, and the surface normal is computed using Open3D [73]. The data rendering can be performed offline over a large area, the size of which is limited by the computer hard- ware and the available geodata. To generate synthetic data from scratch, e.g., to scan one bounded area, we use the LHS [24, 38] to do minimal yet efficient camera viewpoints sampling. Otherwise, one may use any real geo-tagged pho- tos to generate the matching synthetic modalities as shown in Figure 2 (from the leftmost to the right). Quality control. The quality of the multimodal synthetic data is essentially dependent on the precision of the 3D model geo-referencing and the accuracy of the image pixel- scene coordinate ray tracing. Please see the supplement for a detailed case study on the provided benchmark datasets (introduced in Section 4). 3+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Urbanscape Naturescape Urbanscape Naturescape Figure 3: 3D textured models used to render the benchmark datasets with synthetic modalities via LHS sampling or matching the real flight trajectories. Camera positions of in-place and out-of-place data are colored in green and blue respectively, and the LHS synthetic data sampling boundary is denoted by the orange box. Landscape Scene Size Image style Area UrbanscapeLHS-sim 15000 Sim only 45.8ha In-place 3158 Real-Sim pairs 40.4ha Out-of-place 1360 Real-Sim pairs 46.6ha NaturescapeLHS-sim 30000 Sim only 128.25ha In-place 2114 Real-Sim pairs 107.5ha Out-of-place CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data Qi Yan, Jianhao Zheng, Simon Reding, Shanci Li, Iordan Doytchinov \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne (EPFL) {firstname.lastname }@epfl.ch Latin Hypercube SamplingGeoreferenced images OrthophotosTerrain ModelLiDAR point cloudGeodata TOPO-DataGen ORDecoder Enc. 2 Enc. \ud835\udc41 PnP Solver 6D pose CrossLoc Semantics Depth Scene Coordinates Synthetic RGB Scene Coordinates Depth Normals SemanticsEnc. 3 Enc. 1 Figure 1: Left. The proposed TOPO-DataGen workflow generates multimodal synthetic datasets over large scales using off- the-shelf geodata. Right. We present CrossLoc , a cross-modal visual representation learning method for absolute localization. It learns to predict scene coordinates via self-supervised geometric tasks and external labels such as semantics if available. Abstract We present a visual localization system that learns to estimate camera poses in the real world with the help of synthetic data. Despite significant progress in recent years, most learning-based approaches to visual localization tar- get at a single domain and require a dense database of geo-tagged images to function well. To mitigate the data scarcity issue and improve the scalability of the neural lo- calization models, we introduce TOPO-DataGen, a versa- tile synthetic data generation tool that traverses smoothly between the real and virtual world, hinged on the geo- graphic camera viewpoint. New large-scale sim-to-real benchmark datasets are proposed to showcase and evaluate the utility of the said synthetic data. Our experiments re- veal that synthetic data generically enhances the neural net- work performance on real data. Furthermore, we introduce CrossLoc, a cross-modal visual representation learning ap-proach to pose estimation that makes full use of the scene coordinate ground truth via self-supervision. Without any extra data, CrossLoc significantly outperforms the state-of- the-art methods and achieves substantially higher real-data sample efficiency. Our code and datasets are all available athttps://github.com/TOPO-EPFL/CrossLoc . 1. Introduction Due to vulnerabilities in the reception of satellite posi- tioning (GNSS) signals, on which aerial systems rely for navigation and controls, alternative methods are in demand for absolute large-scale localization. The dead-reckoning navigation systems have improved significantly in recent years; however, residual drift is always a challenge for long-term applications [23, 32]. In contrast, the availabil- ity of small and low-cost cameras has made them popular sensors for capturing information on the surrounding land- 1+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 scape. When features of a known environment are rec- ognized in the captured images, these can be used to de- termine the absolute camera poses. Current state-of-the- art machine-learning-based methods for absolute localiza- tion [13, 15, 17, 44, 45, 57] show promising performance but typically focus on single-domain operation and on be- spoke datasets collected for indoor or outdoor city street localization [39]. However, open-source datasets for posi- tioning airborne platforms or workflows to generate geospa- tial learning data are scarce. This poses a severe barrier in adapting the algorithms to real-world aerial scenarios, as they typically require dense datasets consisting of accu- rately geo-referenced photos in the area of interest [28]. De- veloping inclusive datasets for real-life navigation around the globe and employing state-of-the-art visual algorithms for aerial applications can currently be considered econom- ically and technically difficult, if not unfeasible. In this work, we first present a synthetic data generation scheme called TOPO-DataGen (Figure 1, left), that lever- ages the topographic information to produce geo-referenced data with rich modalities for subsequent training. Given the designated camera poses, this scheme renders the sim- ulated RGB images accompanied by 2D and 3D modali- ties such as semantics, scene coordinates, depth, and sur- face normal. In an area of interest with available geodata, one may adopt a stochastic sampling strategy such as Latin hypercube sampling (LHS) [24, 38] or use real geo-tagged photos to attain the camera viewpoints and create the corre- sponding synthetic labels. The real geo-tagged data can be designer sourced, such as data acquisition by drones or from crowd sourced campaigns [4, 5]. Our method hinges on the geospatial location of the camera viewpoint and traverses between reality and simulation smoothly. To mitigate the data-scarcity issue for learning-based vi- sual localization methods via sim-to-real transfer, we cu- rated two large-scale benchmark datasets using the pro- posed data generation workflow on urban and natural sites. They are comprised of primarily synthetic data and a small fraction of accurately geo-tagged real data, with both sec- tions containing dense 3D and semantic labels. Unlike the existing datasets focusing on localization in a single do- main [28, 31, 48], the provided benchmark datasets show- case and evaluate the use of synthetic data to assist localiza- tion in the real world using significantly less real data. In addition, we introduce a cross-modal visual repre- sentation learning approach CrossLoc (Figure 1, right) for absolute localization via scene coordinate regression. CrossLoc exploits the rich information contained in the scene coordinates through self-supervision to achieve im- proved performance. We start from the scene coordinate ground-truth to impose geometrically less complex pretext tasks such as depth estimation without any extra labels. The visual representations learned from the tightly-coupledtasks [42, 67, 68] jointly improve the downstream coordi- nate regression. We find that this approach consistently out- performs the state-of-the-art baselines in our benchmark. Our main contributions are summarized as follows: 1. TOPO-DataGen: an open-source multimodal synthetic data generation tool tailored to aerial scenes. 2. Large-scale benchmark datasets for sim-to-real visual localization, including synthetic and real images with 3D and semantic labels on urban and natural sites. 3. CrossLoc: a cross-modal visual representation learn- ing method via self-supervision for absolute localiza- tion, which outperforms the state-of-the-art baselines. 2. Related work Absolute visual localization aims at estimating the camera pose of a query image within a known environment. Many attempts have been made to achieve this using purely 2D images. Image retrieval [11, 12, 46, 58, 59] methods rely on the image features to build an explicit localization map. The query image is first matched against a database of reference images with known poses, and its pose w.r.t. to the retrieved image is subsequently refined via approximation or relative localization. Absolute pose regression [18, 29, 31, 63] ap- proaches adopt a neural network to learn implicit map rep- resentations and output the camera pose directly. Structure-based methods aim at identifying the 2D-3D matches between the with ResNet skip layers [26] for regression tasks using a downsampling factor of 8. We split the FCN in the middle to obtain an encoder-decoder structure. Please refer to the supplement for further details. Datasets. Each model is trained using data from the LHS- simand either In-place orOut-of-place scene. We evaluate on the real testing data for each scene and report the results. Training. We first initialize the encoder-decoder networks separately using various training tasks on the LHS-sim data; subsequently, each network is fine-tuned with the paired real-sim data on either In-place orOut-of-place scene. Each network is independently trained using the loss in Eq. (4). Lastly, we fine-tune the coordinator network with loss in Eq. (5). For CrossLoc-SE, we use the cross-entropy loss for semantic segmentation. We use the Adam optimizer [33] and the PyTorch [40] framework for implementation. 5+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 In-place localization accuracy Out-of-place localization accuracy Methods Median error \u2193 Accuracy \u2191 Median error \u2193 Accuracy \u2191 transl. rot. <5m, 5\u00b0 <10m, 7\u00b0 <20m, 10\u00b0 transl. rot. <5m, 5\u00b0 <10m, 7\u00b0 <20m, 10\u00b0 DSAC* [17] 11.6m 6.2\u00b0 15.4% 42.1% 64.4% 14.9m 4.1\u00b0 10.3% 33.1% 59.6% DDLoc 10.3m 2.3\u00b0 24.1% 47.2% 67.8% 42.1m 9.5\u00b0 4.1% 12.8% 26.5% AtLoc [63] 23.0m 1.9\u00b0 1.6% 11.1% 40.6% 45.6m 5.3\u00b0 0.1% 2.4% 14.1% CrossLoc 4.0m 2.1\u00b0 61.1% 85.6% 93.4% 6.0m 1.9\u00b0 39.1% 72.4% 87.8% CrossLoc-SE 3.9m 1.9\u00b0 62.2% 86.7% 94.2% 5.8m 1.8\u00b0 40.1% 73.4% 89.1% CrossLoc-CO 7.6m 3.7\u00b0 27.6% 61.3% 80.5% 7.1m 2.2\u00b0 31.5% 64.9% 82.8% (a) Quantitative comparison over Urbanscape dataset. In-place localization accuracy Out-of-place localization accuracy Methods Median error \u2193 Accuracy \u2191 Median error \u2193 Accuracy \u2191 transl. rot. <5m, 5\u00b0 <10m, 7\u00b0 <20m, 10\u00b0 transl. rot. <5m, 5\u00b0 <10m, 7\u00b0 <20m, 10\u00b0 DSAC* [17] 41.9m 4.8\u00b0 1.8% 11.3% 30.1% 33.1m 4.3\u00b0 0.4% 6.7% 26.5% DDLoc 39.8m 4.2\u00b0 2.2% 10.9% 28.0% 57.4m 8.2\u00b0 2.5% 12.0% 22.6% AtLoc [63] 44.7m 5.1\u00b0 0.0% 1.5% 10.7% 70.8m 6.6\u00b0 0.0% 0.0% 2.5% CrossLoc 16.7m 2.9\u00b0 9.6% 28.4% 59.6% 15.0m 2.7\u00b0 8.1% 29.7% 59.0% CrossLoc-SE 16.0m 2.7\u00b0 11.4% 32.6% 59.7% 14.5m 2.2\u00b0 12.7% 32.5% 59.4% CrossLoc-CO 22.6m 3.8\u00b0 6.2% 20.1% 42.5% 17.1m 2.9\u00b0 7.1% 29.0% 55.5% (b) Quantitative comparison over Naturescape dataset. Table 3: Quantitative results on camera pose estimation. We report the median translation and rotation errors as well as the percentage of correctly re-localized camera poses below error thresholds of 5m/5\u00b0, 10m/7\u00b0and 20m/10\u00b0. Baselines. We compare our proposed algorithms against two state-of-the-art scene coordinate regression baselines: DSAC* [17] and sim-to-real coordinate regression DDLoc adapted from [70]. We follow the ARC [70] architecture to train a regressor for coordinates instead of depth with mini- mum modification; see the supplement for details. For a fair comparison, all the coordinate regression-based approaches utilize the PnP solver from DSAC* [17] to compute cam- era poses. Also, our method is compared with AtLoc [63], a state-of-the-art absolute pose regression (APR) method. Please note that AtLoc does not use any 3D labels and has much less information to learn from during training. 6.1. Results on camera pose estimation Quantitative results. Table 3 lists detailed comparisons for pose estimation on Urbanscape and Naturescape datasets. Our CrossLoc outperforms the other two coordinate regres- sion methods by a clear margin, demonstrating the strength of adoping cross-modal representations. Notably, the APR baseline AtLoc [63] relying on 2D images only shows sub- stantially inferior performance to any of the structure-based methods. It is expected as the APR tends to generalize worse from the training data in practice [50]. We also provide the results of our CrossLoc algorithm using dif- ferent visual representations, i.e., CrossLoc-CO applyingonly coordinate regression features and CrossLoc-SE us- ing additional semantic segmentation features (see Table 2). Adding the auxiliary depth and surface normal visual fea- tures substantially boosts the model performance. However, CrossLoc-SE with external semantic labels has limited per- formance gains compared with the vanilla CrossLoc. This indicates incorporating additional labels with less geomet- rical information may not help the scene coordinate learn- ing prominently. This finding is also in line with the recent works on multi-task learning consistency [42, 56, 67, 68]. The localization accuracy in the Naturescape is generally not as promising as in the Urbanscape. We conjecture that this is due to the lower amount of human-made features with distinctive geometries such as buildings. Moreover, the much lower real training data density, as stated in Ta- ble 1, makes the localization task considerably harder. Qualitative results. Figure 4 shows the pose estimation results of two flight trajectories in the Naturescape and Ur- banscape datasets respectively. Our CrossLoc outperforms other localization algorithms, resulting in far more complete trajectory reconstruction with much fewer outliers. 6.2. Results on scene coordinates regression Quantitative results. For coordinate regression-based lo- calization methods, the final pose estimation performance 6+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Piloted Planned Ground truth DSAC* DDLoc CrossLoc Figure 4: Qualitative comparison of the reconstructed trajectories with the studied visual localization methods. We show the results for a planned trajectory on Naturescape and a human-piloted trajectory on Urbanscape, both derived from the testing data. The rightmost color bar denotes the camera position estimation error in meter. 10mRegression accuracy (Naturescape ) 30m100mIn-place Out-of-place 3mRegression accuracy (Urbanscape ) 10m50mIn-place Out-of-place Figure 5: Percentage of correctly predicted scene coordinates with different methods. The accuracy is represented with error thresholds, 3m, 10m and 50m for Urbanscape as well as 10m, 30m and 100m for Naturescape. Only two accuracy values of DDLoc are given in Urbanscape since there is nearly no predicted coordinate with error lower than 3m. is highly dependent on the coordinate prediction quality. Figure 5 shows the accuracy of regressed scene coordinates under different thresholds. Consistent with the pose esti- mation results in Table 3, the CrossLoc family outperform the DSAC* and DDLoc baselines by a substantial mar- gin. Moreover, CrossLoc and CrossLoc-SE improve the co- ordinate accuracy over CrossLoc-CO across all scenarios, which further indicates the benefit of leveraging depth and surface normal cross-modal representations. Qualitative results. Figure 6 compares the scene coordi- nate regression errors of different methods. The regres- sion error of our CrossLoc and CrossLoc-SE is significantlylower than the baselines and the plain CrossLoc-CO without self-supervision. Further, we observe a high similarity be- tween the error map of CrossLoc and CrossLoc-SE, which again verifies the limited performance gains by using the extra semantic labels. For discussion on the failure cases, please refer to the supplementary materials. 6.3. Ablation study on real data scarcity mitigation We show the quantitative results on how the real data scarcity issue can be mitigated by: (i) using the synthetic data for augmentation, which could be generated at ease using the proposed TOPO-DataGen, (ii) applying the real 7+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Urbanscape NaturescapeFigure 6: Qualitative comparison of the scene coordinate error map. Pixels without groundtruth labels such as sky are not evaluated. Our CrossLoc family methods estimate the absloute scene coordinates with the least error. MethodsLocalization accuracy w.r.t. amount of synthetic data Real only Pairwise only Full DSAC* [17] 35.5m, 17.7\u00b0 23.7m, 12.1\u00b0 11.6m, 6.2\u00b0 DDLoc - 11.0m, 2.8\u00b0 10.3m, 2.3\u00b0 CrossLoc 7.6m, 3.6\u00b0 4.6m, 2.4\u00b0 4.0m, 2.1\u00b0 CrossLoc-CO 14.6m, 7.0\u00b0 12.8m, 6.2\u00b0 7.6m, 3.7\u00b0 (a) Effects of synthetic data assisting the real-world localization. MethodsLocalization accuracy w.r.t. fraction of pairwise data 25% data 50% data Full DSAC* [17] 33.1m, 17.6\u00b0 19.4m, 10.2\u00b0 11.6m, 6.2\u00b0 CrossLoc 14.1m, 7.1\u00b0 6.8m, 3.5\u00b0 4.0m, 2.1\u00b0 CrossLoc-CO 28.3m, 13.7\u00b0 13.7m, 6.3\u00b0 7.6m, 3.7\u00b0 (b) Comparison of real data sample efficiency. Table 4: Ablation study on the real data scarcity mitigation. Median translation and rotation error is reported. data sample-efficient CrossLoc algorithm. All experiments in this section are carried out on Urbanscape in-place scene. The added value of the multimodal synthetic data is val- idated in Table 4a. We compare the performance of var- ious methods trained with real data only, real-sim paired data only, and real-sim paired data plus the plentiful LHS- sim data. For each method, the best performance is always achieved with the most synthetic data. Our CrossLoc con- sistently performs the best, which indicates the usefulness of the proposed self-supervision via geometric hierarchy. In Table 4b, we evaluate the real data sample efficiencyof different methods, assuming that the paired synthetic data is available. We randomly sample 25% and 50% of the real training data to train each model. Each algorithm performs better given a larger amount of training data, and in compar- ison, the CrossLoc leverages the real data most efficiently. CrossLoc outperforms the 100%-data-trained DSAC* and all CrossLoc-CO when trained with only 50% of available data. The proposed geometrical self-supervision tasks are particularly helpful for learning in the low-data regime. 7. Conclusion We propose TOPO-DataGen, a scalable workflow to generate as much synthetic data as needed to assist real- world localization. Large-scale sim-to-real benchmark datasets are additionally provided to exemplify the use of TOPO-DataGen. Further, we present CrossLoc that learns to predict coordinates and localize via geometric self- supervision. It significantly outperforms the state-of-the- art baselines in our benchmark, especially in the low-data regime. We believe that TOPO-DataGen, altogether with CrossLoc, could open up new opportunities for large-scale aerial localization applications in the real world. Acknowledgments The support of Armasuisse Science and Technology for this research is very much appreciated. We thank Th \u00b4eophile Schenker and Alexis Roger for their projects work, which helped set the spark of this ", "meta_data": {"split_id": 2}, "response": "Educational score: 2\n\nThis text has some educational value, but it is primarily technical and lacks clear explanations of complex concepts. The text assumes a high level of prior knowledge in the field of computer vision and aerial localization, which may make it challenging for primary school or grade school students to understand.\n\nThe text does provide some information on the importance of aerial localization and the challenges associated with it, which could be useful for introductory", "prediction": 2}
{"text": "", "meta_data": {"split_id": 1}, "response": "I apologize, but it seems that there is no extract provided. Please provide the extract, and I will be happy to evaluate its educational value and assign a score from 1-5.\n\nOnce I receive the extract, I will justify the total score in up to 100 words and conclude with the score in the format: \"Educational score: <total points>\".", "prediction": -1}
{"text": "ing black and white stickers to signs. Recently, several methods [15, 16, 35, 35\u201338] were pro- posed to attack the DNN-based person detection systems. Thys et al. [35] optimized an adversarial patch that can be attached to cardboard and held by a person. Huang et al. [16] propose Universal Physical Camou\ufb02age Attack(UPC) to fool the detectors by simulating 3D objects in virtual environments. Xu et al. [38] designed an adversar- ial T-shirt by introducing Thin Plate Spline (TPS) [2, 8] to simulate the deformation of clothes (e.g., wrinkles). Wu et al. [37] presented a systematic study of the attack on a range of detection models, different datasets, and objects. Wang et al. [36] masked the adversarial patch with preset logos and mapped it into 3D models. Hu et al. [15] used generative adversarial networks (GAN) [3,17] to craft more natural-looking adversarial patches. Some works [16,36,38] reported drops in the attack suc- cess rate when the viewing angles increased. According to Wang et al. [36], part of the patches will not be captured when the camera rotates drastically. It can lead to under- estimating the threat, whereas the cameras can be placed anywhere in real-world scenarios. 3. Methods We aim to generate textures in arbitrary size, and when the textures are printed on cloth, any patch extracted from the cloth are effective in adversarial attack. We \ufb01rst intro- duce an adversarial patch generator and then describe TC- EGA based on the patch generator. 3.1. Adversarial Patch Generator Let denote the whole cloth that is covered with Ad- vTexture, and ~ denote an extracted patch. We assume that~ follows a distribution padv, such that the probabil- itypadv(~ )is higher when its adversarial effectiveness is more signi\ufb01cant. We use an energy function U(~ )to model such a distribution: padv(~ ) =e\u0000U(~ ) ZU; (1) whereZU=R ~ e\u0000U(~ )d~ is called partition function. However, it is hard to sample from padv(~ )directly due to the partition function. Therefore, we use a parame- terized generator G':z!~ to approximate padv(~ ), wherez\u0018N (0;I). We de\ufb01ne q'(~ )as the distribution of~ =G'(z), which can be written as q'(~ ) =Z \u000e(~ \u0000G'(z))pz(z) dz; (2) wherepzis the probability density function (PDF) of the standard normal distribution N(0;I)and\u000e(\u0001)is the Dirac delta function. In order to represent padv(~ )more accurately, we tune G'to minimize the KL divergence KL(q'(~ )jjpadv(~ )). With the aid of Deep InfoMax (DIM) [14] we have the following theorem: Theorem 1 Minizing KL(q'(~ )jjpadv(~ ))is equivalent to min ';!E~ \u0018q'(~ )[U(~ )]\u0000IJSD ';!(~ ;z); (3) 2+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Originalimages\ud835\udc65Modifiedimages\ud835\udc40(\ud835\udc65,\ud835\udf0f\u0303) Patch\ud835\udf0f\u0303Detector\ud835\udc53\ud835\udc48*+, Generator\ud835\udc3a \ud835\udc40Latentvariable\ud835\udc67\ud835\udc48/0Figure 2. The pipeline of the adversary objective function. where IJSD ';!(~ ;z) =E(~ ;z)\u0018q~ ;z '(~ ;z)[\u0000sp(\u0000T!(~ ;z))] \u0000E~ \u0018q'(~ );z0\u0018pz(z0)[sp(T!(~ ;z0))];(4) q~ ;z 'denotes the joint distribution of ~ andz, and sp(t) = log(1 +et)is the softplus function. T!is a scalar function modeled by a neural network whose parameter !must be optimized together with the parameter '. SeeSupplementary Materials for the proof. The objective function in Eq. (3) consists of two terms. The \ufb01rst term E~ \u0018q'(~ )[U(~ )]is called Adversary Objective Function because minimizing it improves the the adversar- ial effectiveness of the generated patches. The second term \u0000IJSD ';!(~ ;z)is called Information Objective Function be- cause minimizing it is equivalent to maximizing the mutual information of zand~ [14], which requires different latent variables to generate different patches. 3.1.1 The Adversary Objective Function The adversary objective function E~ \u0018q'(~ )[U(~ )]can be es- timated by sampling zand generating ~ : 1 NNX i=1[U(G'(zi))]; (5) wherefzigare the latent variables sampled from N(0;I), andNdenotes the total number of the samples. Now we need to set an appropriate energy function such that lowering the energy leads to detection failure of a person detector. We notice that detectors output multiple bounding boxes with a con\ufb01dence score for each box when receiving an image. The boxes whose con\ufb01dence scores are lower than a pre-speci\ufb01ed threshold will then be \ufb01ltered out. Therefore we choose the expectation of the con\ufb01dence scores over boxes as a part of the energy function U(~ ). Then minimizing the adv object function will lower the con- \ufb01dence scores of the boxes, which makes the boxes easily to be \ufb01ltered out.Speci\ufb01cally, we randomly generate patches in every step, and apply a set of physical transformations such as random- izing the scales, contrast, brightness and additional noise ac- cording to Expectation over Transformation (EoT) [32, 35]. We also incorporate random Thin Plate Spin (TPS) [8, 38] deformation as an additional random transformation. We then attach the patches randomly to the persons according to the predicted boxes on the images xfrom the training set. We useM(x;~ )to denote the above process, and obtain the modi\ufb01ed images which are then be sent into the target de- tector. This part of the energy function is thus de\ufb01ned as Uobj=Ex;M[f(M(x;~ ))]; (6) wherefdenotes con\ufb01dence scores of the boxes predicted by the target detector. We use a differentiable variation of total variance (TV) loss [32] as another part of the energy function to encourage the patches to be smoother: UTV=X i;jj i;j\u0000 i+1;jj+j i;j\u0000 i;j+1j (7) Together, we form the energy function as U(~ ) =1 (Uobj+ UTV); (8) where and are coef\ufb01cients. See Fig. 2 for the illustra- tion. When minimizing the adversary objective function, each part of the energy function will be minimized together. 3.1.2 The Information Objective function As described in Eq. (4), we use an auxiliary network T!to increase the mutual information of zand~ . We illustrate the architecture of T!in Fig. 3. Eq. (4) has two terms, and estimating each of them needs random sampling. Following the previous work [14], to estimate the \ufb01rst term, we \ufb01rst samplezfromN(0;I), and then generated ~ byG'(z)in each training step. To estimate the second term, we keep ~ and resample z. 3+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 c\ud835\udf0f\u0303\ud835\udc67\ud835\udc47%(\ud835\udf0f\u0303,\ud835\udc67)Figure 3. The architecture of the auxiliary network T!. It has two inputs, ~ andz, and outputs a scalar value T!(~ ;z). The operation cin the \ufb01gure stands for concatenation. During training, we minimize the adversarial objective function and the information objective function simultane- ously. Therefore, the distribution q'can approximate to padv, which means the the generated patches ~ can be ad- versarial to the target detector. 3.2. Toroidal-Cropping-based Expandable Genera- tive Attack In Sec. 3.1, we have described the method to train a gen- erator for adversarial patches ~ . In this section, we used TC-EGA to generate AdvTextures based on the adversar- ial patch generator. We leverage a speci\ufb01c network archi- tecture and a sample technique to extend adversarial patches to adversarial textures. TC-EGA has two stages. In the \ufb01rst stage, we train a fully convolutional network (FCN) [24,33] to help sample from the distribution of adversarial textures. In the second stage, we search the best latent representation to yield the most effective adversarial texture. 3.2.1 Stage One: Train an Expandable Generator We aim to train a generator so that it can generate patches in arbitrary size easily by taking a random zas input. The critical point is to endow the generator with translation in- variant property by constructing an FCN, where all layers are convolutional layers with zero padding, including the \ufb01rst layer that inputs the latent variable (See Fig. 4a). The latent variable is a B\u0002C\u0002H\u0002Wtensor where Bis the batch size, Cis the number of channels, and H,Ware height and width, respectively. Here we show the reason for using FCN. We assume that the overall texture is generated by a global genera- torG:z! with hidden variable z\u0018N (0;I). We de- note the extracted patch by i;j;w;h whose center is located at the position (i;j)of the overall texture and has a shape of (w;h). Moreover, the patch i;j;w;h can be regarded as the output of a sub-generator Gi;j;w;h :ziz;jz;wz;hz! i;j;w;h , whereziz;jz;wz;hzis the component of zthat consists of all the elements dependent to i;j;w;h (see Fig. 4b). Assum- ing that i;j;w;h follows a distribution Ti;j;w;h . We have the following theorem and corollary. \u2026 \ud835\udc45\ud835\udc3a\ud835\udc35\u2026Multi-dimensional \ud835\udc67Convolution operationGenerated pattern(3 channels)Expandable generator\ud835\udc36\ud835\udc3b\ud835\udc4a(a) \u2026\ud835\udc67\",$,%,&\ud835\udf0f\",$,%,&\ud835\udf0f\"(,$(,%,&\u2026\u2026\u2026\u2026\ud835\udc67\ud835\udf0f\ud835\udc67\"(,$(,%,&\ud835\udc3a\",$,%,&\ud835\udc3a\"(,$(,%,& (b) Figure 4. (a) Illustration of the FCN generator. All layers of the generator network are convolutional layers with zero padding, in- cluding the \ufb01rst layer. (b) Each patch i;j;w;h extracted from posi- tioni;jcan be regarded as the output of a sub-generator Gi;j;w;h when the input is zi;j;w;h . Theorem 2 Let 1=G1(z1), 2=G2(z2),z1\u0018 Z 1, z2\u0018Z 2, 1\u0018T1, 2\u0018T2. IfZ1is identical toZ2andG1 is equivalent to G2, thenT1is identical toT2. Corollary 2.1 Gi;j;w;h andTi;j;w;h are irrelevant to i;j, i.e.,Gi;j;w;h =Gw;handTi;j;w;h =Tw;h, ifGis an FCN and the input z\u0018N(0;I). SeeSupplementary Materials for the proofs. Therefore, as long as the sub-generator Gw;his trained to approximate the distribution of Tw;htopadv, any patch extracted from the overall texture with shape (w;h)also approximately fol- lowspadv, i.e., it has adversarial effectiveness. Moreover, due to the translation invariant property of the convolutional operation, the sub-generator Gw;hand the global generator can share the same architecture and parameters except for the different spatial shape HandWof the latent variable z. As a result, we only need to train a small generator. Note that the height Hand widthWof the hidden vari- ablezcan not be too small, otherwise the output will be too small to crop a patch in spatial shape (w;h). We denote the minimum spatial sizes by HminandWmin. During train- ing, we sampled a small zin shapeB\u0002C\u0002Hmin\u0002Wmin and generated the corresponding patches in each training step. After that, we can produce different textures of ar- bitrary sizes by randomizing zwith anyH\u0015Hminand W\u0015Wmin. 4+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Concatenateblue arrowsConcatenatered arrows ACB321ACB321B1Local patternTorus32A(a) \ud835\udc67\"#$%\" (b) \ud835\udf0f\u0303#$%&# \ud835\udf0f\u0303canbecroppedfrom\ud835\udc67#$%&# (c) Figure 5. Illustration of Toroidal Cropping. (a) By \ufb01rst concate- nating its horizontal edges (red arrow) and then concatenating the vertical edges (blue arrow), the local pattern can be folded to a torus. (b) The latent variable in arbitrary shape can be created by tiling the local pattern side by side, thus the variable cropped at the junctions is equivalent to that cropped on the torus, meaning the pattern is still continuous. (c) This cropping technique also applies to the pixel space. See Sec. 4.3 for this variant. 3.2.2 Stage Two: Find the Best Latent Pattern After training, the generator can generate different textures by sampling latent variables. In order to \ufb01nd the best texture for adversarial attacks, we propose to go one step further, that is, to optimize the latent variable with the parameters of the generator frozen. However, since the texture has no speci\ufb01c shape and the size of the latent variable needs to be large enough to produce a large textured cloth, directly optimizing the latent variable is dif\ufb01cult. Inspired by the unfolding of torus in topology which supports up-down and left-right continuation [12] (Fig. 5a), we introduce the Toroidal Cropping (TC) technique, which aims to optimize a local pattern zlocal as a unit such that the \ufb01nal latent variable zcan be produced by tiling multi- ple identical units. In detail, zlocal can be parameterized as a tensor in shape B\u0002C\u0002L\u0002Lwith a shape hyper-parameter L, which can be regarded as the unfolded plane of a two- dimensional torus T2in topology (Fig. 5a). Therefore the latent variable in arbitrary shape can be cropped from zlocal in a recursive manner (Fig. 5b), which can be regarded as cropping on the torus. We denote such crop operation by Croptorus. During optimization, we randomly sample the latent variableszsample in shapeB\u0002C\u0002Hmin\u0002Wminby such cropping technique. Since we only consider the adversarial effectiveness in this stage, we generated patches by zsample and minimized the adversary loss (Eq. (5)). After optimiza-tion, one can produce a latent variable with arbitrary size by tilingzlocal. 4. Experiment settings 4.1. Subjects We recruited three subjects (mean age: 24:0; range: 21\u000026; two males and one female) to collect physical test set. The recruitment and study procedures were approved by the Department of Psychology Ethics Committee, Ts- inghua University, Beijing, China. 4.2. Dataset We employed the Inria Person dataset [7] as our training set. It is a dataset for pedestrian detection, which consists of614images for training and 288for testing. We evalu- ated the patch-based attack on the Inria test set. For physi- cal evaluation, we produced clothes covered with different adversarial textures. Three subjects wore different adver- sarial clothes and turned a circle slowly in front of a camera which was \ufb01xed at 1:38meters above the ground. The dis- tance between the camera and person is \ufb01xed to 2 munless otherwise Adversarial Texture for Fooling Person Detectors in the Physical World Zhanhao Hu1Siyuan Huang1Xiaopei Zhu1Xiaolin Hu1*Fuchun Sun1Bo Zhang1 1Department of Computer Science and Technology, Institute for Arti\ufb01cial Intelligence, State Key Laboratory of Intelligent Technology and Systems, Tsinghua University fhuzhanha17, zxp18 g@mails.tsinghua.edu.cn fsiyuanhuang, xlhu, fcsun, dcszb g@mail.tsinghua.edu.cn Abstract Nowadays, cameras equipped with AI systems can cap- ture and analyze images to detect people automatically. However, the AI system can make mistakes when receiving deliberately designed patterns in the real world, i.e., phys- ical adversarial examples. Prior works have shown that it is possible to print adversarial patches on clothes to evade DNN-based person detectors. However, these adversarial examples could have catastrophic drops in the attack suc- cess rate when the viewing angle (i.e., the camera\u2019s an- gle towards the object) changes. To perform a multi-angle attack, we propose Adversarial Texture (AdvTexture). Ad- vTexture can cover clothes with arbitrary shapes so that people wearing such clothes can hide from person detec- tors from different viewing angles. We propose a generative method, named Toroidal-Cropping-based Expandable Gen- erative Attack (TC-EGA), to craft AdvTexture with repetitive structures. We printed several pieces of cloth with AdvTex- ure and then made T-shirts, skirts, and dresses in the physi- cal world. Experiments showed that these clothes could fool person detectors in the physical world. 1. Introduction Recent works have shown that Deep Neural Networks (DNNs) are vulnerable to the adversarial examples crafted by adding subtle noise to the original images in the dig- ital world [6, 9, 11, 19, 25\u201327, 34], and that the DNNs can be attacked by manufactured objects in the physical world [1, 4, 10, 32]. These manufactured objects are called physical adversarial examples . Recently, some methods based on patch attacks [32] have been proposed to evade person detectors [15,16,35,37,38,40]. Speci\ufb01cally, Thys et al. [35] proposed to attach a patch to a cardboard. By hold- ing the cardboard in front of the camera, the person cannot be detected by the person detectors. Xu et al. [38] proposed *Corresponding author.an adversarial T-shirt printed with adversarial patches. The person wearing the T-shirt can also evade person detectors. These works impose considerable threats to the widely de- ployed deep learning-based security systems. It urges re- searchers to re-evaluate the safety and reliability of these systems. However, the person detector attack methods mentioned above are effective only when the adversarial patches face the camera. Apparently, a single adversarial patch on a piece of clothing is hard to attack detectors at multiple view- ing angles, as the camera may only capture a segment of the heavily deformed patch (Fig. 1a and Fig. 1b). We call this the segment-missing problem. A naive extension is to cover the clothing with multiple patches (e.g., tiling the patches tightly on the clothing; see Fig. 1c). However, it cannot totally solve the segment-missing problem, because the camera will capture several segments belonging to dif- ferent patch units, making the attack inef\ufb01cient. Another straightforward solution is to build a 3D model of a human body and a speci\ufb01c piece of clothing to render in differ- ent viewing angles as previous work [1] did. However, the clothes are non-rigid, and current 3D rendering tech- niques have dif\ufb01culties in modeling the natural deformation of clothes in the real world. For example, Wang et al. [36] rendered 3D logos on \ufb02at areas (front and back) of 3D hu- man meshes, but the Attack Success Rate (ASR) decreased when applying to unseen meshes. To solve the problem, we propose the idea of using Ad- versarial Texture (AdvTexture). Unlike the patch-based at- tacks, AdvTexture can be generated in arbitrary size, thus can cover any cloth in any size. We require that any local part of the texture has adversarial effectiveness (Fig. 1d). Then, when the clothes are covered with AdvTexture, every local area caught by the camera can attack the detectors, which solves the segment-missing problem. Towards this goal, we propose a two-stage generative method, Toroidal-Cropping-based Expandable Generative Attack (TC-EGA), to craft AdvTexture. In the \ufb01rst stage, we train a fully convolutional network (FCN) [24, 33] as 1arXiv:2203.03373v2 [cs.CV] 8 Mar 2022+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 P3P2P1C1C2C3(a) Patch (b) Tiledpatch (c) Texture (d) Figure 1. Illustration of the attacks at different viewing angles. (a) The camera captures different parts (P1, P2, P3) of the clothes when set to different viewing angles (C1, C2, C3). (b-d) The boxes are the possible areas that the camera may capture. The blue ones indicate the most effective areas for attack, while the red ones are less effective. the generator to produce textures by sampling random la- tent variables as input. Unlike the conventional architecture of the generator in GAN [17, 28], we use convolutional op- eration in every layer, including the latent variable. There- fore, the latent variable is a tensor with spatial dimensions, which enables the generator to generate texture in multiple sizes as long as we expand the latent variable along the spa- tial dimensions. In the second stage, we search the best local pattern of the latent variable with a cropping tech- nique\u2014Toroidal Cropping (TC). After optimization, we can generate a large enough latent variable by tiling the local pattern. We input it to the FCN and \ufb01nally get AdvTexture. We implemented TC-EGA to attack various person de- tectors, and realized AdvTextures in the physical world. Our experiments showed that the clothes made from such textures signi\ufb01cantly lowered the detection performance of different detectors. 2. Related Work Earlier works about adversarial examples [11,19,34] fo- cused on digital attacks. Small adversarial noises can be added to the original images and make DNNs output wrong predictions, posing signi\ufb01cant safety concerns to DNNs. Compared to digital adversarial attacks, physical adver- sarial attacks pose more risks in speci\ufb01c scenarios. Several methods [1, 4, 10, 32] have been proposed to attack image classi\ufb01cation models physically. Sharif et al. [32] designed a pair of glasses to attack face-recognition systems. Atha- lye et al. [1] generated robust 3D adversarial objects by in- troducing the Expectation over Transformation (EoT) [1] method. Brown et al. [4] deceived image classi\ufb01ers by plac- ing adversarial patches in the neighborhood of the objects. Evtimov et al. [10] misled road-sign classi\ufb01cation by adher- speci\ufb01ed. We recorded two videos for each sub- ject and each adversarial piece of clothing. One of the video was recorded indoor (lab room), and the other was recorded in outdoor (brick walkway). We then extracted 32frames from each video. We recorded 3\u00022 = 6 videos and col- lected 6\u000232 = 192 frames for each adversarial piece of clothing. we labeled them manually to construct a test set. 4.3. Baseline Methods We evaluated the adversarial patches produced by Thys et al. [35] and Xu et al. [38], and named them by AdvPatch andAdvTshirt , respectively. We copied the patterns from their original papers. We also tiled AdvPatch and AdvTshirt to form textures with repeated patterns. These two variants are called AdvPatchTile andAdvTshirtTile . In addition, we evaluated a texture with repetitive random colors, which is denoted by Random Moreover, TC-EGA has multiple components and some of them could be applied separately to craft adversarial tex- tures. To investigate the performance of each component, we designed three variants of TC-EGA, as described below. Expandable Generative Attack (EGA) We trained an FCN as the \ufb01rst stage of TC-EGA without optimizing the best latent variable. During evaluation, the \ufb01nal texture can be generated by a latent variable in arbitrary size and sam- pled from a standard normal distribution. Toroidal Cropping Attack (TCA) We directly optimized the texture instead of training an FCN to generate tex- ture. Speci\ufb01cally, we initialized a local texture pattern of 5+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 300\u0002300pixels, and randomly extracted a patch by size 150\u0002150from the texture by Toroidal Cropping in each optimization step. Random Cropping Attack (RCA) We directly opti- mized a large patch whose size is \ufb01xed. We initialized the large patch and randomly cropped a small patch by size 150\u0002150during optimization. This method is named Ran- dom Cropping Attack (RCA). We implemented two attacks, RCA2\u0002and RCA 6\u0002, where the sizes of the large patches are300\u0002300and900\u0002900, respectively. 4.4. Implementation Details We crafted AdvTexture to mainly fool YOLOv2 [29], YOLOv3 [30], Faster R-CNN [31] and Mask R-CNN [13]. The detectors were pre-trained on MS COCO dataset [22]. Their outputs were \ufb01ltered to output the person class only. For each target detector, we \ufb01rst extracted the predicted bounding boxes on the images from the training set with a Non-Maximum Suppression (NMS) threshold 0:4. We chose the boxes whose con\ufb01dence was larger than a cer- tain threshold ( 0:5for YOLOv2 and YOLOv3, and 0:75 for Faster and Mask R-CNN). We additionally \ufb01ltered out boxes with areas smaller than 0:16% of the entire images for Faster and Mask R-CNN. Then, as we described in Sec. 3.1.1, we attached the extracted patches to the persons and input the modi\ufb01ed images to the detector during opti- mization. Moreover, we applied the Adam [18] optimizer to opti- mize parameters in both stages. The hyper-parameters are listed as follows. (1) Stage one: The initial learning rate to train the generator was 0:001. The generator was a 7- layer FCN whose input was the latent variable zwith size B\u0002128\u00029\u00029. The size of the corresponding output was B\u00023\u0002324\u0002324, where the second dimension stands for the RGB channels. (2) Stage two: We optimized a local la- tent variable zlocal with size 1\u0002128\u00024\u00024, followed by the Toroidal Cropping technique to produce samples of zwith sizeB\u0002128\u00029\u00029. The learning rate of the optimization was0:03. To physically implement AdvTexture, we printed the texture on a polyester cloth material by digital textile print- ing. Afterwards, we hired a professional tailor to produce adversarial clothes including T-shirts, skirts and dresses. 5. Results Fig. 6 shows some textures obtained by different meth- ods, and more can be found in Supplementary Materials . 5.1. Patch-Based Attack in the Digital World We \ufb01rst evaluated the attacks in the form of the patch- based attack in the digital world. Speci\ufb01cally, we randomly (a) Random (b) AdvPatchTile (c) AdvTshirtTile (d) YOLOv2 (e) YOLOv3 (f) FasterRCNN Figure 6. Visualization of different textures. (a) The texture with repetitive random colors. (b,c) The Textures formed by tiling the two types of adversarial patches [35,38] repeatedly. (d-f) The tex- tures produced by TC-EGA to attack different detectors. Method AP Expandable Resampled Clean 1:000 Random 0:963 3 3 AdvPatch [35] 0.352 7 7 AdvPatchTile 0:827 3 3 AdvTshirt [38] 0:744*7 7 AdvTshirtTile 0:844 3 3 TC-EGA 0.362 3 3 EGA 0:470 3 3 TCA 0:664 3 3 RCA2\u0002 0:606 7 3 RCA6\u0002 0:855 7 3 Table 1. The APs of YOLOv2 under different attacks on Inria test set. Expandable denotes whether the methods can produce textures in arbitrary size. Resampled denotes whether the patches are randomly extracted. extracted patches from the textures when evaluating most methods except for AdvPatch and AdvTexture. We denote such patches by resampled patches. We then attached the patches to the images from the Inria test set the same way as crafting the adversarial patches. We used the bounding boxes proposed by the target detectors on the original test images with a con\ufb01dence threshold of 0:5as the ground truth. We computed the average precision (AP) of the pro- posed bounding boxes on the modi\ufb01ed test images to mea- sure the adversarial effectiveness. Note that lower AP indi- cates stronger attack. Tab. 1 presents the AP of YOLOv2 in different condi- tions. clean denotes the AP on the original test set. Since we used the detector\u2019s prediction on the original images as 6+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 AdvPatchAdvPatchTileTC-EGAFigure 7. Numerical study of the segment-missing problem. The patches are cropped near the original patches with a shifting ratio. For AdvPatch as an example, the shifting ratio is 0:0when the cropped patch is precisely the original patch. The shifting ratio is 1:0when the original patch is shifted totally outside the cropping range. the ground truth, the AP is 1:000. The AdvPatch lowered the AP of YOLOv2 to 0:3521. Compared to AdvPatch, the expandable variant Adv- PatchTile increases the AP from 0:352to0:827. Since Ad- vTshirt was trained on a different dataset (its authors\u2019 pri- vate dataset), it only got an AP of 0:744. Similarly, Ad- vTshirtTile increases the AP to 0:844. We attribute the in- crease to the segment-missing problem. Compared to its variants, TC-EGA got the lowest AP 0:362, which was also the lowest among all the resampled patches. AdvPatch made the AP slightly lower than TC-EGA. However, it is not expandable and thus unsuitable for the attack at multiple viewing angles. Moreover, EGA decreased the AP to 0:470, TCA created expandable patches with AP 0:664. It was lower than AdvPatchTile, which indicates the effectiveness of the Toroidal Cropping technique. Moreover, RCA 6\u0002 was much worse than RCA 2\u0002, which indicates dif\ufb01culties in optimizing a large patch. We further investigated the segment-missing problem by evaluating the adversarial effectiveness of the patches which is cropped at shifted positions (See Fig. 7). The patch-based attack, AdvPatch, became less effective when the shifting ratio increased. Tiling the patches alleviated the problem, but was still problematic. The text", "meta_data": {"split_id": 2}, "response": "The educational value of this text is moderate, scoring 3 out of 5.\n\nThe text provides some basic information on adversarial attacks and their applications in the field of artificial intelligence. It describes the concept of adversarial textures and presents a novel method for generating such textures using a generative model. The text also provides some experimental results and comparisons with other methods.\n\nHowever, the text has some limitations that prevent it from being highly educational. The language is technical and may be difficult for non-experts to understand. The text assumes a certain level of background knowledge in the field of artificial intelligence and machine learning, which may not be familiar to all readers. Additionally, the text does not provide a clear explanation of the underlying mathematical and computational concepts, which may make it challenging for readers to understand the details of the method.\n\nOverall, the text is suitable for readers who have a basic understanding of artificial intelligence and machine learning and are interested in learning more about adversarial attacks and their applications. However, readers who are new to the field may find the text challenging to follow without additional background knowledge or resources.\n\nEducational score: 3", "prediction": 3}
{"text": "of diffusion model \u0012and encoder network are trained using the trajectory data. Note that we share the net- work parameters for all transitions. As shown the previous work [14], the variance term of the Gaussian transition can be set as \u0006\u0012(yK;k) =\u001b2 kI= kI. This setting denotes the upper bound on reverse process entropy for data and shows good performance in practice [40]. 3.3. Training Objective Having formulated diffusion and reverse diffusion pro- cesses, we describe how to train the diffusion model. To predict the real trajectory y0, the desired training should op- timize the log-likelihood E[logp\u0012(y0)]in the reverse pro- cess. However, the exact log-likelihood is intractable, we thus maximize the variational lower bound for optimization: E[logp\u0012(y0)]\u0015Eq[logp\u0012(y0:K;f) q(y1:Kjy0)] =Eq[logp(yK) +KX k=1logp\u0012(yk\u00001jyk;f) q(ykjyk\u00001)]: (4) We utilize the negative bound as the loss function and per- form the training by optimizing it as: L(\u0012; ) =Eq[KX k=2DKL(q(yk\u00001jyk;y0)kp\u0012(yk\u00001jyk;f)) \u0000logp\u0012(y0jy1;f)]: (5) In this loss function, we ignore the term with Eqlogp(yK) in (4), since p(yK)is a standard Gaussian and q(yKjy0) has no learnable parameters as shown in (2). Here we describe how to calculate the \ufb01rst term DKL. The posterior q(yk\u00001jyk;y0)inDKLis tractable and can be represented by Gaussian distribution as: q(yk\u00001jyk;y0) =N(yk\u00001;~\u0016k(yk;y0);~ kI);(6) where the closed form of ~\u0016k(yk;y0)and~ kis calculated as: ~\u0016k(yk;y0) =p\u0016 k\u00001 k 1\u0000\u0016 ky0+p k(1\u0000\u0016 k\u00001) 1\u0000\u0016 kyk ~ k=1\u0000\u0016 k\u00001 1\u0000\u0016 k kI:(7) Since both diffusion process (6) and reverse process (3) are Gaussian, we can calculate the DKLby the difference be-tween the means of ~\u0016kand\u0016\u0012as: DKL=Eq\u0002 \u0015k~\u0016k(yk;y0)\u0000\u0016\u0012(yk;k;f)k2\u0003 +C; (8) where\u0015andCare coef\ufb01cients with no effect on the gradient direction. Note that the second term \u0000logp\u0012(y0jy1;f)can also be formulated as the form in (8) when k= 1. Finally, we apply the parameterization method as shown in [14] to reparameterize: \u0016\u0012(yk;k;f) =1p k(yk\u0000 kp1\u0000\u0016 k\u000f\u0012(yk;k;f));(9) and obtain a simpli\ufb01ed loss function as: L(\u0012; ) =E\u000f;y0;kk\u000f\u0000\u000f(\u0012; )(yk;k;x)k; (10) where\u000f\u0018 N (0;I),yk=p\u0016 ky0+p1\u0000\u0016 k\u000fand the training is performed at each step k21;2;\u0001\u0001\u0001;K. (Please see the detailed derivation and detail algorithms in the sup- plementary material.) 3.4. Inference Once the reverse process is trained, we can generate the plausible trajectories by a noise Gaussian yK\u0018N (0;I) through the reverse process p\u0012. With the reparameterization in (9), we generate the trajectories from yKtoy0as: yk\u00001=1p k(yk\u0000 kp1\u0000\u0016 k\u000f\u0012(yk;k;f)) +p kz; (11) where zis a random variable in standard Gaussian distribu- tion and\u000f\u0012is the trained network whose inputs include the previous step\u2019s prediction yk, state embedding f, and step k. 3.5. Network Architecture Different from the widely used UNet [36] in image- based diffusion models [8, 14, 31], we design a new Transformer-based network architecture for our MID. With the Transformer, the model can better explore the temporal dependency of paths for the trajectory prediction task. To be speci\ufb01c, MID consists of two key networks: an encoder net- work with parameters which learns the state embedding by observed history trajectories and their social interactions, and a Transformer-based decoder parameterized by \u0012for the reverse diffusion process. An overview of the whole archi- tecture is depicted in Figure 2. We will introduce each part in detail in the following. The encoder network models the history behaviors and social interactions as the state embedding f. This embed- ding is fed into the decoder network as the condition of the diffusion model. Note that, designing the network to model social interactions is not the main focus of this work, and MID is an encoder-agnostic framework which can directly equip with different encoders introduced in previous meth- ods. In the experiments, we apply the encoder of Trajec- tron++ [38] for its superior representation ability.+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 For the decoder, we design a Transformer-based archi- tecture to model the Gaussian transitions in Markov chain. As shown in Figure 2, the inputs of decoder include the ground truth trajectory y0, the noise variable \u000f\u0018N (0;I), the condition feature ffrom the encoder, and a time em- bedding. In step k, we \ufb01rst add noise into trajectory to get yk=p\u0016 ky0+p1\u0000\u0016 k\u000f. Simultaneously, we calculate the time embedding and concatenate it with the feature of observed trajectory. Then, we apply fully-connected lay- ers to upsample both trajectory ykand condition f, then sum up the outputs as the fused feature. We also introduce the positional embedding in the form of sinusoidal func- tions on the summation to emphasize the positional relation at different trajectory timestamp t. Finally, the fused fea- ture with positional embedding is fed into the Transformer network to learn the complex spatial-temporal clues. The Transformer-based decoder network consists of three self- attention layers to suf\ufb01ciently model the temporal depen- dencies in trajectories, which takes the high dimension se- quence as input and outputs the sequence with the same di- mension. With a fully-connected layer, we downsample the output sequence to the trajectory dimension. We \ufb01nally per- form mean square error (MSE) loss between the output and a random Gaussian as (20) for current iteration to optimize the network. Please see the network details in the supple- mentary material. 4. Experiments In this section, we \ufb01rst compared the proposed method with state-of-the-art approaches on two widely-used pedes- trian trajectories prediction benchmarks, then conducted ab- lation studies to analyze the effectiveness of key compo- nents of our MID framework and provided an analysis re- garding the reverse diffusion process. 4.1. Experimental Setup Datasets: We evaluated our method on two public pedestrian trajectories forecasting benchmarks including Stanford Drone Dataset (SDD) [35] and UCY/ETH [21,33]. Stanford Drone Dataset: Stanford Drone Dataset [35] is a well established benchmark for human trajectory predic- tion in bird\u2019s eye view. The dataset consists of 20 scenes captured using a drone in top down view around the univer- sity campus containing several moving agents like humans and vehicles. ETH/UCY: The ETH [33] and UCY [21] dataset group consists of \ufb01ve different scenes \u2013 ETH & HOTEL (from ETH) and UNIV , ZARA1, & ZARA2 (from UCY). All the scenes report the position of pedestrians in world- coordinates and hence the results we report are in me- tres. The scenes are captured in unconstrained environ- ments with few objects blocking pedestrian paths.Table 1. Quantitative results on the Stanford Drone dataset with Best-of-20 strategy in ADE/FDE metric. \u201cT\u201d denotes the method Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion Tianpei Gu*;1;5, Guangyi Chen\u0003;2;3, Junlong Li4, Chunze Lin5, Yongming Rao4, Jie Zhou4, Jiwen Luy;4 1University of California, Los Angeles,2MBZUAI,3Carnegie Mellon University, 4Tsinghua University,5SenseTime Research Abstract Human behavior has the nature of indeterminacy, which requires the pedestrian trajectory prediction system to model the multi-modality of future motion states. Unlike ex- isting stochastic trajectory prediction methods which usu- ally use a latent variable to represent multi-modality, we explicitly simulate the process of human motion variation from indeterminate to determinate. In this paper, we present a new framework to formulate the trajectory prediction task as a reverse process of motion indeterminacy diffu- sion (MID ), in which we progressively discard indetermi- nacy from all the walkable areas until reaching the desired trajectory. This process is learned with a parameterized Markov chain conditioned by the observed trajectories. We can adjust the length of the chain to control the degree of in- determinacy and balance the diversity and determinacy of the predictions. Speci\ufb01cally, we encode the history behav- ior information and the social interactions as a state em- bedding and devise a Transformer-based diffusion model to capture the temporal dependencies of trajectories. Exten- sive experiments on the human trajectory prediction bench- marks including the Stanford Drone and ETH/UCY datasets demonstrate the superiority of our method. Code is avail- able at https://github.com/gutianpei/MID . 1. Introduction Human trajectory prediction plays a crucial role in human-robot interaction systems such as self-driving vehi- cles and social robots, since human is omnipresent in their environments. Although signi\ufb01cant progresses have been achieved over past few years [6,28,29,32,38,45,49,53], pre- dicting the future trajectories of pedestrians remains chal- lenging due to the multi-modality of human motion. The future trajectories of pedestrians are full of indeter- minacy, because human can change future motion accord- ing to their will or adjust their movement direction based on the surroundings. Given a history of observed trajecto- *Equal contribution.\u2020Corresponding author. !&!'#$!%\u2026\u2026!!\"%#$|\"%,&f!' Observed TrajectoryFuture Trajectory \u2026\u2026 determinacydiversity'\"%|\"%#$Figure 1. Illustration of the reverse diffusion process for hu- man motion indeterminacy variation. Under high indeterminacy, the trajectory distribution can be regarded as a noise Gaussian distribution which denotes ambiguous walkable areas. With the decreasing of indeterminacy, this distribution gradually approxi- mates the real data distribution to generate desired trajectory. This process from high indeterminacy to low indeterminacy is de\ufb01ned as a reverse diffusion process, in which we learn a Markov chain to progressively discard the indeterminacy. By adjusting the length of the chain, we can make a trade-off between diversity and deter- minacy, where the longer chain leads to lower diversity and higher determinacy. Best viewed in color. ries, there exist many plausible paths that pedestrians could move in the future. Facing this challenge, most of prior researches apply the generative model to represent multi- modality by a latent variable. For instance, some meth- ods [6,9,12,19,37,43,54] utilize generative adversarial net- works (GANs) to spread the distribution over all possible future trajectories, while other methods [3,16,20,25,38,46] exploit conditional variational auto-encoder (CV AE) to en- code the multi-modal distribution of future trajectories. De- spite the remarkable progress, these methods still face in- herent limitations, e.g., training process could be unstable for GANs due to adversarial learning, and CV AE tends to produce unnatural trajectories. In this paper, we propose a new trajectory prediction framework, called motion indeterminacy diffusion (MID), to model the indeterminacy of human behavior. Inspired byarXiv:2203.13777v1 [cs.CV] 25 Mar 2022+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 non-equilibrium thermodynamics, we consider the future positions as particles in thermodynamics in our framework. The particles (positions) gather and deform to a clear trajec- tory under low indeterminacy, while stochastically spread over all walkable areas under high indeterminacy. The pro- cess of particles evolving from low indeterminacy to high indeterminacy is de\ufb01ned as the diffusion process. This pro- cess can be simulated by gradually adding noise to the tra- jectory until the path is corrupted as Gaussian noise. The goal of our MID is to reverse this diffusion process by pro- gressively discarding indeterminacy, and converting the am- biguous prediction regions into a deterministic trajectory. We illustrate the reverse diffusion process of motion inde- terminacy in Figure 1. Contrary to other stochastic pre- diction methods that add a noise latent variable on the tra- jectory feature to obtain indeterminacy, we explicitly simu- late the motion indeterminacy variation process. Our MID learns a Markov chain with parameterized Gaussian transi- tion to model this reverse diffusion process and train it using variational inference conditioned on the observed trajecto- ries. By choosing different lengths of the chain, we can ob- tain the predictions with a \ufb02exible indeterminacy that is ca- pable of adapting to dynamic environment. Moreover, our method is more ef\ufb01cient to train than GANs, and is capable of producing more high-quality samples than CV AEs. To be more speci\ufb01c, we encode the history human tra- jectories and the social interactions as state embedding via a spatial-temporal graph network. Then, we exploit this state embedding as condition in the Markov chain to guide the learning of reverse diffusion process. To model the temporal dependencies in trajectories, we carefully design a Transformer-based architecture as the core network of MID framework. In the training process, we optimize the model with the variational lower bound, and during the in- ference, we sample the reasonable trajectories by progres- sive denoising from a noise distribution. Extensive experi- ments demonstrate that our method accurately forecasts rea- sonable future trajectories with multi-modality, achieving state-of-the-art results on Stanford Drone and ETH/UCY datasets. We summarize the main contributions of this pa- per as follows: \u2022 We present a new stochastic trajectory predic- tion framework with motion indeterminacy diffusion, which gradually discards the indeterminacy to obtain desired trajectory from ambiguous walkable areas. \u2022 We devise a Transformer-based architecture for the proposed framework to capture the temporal depen- dencies in trajectories. \u2022 The proposed method achieves state-of-the-art perfor- mance on widely used human trajectory prediction benchmarks and provides a potential direction for bal- ancing the diversity and accuracy of predictions.2. Related Work Pedestrian Trajectory Prediction: Given the observed paths, human trajectory forecasting system aims to esti- mate the future positions. Most existing methods formu- late trajectory forecasting as a sequential prediction prob- lem and focus on modeling the complex social interaction. For instance, Social Forces [13] introduces attractive and repulsive forces to model human interaction. With the suc- cess of deep learning, many methods design ingenious net- works to model the social interactions. For example, Social- LSTM [1] devises a social pooling layer to aggregate the interaction information of neighborhoods. Some methods apply the attention models [10, 19, 37, 47, 51] to explore the key interactions of the crowd. In addition, the spatial- temporal graph model is applied to jointly model the tem- poral clues and social interactions [15, 16, 30, 38, 44, 50]. Beyond social interactions, many methods incorporate the physical environment interactions by introducing the map images [6, 19, 20, 28, 37]. Recently, some methods analyze the effect of social interaction and show it is biased [2, 27]. Stochastic Prediction Model: Due to the inherent inde- terminacy of human behavior, Many stochastic prediction methods are proposed to model the multi-modality of fu- ture motions. Some methods [6,9,12,19,37,43,54] employ GANs [11] to model the multi-modality with a noise vari- able, and another line of methods [3,16,20,25,38,46] apply the CV AE [41] instead. Besides, some methods [7, 23, 24] propose to learn the grid-based location encoder for multi- modal probability prediction. Recently, the goals of pedes- trians [28,29,52,53] are introduced in the trajectory predic- tion system as condition to analyze the probability of mul- tiple plausible endpoints. While remarkable progress have been made, these stochastic prediction methods have some inherent limitations, e.g., the unstable training or unnatu- ral trajectories. In this paper, we propose a new stochastic framework with motion indeterminacy diffusion, which for- mulates the trajectory prediction problem as a process from an ambiguous walkable region to the desired trajectory. Denoising Diffusion Probabilistic Models: Denoising diffusion probabilistic models (DDPM) [14, 40], as known as diffusion models for brevity, are a class of deep gen- erative models inspired by non-equilibrium thermodynam- ics. It is \ufb01rst proposed by Sohl-Dickstein et al. [40] and attracts much attention recently due to state-of-the-art per- formance in various generation tasks including image gen- eration [5,8,14,31], 3D point cloud generation [26,55], and audio generation [4,18,34]. The diffusion models generally learn a parameterized Markov chain to gradually denoise from an original common distribution to a speci\ufb01c data dis- tribution. In this paper, we introduce the diffusion model to simulate the variation of indeterminacy for trajectory pre- diction, and design a Transformer-based architecture for the temporal dependency of trajectories.+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Transformer MSE Past Trajectory \u2026 k Temporal-Social EncoderK \u2026Future Trajectory\u03f5~!(0,1) FC FC FC\u2026,\"-\"\"-#\"-$\"-%\"-&\"-'\"-(\"-)\".*,\",#,&.\".!1231Figure 2. The architecture of our MID framework. MID consists of a temporal-social encoder network and a Transformer-based decoder network. The encoder maps the history path and social interaction clues into a state embedding. The decoder then takes ykalong with state embedding and the time embedding, where ykis corruptedktimes by a noise variable from the ground truth trajectory y0. We learn the model with the MSE loss between the model output and a noise variable in standard Gaussian distribution. 3. Proposed Approach In this section, we introduce our MID method, which models stochastic trajectory prediction task by motion in- determinacy diffusion. We \ufb01rst explicitly formulate the in- determinacy variation as a reverse diffusion process. Then we describe how to train this diffusion model using the vari- ational inference. Finally, we present the detailed network architecture of our method shown in Figure 2. 3.1. Problem Formulation The goal of pedestrian trajectory prediction is to gen- erate plausible future trajectories for pedestrians based on their prior movements. The input of the prediction system is theNhistory trajectories in a scene such that xi=fsi t2 R2jt=\u0000Tinit;\u0000Tinit+ 1;\u0001\u0001\u0001;0g,8i2f1;2;\u0001\u0001\u0001;Ng, where thesi tis the 2D location at timestamp t,Tinitde- notes the length of the observed trajectory, and the current timestamp is t= 0. Similarly, the predicted future trajecto- ries can be written as yi=fsi t2R2jt= 1;2;\u0001\u0001\u0001;Tpredg. For clarity, we use xandywithout the superscript ifor the history and future trajectory in the following subsections. 3.2. Motion Indeterminacy Diffusion Due to the indeterminacy of human behavior, one per- son has multiple plausible paths in future state. Thus, we present a new framework to formulate the stochastic trajec- tory prediction by motion indeterminacy diffusion. Unlike other stochastic prediction methods that add a latent vari- able on the trajectory feature to obtain indeterminacy, our MID generates the trajectory by gradually reducing the in- determinacy from all walkable areas to the determinate pre- diction with a parameterized Markov chain. As shown in Figure 1, given the initial ambiguous regionyKunder the noise distribution and the desired trajectory y0under the data distribution, we de\ufb01ne the diffusion pro- cess as (y0;y1;\u0001\u0001\u0001;yK), whereKis the maximum num- ber of diffusion steps. This process aims to gradually add the indeterminacy until the ground truth trajectory is cor- rupted into a noisy walkable region. On the contrary, we learn the reverse process as (yK;yK\u00001;\u0001\u0001\u0001;y0)to gradu- ally reduce the indeterminacy from yKto generate the tra- jectories. Both diffusion and reverse diffusion processes are formulated by a Markov chain with Gaussian transitions. First, we formulate the posterior distribution of the dif- fusion process from y0toyKas: q(y1:Kjy0) :=KY k=1q(ykjyk\u00001) q(ykjyk\u00001) :=N(yk;p 1\u0000 kyk\u00001; kI);(1) where 1; 2;\u0001\u0001\u0001 Kare \ufb01xed variance schedulers that con- trol the scale of the injected noise. Due to the notable prop- erty of the Gaussian transitions, we calculate the diffusion process at any step kin a closed form as: q(ykjy0) :=N(yk;p\u0016 ky0;(1\u0000\u0016 k)I); (2) where k= 1\u0000 kand \u0016 k=Qk s=1 s. Therefore, whenKis large enough, we approximately obtain that yK\u0018 N (0;I). It indicates that the signal is corrupted into a Gaussian noise distribution when gradually adding noise, which conforms to the non-equilibrium thermody- namics phenomenon of diffusion process. Next, we formulate the trajectories generation process as a reverse diffusion process from noise distribution. We model this reverse process by parameterized Gaussian tran- sitions with the observed trajectories as condition. Given a state feature flearned by a temporal-social encoder F pa- rameterized by with the history trajectories xas input, we+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 formulate the reverse diffusion process as: p\u0012(y0:Kjf) :=p(yK)KY k=1p\u0012(yk\u00001jyk;f) p\u0012(yk\u00001jyk;f) :=N(yk\u00001;\u0016\u0012(yk;k;f);\u0006\u0012(yk;k)); (3) wherep(yK)is an initial noise Gaussian distribution, and \u0012denotes the parameter of the diffusion model. Both pa- rameters only using the trajectory position information, and \u2018T + I\u201d denotes the method using both position and visual image information. y means the results are reproduced by us with the of\ufb01cial released code. Lower is better. Methods Input Sampling ADE FDE CGNS [22] T + I 20 15.60 28.20 SimAug [23] T + I 20 10.27 19.71 yY-Net [28] T + I 20 8.97 14.61 Y-Net [28]+ TTST T + I 10000 7.85 11.85 Social-GAN [12] T 20 27.23 41.44 PECNet [29] T 20 9.96 15.88 yTrajectron++ [38] T 20 8.98 19.02 LB-EBM [32] T 20 8.87 15.61 PCCSNET [45] T 20 8.62 16.16 yExpert [53] T 20 10.67 14.38 yExpert [53]+GMM T 20\u000220 7.65 14.38 MID T 20 7.61 14.30 Evaluation Metric: We adopted the widely-used evalu- ation metrics Average Displacement Error (ADE) and Final Displacement Error (FDE). ADE computes the average er- ror between all the ground truth positions and the estimated positions in the trajectory, and FDE computes the displace- ment between the end points of ground truth and predicted trajectories. The trajectories are sampled 0.4 seconds in- terval, where the \ufb01rst 3.2 seconds of a trajectory is used as observed data to predict the next 4.8 seconds future tra- jectory. For the ETH/UCY dataset, we followed the leave one out cross-validation evaluation strategy such that we trained our model on four scenes and tested on the remain- ing one [12,15,19,38]. Considering the stochastic property of our method, we used Best-of-N strategy to compute the \ufb01nal ADE and FDE with N= 20 . Implementation Details: We devised a three-layers Transformer as the core network for our MID, where the Transformer dimension is set to 512, and 4 attention heads are applied. We employed one fully-connected layer to upsample the input of the model from dimension 2 to the Transformer dimension, and another fully-connected layer to upsample the observed trajectory feature fto the same dimension. We utilized three fully-connected layers to pro- gressively downsample the Transformer output sequence to the predicted trajectory, such that 512d-256d-2d. The train- ing was performed with Adam optimizer, with a learning rate of 0:001and batch size of 256. All the experiments were conducted on a single Tesla V100 GPU. 4.2. Comparison with state-of-the-art methods We quantitatively compare our method with a wide range of current methods. As shown in Table 1, we provide+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Table 2. Quantitative results on the ETH/UCY dataset with Best-of-20 strategy in ADE/FDE metric. Lower is better. Input SamplingETH HOTEL UNIV ZARA1 ZARA2 A VG ADE FDE ADE FDE ADE FDE ADE FDE ADE FDE ADE FDE SoPhie [37] T + I 20 0.70 1.43 0.76 1.67 0.54 1.24 0.30 0.63 0.38 0.78 0.54 1.15 CGNS [22] T + I 20 0.62 1.40 0.70 0.93 0.48 1.22 0.32 0.59 0.35 0.71 0.49 0.97 Social-BiGAT [19] T + I 20 0.69 1.29 0.49 1.01 0.55 1.32 0.30 0.62 0.36 0.75 0.48 1.00 MG-GAN [6] T + I 20 0.47 0.91 0.14 0.24 0.54 1.07 0.36 0.73 0.29 0.60 0.36 0.71 Y-Net [28] + TTST T + I 10000 0.28 0.33 0.10 0.14 0.24 0.41 0.17 0.27 0.13 0.22 0.18 0.27 Social-GAN [12] T 20 0.81 1.52 0.72 1.61 0.60 1.26 0.34 0.69 0.42 0.84 0.58 1.18 Causal-STGCNN [2] T 20 0.64 1.00 0.38 0.45 0.49 0.81 0.34 0.53 0.32 0.49 0.43 0.66 PECNet [29] T 20 0.54 0.87 0.18 0.24 0.35 0.60 0.22 0.39 0.17 0.30 0.29 0.48 STAR [49] T 20 0.36 0.65 0.17 0.36 0.31 0.62 0.26 0.55 0.22 0.46 0.26 0.53 Trajectron++ [38] T 20 0.39 0.83 0.12 0.21 0.20 0.44 0.15 0.33 0.11 0.25 0.19 0.41 LB-EBM [32] T 20 0.30 0.52 0.13 0.20 0.27 0.52 0.20 0.37 0.15 0.29 0.21 0.38 PCCSNET [45] T 20 0.28 0.54 0.11 0.19 0.29 0.60 0.21 0.44 0.15 0.34 0.21 0.42 yExpert [53] T 20 0.37 0.65 0.11 0.15 0.20 0.44 0.15 0.31 0.12 0.26 0.19 0.36 yExpert [53]+GMM T 20\u000220 0.29 0.65 0.08 0.15 0.15 0.44 0.11 0.31 0.09 0.26 0.14 0.36 MID T 20 0.39 0.66 0.13 0.22 0.22 0.45 0.17 0.30 0.13 0.27 0.21 0.38 Table 3. Ablation studies on MID and network architecture de- signs. Trans is the abbreviation of Transformer Group Method Architecture ADE FDE 1 MID Trans-512d 7.61 14.30 2MID Tran", "meta_data": {"split_id": 2}, "response": "The educational value of this extract is relatively low due to several reasons:\n\n1. The text is primarily focused on a specific research topic in the field of computer vision and deep learning, making it difficult for students outside of this field to understand.\n2. The writing style is formal and technical, with a significant amount of jargon and complex terminology that may be unfamiliar to some readers.\n3. The text does not provide a clear, concise explanation of the concepts and ideas presented, making it challenging for students to follow along.\n4. The text does not offer any practical or real-world applications of the concepts and ideas presented, making it difficult for students to see the relevance and importance of the topic.\n\nBased on the additive 5-point scoring system, I would give this extract a score of 2 out of 5.\n\nHere's a breakdown of the score:\n\n* Criterion 1: Provides some basic information relevant to educational topics (1 point)\n* Criterion 2: Addresses certain elements pertinent to education but does not align closely with educational standards (0 points)\n* Criterion 3: Introduces key concepts relevant to school curricula (0 points)\n* Criterion 4: Offers substantial educational content with minimal irrelevant information (0 points)\n* Criterion 5: Is easy to follow and offers profound and thorough insights into the subject matter (0 points)\n\nEducational score: 2", "prediction": 2}
{"text": "", "meta_data": {"split_id": 1}, "response": "I apologize, but you didn't provide the extract. Please provide the extract, and I'll be happy to evaluate its educational value and assign a score using the additive 5-point scoring system.\n\nOnce I receive the extract, I'll justify my total score in up to 100 words and conclude with the score in the format: \"Educational score: <total points>\".", "prediction": -1}
{"text": "the intensity of feature perturbation. We suppose that varying will let the discrim- inator learn from various strengths of regularization. Denoting an i-th layer of the discriminator as fi, a con- tent image as c, and a style reference image as swhich is randomly chosen from the current mini-batch samples, we de\ufb01ne the mixed feature maps ~xand~ythrough feed- forward operations with FSM: ~x1=x1=f1(c); ~y1=y1=f1(s); ~xi+1=fi+1(FSM(~xi;~yi)); ~yi+1=fi+1(FSM(~yi;~xi)):(7) Then the \ufb01nal output logit of the mixed feed-forward pass through the discriminator with nconvolutional layers be- comes: DFSM(c;s) =Linear (~xn): (8) Given the original output D(c)and the mixed output DFSM(c;s), we penalize their discrepancy with a loss: LFSMR =Ec;s\u0018pdata\u0002 (D(c)\u0000DFSM(c;s))2\u0003 : (9) Figure 2 illustrates the full diagram of FSMR. This loss is added to the adversarial loss [8] when updating the dis- criminator parameters. It regularizes the discriminator to produce consistent output under different statistics of the features varying through the layers. Our design of LFSMR is general-purpose and thereby can be combined with other methods [19, 20, 42]. As shown in Algorithm 1, FSM can be implemented with only a few lines of code. Also, we provide the Tensor\ufb02ow-like pseudo-code of FSMR in Ap- pendix C. 3.3. Visualizing the effect of FSM To visually inspect the effect of FSM in the discrimi- nator, we train a decoder (same architecture as the one for AdaIN [13]) which reconstructs the original image from the 32\u000232feature maps of the original discriminator. 4+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 (a) Style (b) Content (c) Stylization by AdaIN [13] (d) Visualization of FSM Figure 3. Visualization of the effect of FSM (Section 3.3). (a) Example style images. (b) Example content images. (c) AdaIN largely distorts \ufb01ne details. (d) Reconstruction of FSMed features preserves them. In Figure 3, the content images go through the discrim- inator with FSM on all layers with respect to the style im- ages to produce stylized ( i.e.,FSMed ) intermediate features. Then the learned decoder synthesizes the result images from the FSMed features. The FSMed images have similar global styles to the style images but contain semantics of the content images. It has a similar effect to AdaIN but better preserves the \ufb01ne details of the content. We suggest that it is the key for the discrim- inator to be able to provide gradients toward more realistic images for the generator leading to higher quality images than the on-the-\ufb02y stylization baseline (Section 4.1). 4. Experiments We conduct extensive experiments on six datasets of CIFAR-10 [26], FFHQ [20], AFHQ [5], CelebA-HQ [18], LSUN Church [37], and MetFaces [19] with \ufb01ve GAN methods such as DCGAN [32], bCRGAN [43], StyleGAN2 [21], DiffAugment [42], and ADA [19]. We choose the datasets and baseline methods following the recent experi- mental setups [19,42]. We use the relative distance \u001a(Eq. 3), Fr\u00b4echet inception distance (FID) [11], and inception score (IS) [33] as evaluation metrics. When we compute FID, we use all training samples and the same number of fake sam- ples. All the baseline methods are trained using the of\ufb01cial implementations provided by the authors. See Appendix A for more details. We next conduct thorough experiments to demonstrate the superiority of our method over the straight- forward solution and the baselines. 4.1. Comparison with the on-the-\ufb02y stylization In this section, we compare our method with the on-the- \ufb02y stylization, i.e., generating stylized images via AdaIN during training and applying consistency regularization (Section 2.2). To perform this, we collect 100 style images from WikiArt [1] and randomly sample one for stylizing each image during training. Note that, unlike the on-the- \ufb02y stylization, FSMR does not rely on external style im-ages. We conduct experiments on \ufb01ve benchmark datasets: CIFAR-10, CelebA-HQ, FFHQ, AFHQ, and LSUN Church. Table 1 compares effect of regularization with on-the-\ufb02y stylization and FSMR in FID. While the former improves FID compared to the baselines to some extent, improve- ments due to FSMR are larger in all cases. For comparison with additional networks and datasets, see Appendix F. To measure the discriminator\u2019s sensitivity to style, we compute the relative distance \u001a(Eq. 3) for each method. Figure 4 shows the relative distance on CIFAR-10, FFHQ, and AFHQ. As one can easily expect, utilizing the stylized dataset reduces the discriminator\u2019s sensitivity toward style. It is worth noting that FSMR not only consistently reduces the sensitivity but also outperforms the competitor in all cases. This is a very meaningful result because FSMR does not use any external stylized dataset but it uses only the original images during training. We also observe that the lower relative distances agree with the lower FIDs within the same environment. We compare the time and memory costs in Table 1. FSMR requires 3.0 \u00187.4% extra training time, but the on- the-\ufb02y method requires 17.2 \u001826.8% extra training time for additional feedforward passes in image stylization. In ad- dition, the on-the-\ufb02y method requires 70.0 \u001887.5% extra GPU memory to hold pretrained networks and features for image stylization, but FSMR only adds negligible ( \u00182%) GPU memory. To avoid extra costs for the on-the-\ufb02y styl- ization during training, we can prepare the stylized datasets before training ( i.e., different approach but has the same ef- fect as the on-the-\ufb02y stylization). However, the one-to-many stylization in advance requires heavy computation and pro- hibitively large storage as shown in Table 2. For example, to construct the stylized dataset for 1024 \u00021024 FFHQ with 100 style references, we need to process and store more than 7.0M (70k\u0002100) images (8.93TB). As an ablation study, we push toward harsher regular- ization: using randomly shifted feature maps instead of FSM. We observe that using arbitrary mean and standard deviation in AdaIN (Eq. 5) signi\ufb01cantly hampers adversar- 5+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Figure 4. The relative distance of the discriminators on CIFAR-10, FFHQ, and AFHQ. We observe a positive correlation with FID in each case. See Appendix F for more results on other baselines and datasets. MethodStandard dataset Costs CIFAR-10 FFHQ AFHQ CelebA-HQ LSUN Church Time (Hours) Memory (GB) DCGAN 15.89 \u00060.12 7.82\u00060.10 17.27\u00060.13 6.71\u00060.09 17.33\u00060.11 25.4 (1.5y) 5 (4y) DCGAN w/ on-the-\ufb02y 15.88 \u00060.11 7.33\u00060.17 14.22\u00060.15 5.41\u00060.10 26.05\u00060.14 31.5 (1.8y) 8.5 (7.5y) DCGAN w/ FSMR 14.98\u00060.09 6.76\u00060.08 13.19\u00060.09 5.23\u00060.10 13.84\u00060.10 26.2 (1.6y) 5.1 (4y) bCRGAN 12.46 \u00060.09 6.43\u00060.08 some example stylized images from AFHQ [5]. We de\ufb01ne style distancedsbetween images with different styles and the same content. The content distance dcis de\ufb01ned vice versa: ds(c;s1;s2)|{z} style distance=d(T(c;s1);T(c;s2)); (1) (a) Style distance and content distance (b) Relative distance Figure 1. (a) The style transfer method T(c;s)transfers the style ofson the content of c. We de\ufb01ne style distance as the output difference due to style variations. Content distance is de\ufb01ned vice versa. (b) Relative distance across various GAN methods. Relative distance indicates how sensitive a discriminator is to style changes (Eq. 3). See Section 2 for details. dc(s;c1;c2)|{z} content distance=d(T(c1;s);T(c2;s)); (2) whereT(c;s)transfers the style of the reference image s 2RC\u0002H\u0002Wto the content image c2RC\u0002H\u0002W, andd measures cosine distance in the last feature vectors of the discriminator. In practice, we use adaptive instance normal- ization (AdaIN) [13] as T. Figure 1 illustrates the process of calculating the content and style distances in Eq. (1) and (2). 2+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Figure 2. Overview of feature statistics mixing regularization (Section 3.2). Within the forward pass in the discriminator, we perturb features by applying AdaIN with a different sample. In deeper layers, the perturbations are applied recursively. A scalar \u0018Uniform (0;1) moderates their strength. Then we enforce similarity between the original output and the perturbed one. As we do not have a unit of measurement, we compute relative distance \u001a,i.e., the style distance divided by the content distance: \u001a|{z} relative distance=E c1;c22C; s1;s22S\u0014ds(c1;s1;s2) dc(s1;c1;c2)\u0015 ; (3) where CandSdenote the training dataset and an external style dataset, respectively. The larger the \u001avalue, the more sensitive the discriminator is to style when classifying real and fake images. We will use the relative distance \u001afor fur- ther analysis from here on. Our goal is to reduce the style distance so that the discriminators consider contents more important and produce richer gradients to the generators. The relative distances of ImageNet-pretrained ResNet50 and ResNet50 pretrained for classifying Stylized ImageNet [7] supports validity of the metric. As the relative distance of the latter is less than the former and the latter is proven to be less biased toward style, we argue that the discrimi- nators with lower relative distance are less sensitive to style (\ufb01gures are deferred to Section 4.2). 2.2. Baseline: On-the-\ufb02y stylization A well-known technique for preventing the classi\ufb01ers from being biased toward styles is to augment the images with their style-transferred versions, especially using theWikiArt dataset [1] as style references [7]. It works because the style transfer does not alter the semantics of the origi- nal images or the anticipated output of the network. On the other hand, in GAN training, style transfer drives the im- ages out of the original data distribution, thus it changes the anticipated output of the discriminator [19]. There are two workarounds for such a pitfall: 1) applying stochastic aug- mentations for both real and fake data [19, 42] and 2) pe- nalizing the output difference caused by the augmentation instead of feeding the augmented images to the discrimina- tor [41, 43]. As our goal is to make the discriminator less sensitive to style changes, we take the second approach as a straightforward baseline, for example, imposing consis- tency on the discriminator between the original images c and their randomly stylized images T(c;s)by Lconsistency =Ec;s\u0002 (D(c)\u0000D(T(c;s)))2\u0003 ; (4) whereD(\u0005)denotes the logit from the discriminator. How- ever, it raises other questions and dif\ufb01culties: the criteria for choosing the style images are unclear, and stylizing each image on-the-\ufb02y requires additional costs and an external dataset. Another option is to prepare a stylized dataset in- stead of on-the-\ufb02y stylization but it further requires pro- hibitively large storage. To combat this, we propose an ef\ufb01- cient and generally effective method, feature mixing statis- tics regularization, whose details are described in the next 3+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 section. 3. Proposed method We \ufb01rst describe the traditional style transfer algo- rithm, AdaIN, as a preliminary. Then, we discuss how our proposed method, feature statistics mixing regularization (FSMR), incorporates AdaIN to induce the discriminator to be less sensitive to style. 3.1. Preliminary: AdaIN Instance normalization (IN) [35] performs a form of style removal by normalizing feature statistics. Adaptive in- stance normalization (AdaIN) [13] extends IN to remove the existing style from the content image and transfer a given style. Speci\ufb01cally, AdaIN transforms content feature maps xinto feature maps whose channel-wise mean and variance are the same as those of style feature maps y: AdaIN (x;y) =\u001b(y)\u0012x\u0000\u0016(x) \u001b(x)\u0013 +\u0016(y); (5) where x;y2RC\u0002H\u0002Ware features obtained by a pre- trained encoder, and \u0016(\u0001)and\u001b(\u0001)denote their mean and standard deviation their spatial dimensions, calculated for each channel, respectively. Then, through a properly trained decoder, the transformed features become a stylized image1. Much work has adopted AdaIN within the generator for im- proving the generation performance [5, 14, 20, 22, 23, 25]. On the contrary, our proposed method (FSMR) employs it within the discriminator for ef\ufb01cient regularization, as de- scribed below. 3.2. Feature statistics mixing regularization Our goal is to make the discriminator do not heavily rely on the styles of the input images, without suffering from the dif\ufb01culties of the on-the-\ufb02y stylization (Section 2.2). Hence, we propose feature statistics mixing regularization (FSMR), which does not require any external dataset and can be ef\ufb01ciently implemented as per-layer operations in the discriminator. FSMR mixes the mean and standard de- viation of the intermediate feature maps in the discrimina- tor using another training sample and penalizes discrepancy between the original output and the mixed one. Speci\ufb01cally, we de\ufb01ne feature statistics mixing (FSM) for feature maps xwith respect to feature maps yto be AdaIN followed by linear interpolation: FSM(x;y) = x+ (1\u0000 )AdaIN (x;y); (6) 1AdaIN may denote the full stylization process but it denotes the oper- ation on the feature maps (Eq. 5) in this paper.Algorithm 1 FSM Pseudocode, Tensor\ufb02ow-like # N: batch size, H: height, W: width, C: channels def FSM(x, y, eps=1e-5): x_mu, x_var = tf.nn.moments(x, axes=[1,2]) y_mu, y_var = tf.nn.moments(y, axes=[1,2]) # normalize x_norm = (x - mu) / tf.sqrt(var + eps) # de-normalize x_fsm = x_norm *tf.sqrt(y_var + eps) + y_mu # combine alpha = tf.random.uniform(shape=[]) x_mix = alpha *x + (1 - alpha) *x_fsm return x_mix # NxHxWxC where \u0018Uniform (0;1)controls Feature Statistics Mixing Regularization for Generative Adversarial Networks Junho Kim1Yunjey Choi1Youngjung Uh2y 1NA VER AI Lab2Yonsei University Abstract In generative adversarial networks, improving discrimi- nators is one of the key components for generation perfor- mance. As image classi\ufb01ers are biased toward texture and debiasing improves accuracy, we investigate 1) if the dis- criminators are biased, and 2) if debiasing the discrimina- tors will improve generation performance. Indeed, we \ufb01nd empirical evidence that the discriminators are sensitive to the style ( e.g., texture and color) of images. As a remedy, we propose feature statistics mixing regularization (FSMR ) that encourages the discriminator\u2019s prediction to be invari- ant to the styles of input images. Speci\ufb01cally, we generate a mixed feature of an original and a reference image in the discriminator\u2019s feature space and we apply regulariza- tion so that the prediction for the mixed feature is consis- tent with the prediction for the original image. We conduct extensive experiments to demonstrate that our regulariza- tion leads to reduced sensitivity to style and consistently improves the performance of various GAN architectures on nine datasets. In addition, adding FSMR to recently- proposed augmentation-based GAN methods further im- proves image quality. Our code is available at https: //github.com/naver-ai/FSMR . 1. Introduction Generative adversarial networks (GANs) [8] have achieved signi\ufb01cant development over the past several years, enabling many computer vision and graphics appli- cations [4, 5, 14, 22, 23, 25, 31, 44]. On top of the care- fully designed architectures [3, 18, 20, 21, 30, 32, 40], GAN- speci\ufb01c data augmentation and regularization techniques have been keys for improvements. Regularization tech- niques [9,15\u201317,28,29,41,43] stabilize the training dynam- ics by penalizing steep changes in the discriminator\u2019s output within a local region of the input. On the other hand, data augmentation techniques [19, 42] prevent the discriminator from over\ufb01tting as commonly adopted in classi\ufb01cation do- yCorresponding author.mains. Note that both efforts aim to guide the discrimina- tor not to \ufb01xate on particular subsets of observations and to generalize over the entire data distribution. Texture has been shown to provide a strong hint for clas- si\ufb01ers [6,7,10]. If such a hint is suf\ufb01cient enough to achieve high accuracy, the models tend not to learn the complexity of the intended task [2]. As the GAN discriminators are in- herently classi\ufb01ers, we presume that they also tend to rely on textures to classify real and fake images. Accordingly, the generators would focus on synthesizing textures which are regarded as real by the biased discriminator. In this pa- per, we answer the two questions: 1) are discriminators sen- sitive to style ( e.g., texture and color)? and 2) if yes, will debiasing the discriminators improve the generation perfor- mance? To answer the \ufb01rst question, we de\ufb01ne style distance as shown in Figure 1a. An ideal discriminator would produce small style distance because the two images have the same content. As we do not have a unit of measurement, we com- pute relative distance: the style distance divided by the con- tent distance. In other words, we measure the sensitivity to style as multiples of the distance between images with dif- ferent content. Surprisingly, Figure 1b shows that all base- lines have noticeable values in relative distance. To answer the second question, we debias the discrim- inators and measure improvements in generative perfor- mance. A straightforward approach for debiasing is to sup- press the difference in the discriminator\u2019s output with re- spect to the style changes of the input image. Indeed, we observe that imposing a consistency loss [41,43] on the dis- criminator between the original image and its stylized ver- sion improves the generator as mimicking contents becomes easier than mimicking style to fool the discriminator. However, this approach leads to other dif\ufb01culties: the cri- teria for choosing style images are unclear, and stylizing all training images with various style references requires a huge computational burden and an external style dataset. To ef\ufb01ciently address the style bias issue, we propose feature statistics mixing regularization (FSMR ) which encourages the discriminator\u2019s prediction to be invariant to the styles of input images by mixing feature statistics within the discrim- 1arXiv:2112.04120v2 [cs.CV] 25 Mar 2022+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 inator. Speci\ufb01cally, we generate mixed features by combin- ing original and reference features in the discriminator\u2019s in- termediate layers and impose consistency between the pre- dictions for the original and the mixed features. In the experiments, we show that FSMR indeed induces the discriminator to have reduced sensitivity to style (Sec- tion 4.1). We then provide thorough comparisons to demon- strate that FSMR consistently improves various GAN meth- ods on benchmark datasets (Section 4.2). Our method can be easily applied to any setting without burdensome prepa- ration. Our implementation and models will be publicly available online for the research community. Our contribu- tions can be summarized as follows: \u2022 To the best of our knowledge, our work is the \ufb01rst style bias analysis for the discriminator of GANs. \u2022 We de\ufb01ne the relative distance metric to measure the sensitivity to the styles (Section 2). \u2022 We propose feature statistics mixing regularization (FSMR), which makes the discriminator\u2019s prediction to be robust to style (Section 3). \u2022 FSMR does not use external style images and outper- forms the straightforward solution with external style images (Section 4.1). \u2022 FSMR improves \ufb01ve baselines on all standard and small datasets regarding FID and relative distance (Section 4.2, 4.3). 2. Style-bias in GANs Our work is motivated by the recent \ufb01nding that CNNs are sensitive to style rather than content, i.e., ImageNet- trained CNNs are likely to make a style-biased decision when the style cue and content cue have con\ufb02ict [7]. To quantitatively measure how sensitive a discriminator is to style, we compute style distance, content distance, and then relative distance. Afterward, we describe a straightforward baseline solution to reduce the discriminator\u2019s distance to style. 2.1. Style distance and content distance We de\ufb01ne a quantitative measure for how sensitive a dis- criminator is sensitive to style. First, given a set of training images, we utilize a style transfer method to synthesize dif- ferently stylized images of the same content. The styles are randomly chosen from WikiArt [1]. Figure 1a shows 9.35\u00060.10 4.31\u00060.09 13.20\u00060.10 26.1 (1.6y) 5 (4y) bCRGAN w/ on-the-\ufb02y 12.43 \u00060.10 5.20\u00060.09 8.63\u00060.12 3.47\u00060.09 10.51\u00060.10 33.1 (1.9y) 8.5 (7.5y) bCRGAN w/ FSMR 11.17\u00060.07 4.68\u00060.08 8.33\u00060.08 3.43\u00060.09 9.09\u00060.07 27.7 (1.7y) 5.1 (4y) Table 1. FID comparison on DCGAN variants with FSMR and the baseline on-the-\ufb02y stylization. The bold numbers indicate the best FID for each baseline. We report the mean FID over 3 training runs together with standard deviations and the additional costs. All image resolutions are set to 128\u0002128due to the backbone architecture except CIFAR-10 ( 32\u000232). Time and memory are measured in 128\u0002128 images, andyindicates what is measured in 32\u000232images. Time means a full training time. CIFAR-10 CelebA-HQ FFHQ AFHQ LSUN Church Time 8 10 30 5 40 Table 2. The time to create the stylized dataset for each standard dataset, measured in hours. ial training between a generator and a discriminator, i.e., the training diverges. On the other hand, FSMR using in- domain samples shows the anticipated effect. 4.2. Standard datasets We evaluate the effectiveness of FSMR on three bench- mark datasets, all of which have more than 10k training im- ages: CIFAR-10 (50k), FFHQ (70k), and AFHQ (16k). Ta- ble 3 (left) shows that FSMR consistently improves Style- GAN2 even with existing augmentation techniques [19,42]. We emphasize that FSMR enhances baselines by a large gap on AFHQ, in which case the discriminator might be easily biased toward color and texture of the animals. Figure 5 shows the relative distances on CIFAR-10,FFHQ, and AFHQ for StyleGAN2 variants. FSMR reduces the relative distances in all cases and they agree with the im- provements in FID. We also provide the relative distances of ResNet50 networks pretrained on ImageNet and Stylized ImageNet as references in each dataset (Section 2.1). As the lower relative distances agree with the higher classi\ufb01cation performances, the lower relative distances of the discrimi- nator agree with the higher generativ", "meta_data": {"split_id": 2}, "response": "I would evaluate the text as having a moderate educational value. The text provides some basic information on generative adversarial networks (GANs) and feature statistics mixing regularization (FSMR), which is relevant to educational topics in computer science and machine learning. However, the text does not align closely with educational standards, and the writing style is not coherent or focused, making it difficult to extract specific educational content.\n\nI would score the text as follows:\n\n* The text provides some basic information relevant to educational topics (1 point).\n* The text addresses certain elements pertinent to education but does not align closely with educational standards, and the writing style is not coherent or focused (2 points).\n* The text does not introduce key concepts relevant to school curricula, and it is not suitable for structured learning (0 points).\n* The text is not clear and consistent in its writing style, and it is not easy to follow (0 points).\n* The text is not relevant or beneficial for educational purposes at the primary school or grade school level (0 points).\n\nTherefore, I would score the text as 3 out of 5 points.\n\nEducational score: 3", "prediction": 3}
{"text": "", "meta_data": {"split_id": 1}, "response": "I apologize, but you didn't provide the extract. Please provide the text, and I'll be happy to evaluate its educational value and score it using the additive 5-point system.\n\nOnce I receive the extract, I'll provide a justification for my total score in up to 100 words and conclude with the score in the format \"Educational score: <total points>\".", "prediction": -1}
{"text": "models have substantially different decision re- gions from convolutional models and from each other. \u2022 Wider convolutional models appear to have higher reproducibility in their decision regions, with WideRN30 being both the widest and most repro- ducible model in this study. WideRN30 WideRN20 WideRN10ResNet18 DenseNetVGGViT MLPMixerFullyConWideRN30 WideRN20 WideRN10 ResNet18 DenseNet VGG ViT MLPMixer FullyCon0.87 0.85 0.85 0.82 0.81 0.78 0.63 0.61 0.45 0.85 0.86 0.85 0.82 0.81 0.78 0.63 0.6 0.44 0.85 0.85 0.86 0.81 0.81 0.78 0.63 0.6 0.44 0.82 0.82 0.81 0.83 0.81 0.78 0.63 0.61 0.45 0.81 0.81 0.81 0.81 0.82 0.77 0.64 0.61 0.44 0.78 0.78 0.78 0.78 0.77 0.79 0.63 0.6 0.44 0.63 0.63 0.63 0.63 0.64 0.63 0.75 0.64 0.47 0.61 0.6 0.6 0.61 0.61 0.6 0.64 0.67 0.46 0.45 0.44 0.44 0.45 0.44 0.44 0.47 0.46 0.69 0.450.500.550.600.650.700.750.800.85 Reproducibility score Figure 4. Reproducibility across several popular architectures. \u2022 Skip connections have little impact on the shape of decision regions. ResNet (with residual connections across blocks), DenseNet (with many convolutional connections within blocks), and VGG (no skip connec- tions) all share very similar decision regions. However it is worth noting that skip connection architectures achieve slightly higher reproducibility scores than the very wide VGG network. 3.3. Does distillation preserve decision boundaries? Distillation [19] involves training a student model on the outputs of an already trained teacher model. Some believe that distillation does indeed convey information about the teacher\u2019s decision boundary to the student [15], while oth- ers argue distillation improves generalization through other mechanisms [34]. We calculate the relative similarity of the student\u2019s decision boundary to its teacher\u2019s boundary and compare this to the similarity between teacher network and a network of the student\u2019s architecture and initializa- tion but trained in a standard fashion. Across the board, dis- tilled students exhibit noticeably higher similarity to their teachers compared with their vanilla trained counterparts. In Figure 5, we see that almost every student-teacher com- bination has a higher reproducibility score than the same teacher compared to an identically initialized model trained without distillation. 3.4. The effect of the optimizer In addition to the in\ufb02uence of initialization, data order- ing, and architecture, the choice of optimizer/regularizer used during training can greatly impact the resulting model [13]. Thus, we study the effect of optimizer choice on the reproducibility of a network\u2019s decision boundary. In Table 1, we can see that SAM [12] induces more repro- ducible decision boundaries than standard optimizers such as SGD and Adam. This observation suggests that SAM has a stronger regularization effect. However, more reg- 4+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 ResNet WideRN10DenseNetVGG ViT-pt Distillation StudentResNet WideRN10 DenseNet VGG ViT-ptTeacher0.87 0.87 0.86 0.85 0.77 0.86 0.86 0.86 0.82 0.75 0.86 0.86 0.87 0.82 0.75 0.81 0.81 0.81 0.82 0.73 0.76 0.76 0.76 0.75 0.72Distillation ResNet WideRN10DenseNetVGG ViT-pt Vanilla Trained Model0.82 0.82 0.81 0.78 0.75 0.82 0.84 0.82 0.78 0.76 0.81 0.82 0.83 0.78 0.75 0.78 0.78 0.78 0.8 0.73 0.75 0.76 0.75 0.73 1Vanilla Training 0.650.700.750.800.85 Reproducibility Score *Figure 5. Differences in reproducibility comparing distilled model to vanilla trained model. *The reproducibility score is not appli- cable for this diagonal entry because we start from the same pre- trained model. Reproducibility Adam SGD SGD + SAM ResNet-18 79.81% 83.74% 87.22% VGG 81.19% 80.92% 84.21% MLPMixer 67.80% 66.51% 68.06% VIT 69.55% 75.13% 75.19% Test Accuracy Adam SGD SGD + SAM ResNet-18 93.04 95.30 95.68 VGG 92.87 93.13 93.90 MLPMixer 82.22 82.04 82.18 VIT 70.89 75.49 74.72 Table 1. Reproducibility of different models when using different optimizers. SGD produces more reproducible decision boundaries relative to Adam, and SGD+SAM almost always consistently in- crease reproducibility of the model relative to SGD. ularization doesn\u2019t always mean better test accuracy. For example, for MLPMixer and ViT, using SAM does not al- ways achieve the highest test accuracy but does achieve the highest reproducibility. 4. Double descent In classical learning theory, it is thought that models with too few parameters (e.g., low width) generalize poorly be- cause they are not expressive enough to \ufb01t the data, while models with too many parameters generalize poorly be- cause of over-\ufb01tting. This is known as the Bias-Variance trade-off [14]. In contrast, the strong inductive bias of neu- ral networks enables them to achieve good performance even with extremely large numbers of parameters. Belkin et al. [4] and Nakkiran et al. [27] have shown that under the right training conditions, we can see neural models oper- 024681012141618202224262830323436384042444648505254565860626466 Width parameter, k0.10.20.30.4Test error Label Noise 0 20Figure 6. Test error curves with 0 and 20% label noise in training. ating in both the classical and over-parameterized regimes. This is depicted in Fig. 6, which plots test error as a func- tion of model width on CIFAR-10. We observe a classic U- shaped curve for widths less than 10 (the underparametrized regime). For models of width greater than 10, the test er- ror fall asymptotically (overparametrized regime). This be- haviour is referred to as \u201cdouble descent\u201d and discussed in generality in Belkin et al. [4]. Between the two regimes is a model that lives at the \u201cinterpolation threshold\u201d; here, the model has too many parameters to bene\ufb01t from clas- sical simplicity bias, but too few parameters to be regular- ized by the inductive bias of the over-parameterized regime. Double descent has been studied rigorously for several sim- ple and classical model families, including kernel methods, linear models, and simple MLPs [2, 6, 16, 26, 29, 30, 33]. Double descent is now well described for linear models and random feature networks in [1, 6, 10, 16]. In the classical regime, bias decreases with increased model complexity, while the variance increases at the same time, resulting in a U-shaped curve. Then, in the overparameterized regime, the variance decreases rapidly while bias remains low [28, 37]. In our studies above, we visualized the over-parameterized regime and saw that models become highly reproducible, with wide architectures producing nearly identical models across training runs. These visualizations captured the low- variance of the over-parameterized regime. In this section, our goal is to gain insight into the model behaviors that emerge at the interpolation threshold, caus- ing double descent. We observe closely what is happening at critical points (i.e., the transition between Can Neural Nets Learn the Same Model Twice? Investigating Reproducibility and Double Descent from the Decision Boundary Perspective Gowthami Somepalli1, Liam Fowl1, Arpit Bansal1, Ping Yeh-Chiang1, Yehuda Dar2, Richard Baraniuk2, Micah Goldblum3, Tom Goldstein1 1University of Maryland, College Park fgowthami, pchiang, tomg g@cs.umd.edu lfowl@math.umd.edu ,bansal01@umd.edu2Rice University fydar, richb g@rice.edu3New York University goldblum@nyu.edu Abstract We discuss methods for visualizing neural network deci- sion boundaries and decision regions. We use these visual- izations to investigate issues related to reproducibility and generalization in neural network training. We observe that changes in model architecture (and its associate inductive bias) cause visible changes in decision boundaries, while multiple runs with the same architecture yield results with strong similarities, especially in the case of wide architec- tures. We also use decision boundary methods to visualize double descent phenomena. We see that decision boundary reproducibility depends strongly on model width. Near the threshold of interpolation, neural network decision bound- aries become fragmented into many small decision regions, and these regions are non-reproducible. Meanwhile, very narrows and very wide networks have high levels of re- producibility in their decision boundaries with relatively few decision regions. We discuss how our observations re- late to the theory of double descent phenomena in convex models. Code is available at https://github.com/ somepago/dbViz . 1. Introduction The superiority of neural networks over classical linear classi\ufb01ers stems from their ability to slice image space into complex class regions. While neural network training is cer- tainly not well understood, existing theories of neural net- work training mostly focus on understanding the geometry of loss landscapes [5, 8, 25]. Meanwhile, considerably less is known about the geometry of class boundaries. The ge- ometry of these regions depends strongly on the inductive bias of neural network models, which we do not currently have tools to rigorously analyze. To make things worse, the Fully ConnectedNetworkVision TransformerWideResNettrial #1trial #2 Ground truth:AirplaneFrogBirdImages: Figure 1. The class boundaries of three architectures, plotted on the plane spanning three randomly selected images. Each model is trained twice with random seeds. Decision boundaries are repro- ducible across runs, and there are consistent differences between the class regions created by different architectures. inductive bias of neural networks is impacted by the choice of architecture, which further complicates theoretical anal- ysis. In this study, we use empirical tools to study the geom- etry of class regions, and how neural architecture impacts inductive bias. We do this using visualizations and quanti- tative metrics calculated using realistic models. We start by presenting simple methods for decision boundary visualiza- tion. Using visualization as a tool, we do a deep dive on three main issues: \u2022 Do neural networks produce decision boundaries that are consistent across random initializations? Put sim- ply, can a neural network learn the same model twice? We see empirically that the decision boundaries of a network have strong similarities across runs, and we con\ufb01rm this using quantitative measurements. 1arXiv:2203.08124v1 [cs.LG] 15 Mar 2022+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 \u2022 Do different neural architectures have measurable dif- ferences in inductive bias? Indeed, we \ufb01nd clear vis- ible differences between the class regions of different model architectures (e.g., ResNet-18 vs ViT). \u2022 We use decision boundary visualizations to investigate the \u201cdouble descent\u201d phenomenon. We see that de- cision boundaries become highly unstable and frag- mented when model capacity is near the interpolation threshold, and we explore how double descent in neu- ral networks relates to known theory for linear models. 2. Plotting decision boundaries Most prior work on decision boundary visualization is for the purpose of seeing the narrow margins in adversar- ial directions [18, 21]. Fawzi et al. [11] visualize the topo- logical connectivity of classi\ufb01cation regions. To facilitate our studies, we seek a general-purpose visualization method that is simple, controllable, and captures import parts of de- cision space that lie near the data manifold. 2.1. On-manifold vs off-manifold behavior When plotting decision boundaries, it is important to choose a method that captures the behavior of models near the data manifold. To understand why, consider the plots of decision boundaries through planes spanning randomly chosen points in input space as shown in Figure 2. We see that decision regions are extremely smooth and uniform with few interesting features. The training process, which structures decision boundaries near the data manifold (e.g. Fig. 1), fails to produce strong structural effects far from the manifold (e.g. Fig. 2). The uniform off-manifold behavior is not particular to our training method or architecture but is rather an in- evitable consequence of the concentration of measures phe- nomenon [24, 31]. In fact, we can show that any neural network that varies smoothly as a function of its input will assume nearly constant outputs over most of input space. The proof of the following result is in Appendix A. Lemma 2.1 Letf: [0;1]n![0;1]be a neural network satisfyingjf(x)\u0000f(y)j\u0014Lpnkx\u0000yk:Let\u0016fdenote the median value of fon the unit hypercube. Then, for an image x2[0;1]nof uniform random pixels, we have jf(x)\u0000\u0016fj\u0014 twith probability at least 1\u0000Le\u00002\u0019nt2=L2 \u0019tpn: 2.2. Capturing on-manifold behavior The lemma above shows the importance of capturing the behavior of neural networks near the data manifold. Unfor- tunately, the structure of image distributions is highly com- plex and dif\ufb01cult to model. Rather than try to identify and Dog,Frog,HorseTruck,Airpl,CatAirpl,Ship,CatViTDenseNetResNet-18 Figure 2. Off-manifold decision boundaries near \u201crandom\u201d images created by shuf\ufb02ing pixels in CIFAR-10 images. Each column\u2019s title shows the labels of the unshuf\ufb02ed base images. Below each column we show the shuf\ufb02ed image triplet. Color-class mapping is as follow Red:Frog, Green:Bird, Orange:Automobile. \ufb02atten the complex structures on which images lie, we take an approach that is inspired by the recent success of the highly popular paper on the mixup regularizer [40], which observed that, in addition to possessing structure near the data manifold, decision boundaries are also structured in the convex hull between pairs of data points . We take a page from the mixup playbook and plot de- cision boundaries along the convex hull between data sam- ples. We \ufb01rst sample a triplet (x1;x2;x3)\u0018D3of i.i.d. im- ages from the distribution D. Then, we construct the plane spanned by the vectors ~ v1=x2\u0000x1,~ v2=x3\u0000x1and plot the decision boundaries in this plane. To be precise, we sample inputs to the network with coordinates \u0001max(~ v1\u0001~ v1;jproj~ v1~ v2\u0001~ v1j)~ v1+ (~ v2\u0000proj~ v1~ v2) for\u00000:1\u0014 ; \u00141:1. This plotting methods using planes has several advantages. It shows the regions surrounding multiple data points at once and also the decision bound- aries between their respective classes, using just one plot. Furthermore, these classes can be chosen by the user. It also focuses on the convex hull between points rather than random directions that may point away from the manifold. Figure 1 shows decision regions plotted along the plane spanned by three data points chosen at random from the Air- plane ,Frog , and Bird classes of CIFAR-10 [23]. In these plots, each color represents a class label. Same color-class schema is maintained through out the paper and can be seen in legends of multiple plots. 2+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 Fully ConnectedMLP MixerViTVGGDenseNetResNet-18WideResNettrial #1trial #2trial #3Ground truth:AirplaneFrogBird Figure 3. Decision regions through a triplet of images, for various architectures (columns) and initialization seeds (rows). 2.3. Experimental Setup: Architectures used: We select several well-known net- works from diverse architecture families1. We con- sider a simple Fully Connected Network with 5 hid- den layers and ReLU non-linearities, DenseNet-121 [20], ResNet-18 [17], WideResNet-28x10, WideResNet-28x20, WideResNet-28x30 [38], ViT [9], MLPMixer [35], and VGG-19 [32]. For fast training, our ViT has only 6lay- ers,8heads, and patchsize 4. The custom MLPMixer we use has 12hidden layers with hidden embedding dimension 512 and patch size 4. Unless otherwise stated, architec- tures are trained for 100epochs using SGD optimizer, and 3multi-step learning rate drops. Random Crop and Hor- izontal Flip data augmentations are used in training. For distillation experiments, we also use a ViT-S/16 pretrained on ImageNet [7] as a teacher [36]. Some experiments use the Sharpness-Aware Minimization (SAM) optimizer [12] adversarial radius set to \u001a= 0:01. We select learning rates using a grid search across f 0.001, 0.002, 0.005, 0.01, 0.02, 0.05 gfor each architec- ture and optimizer (Adam [22] and SGD) combination, and training for 200 epochs. Mean test accuracy over 3 runs per model is reported in Table 1. 3. Model reproducibility and inductive bias It is known that neural networks can easily over\ufb01t com- plex datasets, and can even interpolate randomly labeled images [39]. Despite this \ufb02exibility, networks have an im- portant inductive bias \u2013 they have a strong tendency to con- verge on decision boundaries that generalize well. Our goal in this section is to display the inductive bias phenomenon 1Architecture implementations from https://github.com/ kuangliu / pytorch - cifar andhttps : / / github . com / lucidrains/vit-pytorchusing decision boundary visualizations. We ask two ques- tions: \u2022 Can a model replicate the same decision boundaries twice, given different random initializations? \u2022 Are there disparities between the inductive biases of different model families that result in different decision boundaries? Below, we consider various sources of inductive bias, in- cluding neural architecture family, network width, and the choice of optimizer. 3.1. Inductive bias depends on model class We choose three random images from the CIFAR-10 training set, construct the associated plane through input space, and plot the decision regions for 7 different archi- tectures in Figure 3. For each model, we run the training script three times with different random initializations. Several interesting trends emerge in this visualization. First, we observe systematic differences between model families. Convolutional models all share similar decision boundaries, while the boundaries of Fully Connected Nets, ViT, and MLP Mixer share noticeable differences. For ex- ample, ViT and MLP Mixer consistently show the presence of an orange \u201cAutomobile\u201d region that CNNs do not. Fully Connected Nets show considerably more complex and frag- mented decision regions than other model families. At the same time, we observe strong reproducibility trends across runs with different random seeds. This trend is particularly high for convolutional architectures, and the effect is quite strong for WideResNet, which leads us to hy- pothesize that there may a link between model width and reproducibility \u2013 an issue that we will investigate in more detail below. 3+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 3.2. Quantitative analysis of decision regions The visualizations in Figure 3 suggest that reproducibil- ity is high within a model class, while differences in induc- tive bias result in low similarities across model families. To validate our intuitions, we use quantitative metrics derived from the decision plots averaged over many trials to provide a more sensitive and conclusive analysis. Reproducibility Score: We de\ufb01ne a metric of similarity between the decision boundaries of pairs of models. We \ufb01rst sample triplets Ti= (x0;x1;x2)iof i.i.d. images from the training distribution. Let Sibe the set of points in the plane de\ufb01ned byTiat which the decision regions are evaluated. We de\ufb01ne the reproducibility score: R(\u00121;\u00122) =ETi\u0018D\u0014 (jf(Si;\u00121)\\f(Si;\u00122)j)=jSij\u0015 (1) where for notation simplicity we denote the set of class predictions within each decision region as f(Si;\u0012) = f(x;f(x;\u0012))gx2Sifor a model with parameters \u0012. Practi- cally, we estimate the expectation in Eq. (1) by sampling 500 triplets and 2500 points in each truncated plane for a total of 1:25Mforward passes. Simply put, this corre- sponds to the \u201cintersection over union\u201d score for two deci- sion boundary plots. This score can quantify reproducibility of decision re- gions across architectures, initializations, minibatch order- ing, etc. In earlier work [3], variability of the decision boundaries is studied by examining the similarity of pre- dictions at test points. In contrast, our method gives a much richer picture of the variance of the classi\ufb01cation regions not just at the input points, but also in the regions around them and can be applied to both train and test data. Measuring architecture-dependent bias We apply the reproducibility score to measure model similiarity between different training runs with the same architecture and across different architectures. For each model pair, we compute the reproducibility score across 5 different training runs and 500 local decision regions, each containing 2,500 sampled points (6.25M total forward passes compared). Figure 4 shows reproducibility scores for various archi- tectures, and we see that quantitative results strongly re\ufb02ect the trends observed in the decision regions of Figure 3. In particular, it becomes clear that \u2022 The inductive biases of all the convolutional architec- tures are highly similar. Meanwhile, MLPMixer, ViT and FC the under and overparameterized regimes), and how the class boundaries transition as we increase the capacity of the model class. We \ufb01nd that the behaviour of class boundaries aligns with the bias-variance decomposition \ufb01ndings of [28, 37], how- ever the model instabilities that cause variance to spike in neural networks is manifested as a complex fragmentation of decision space that is not, to the best of our knowledge, described in the literature on classical models. Experimental setup: We follow the experimental setting from Nakkiran et al. [27] to replicate the double descent phenomenon for ResNet-18 [17]. We increase model capac- 5+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587 ity by varying the number of \ufb01lters in the convolutional lay- ers by a \u201cwidth\u201d parameter, k. Note that a standard ResNet- 18 model has k= 64 and lives in the over-parameterized regime on CIFAR-10. We train models with cross-entropy loss and the Adam optimizer with learning-rate 0.0001 for 4000 epochs. This gentle but long training regiment en- sures stability and convergence for the wide range of mod- els needed for this study. It was observed in [27] that label noise is important for creating easily observable double descent in realistic mod- els. We train two sets of models, one with a clean train- ing set and another with 20% label noise (uniform random incorrect class labels). In both cases, we use the standard (clean) test set. For noisy experiments, the same label er- rors are used across epochs and experiments. RandomCrop and RandomHorizontalFlip augmentations are used while training. We observe a pronounced double-descent when label noise is present. See Figure 6, which replicates the double-descent curve of Nakkiran et al. [27]. We focus on several important model widths: k= 4 is the local minimum of test error in the underparametrized regime, and k= 10 achieves peak error ( \u0019interpolation threshold) beyond which the test error will continually fall. We refer the reader to Appendix D for training error plots showing the onset of interpolation near k= 10: 4.1. How do decision boundaries change as we cross the interpolation threshold? In Figure 7, we plot decision boundaries for models trained with and without label noise and with varying ca- pacities. As above, visualizations take place in the plane spanned by three data points. We present examples using two different methods for sampling \u2013 one with all three images from the same class and one with three different classes. The three images are drawn from the training set and are correctly labeled (even for the experiments involv- ing label noise). Similar behaviours are observed for other randomly sampled images and with other combinations of classes. See Appendix D for additional examples. As we move from left to right in the \ufb01gure, model ca- pacity sweeps from k= 1(under-parameterized) to k= 64 (standard ResNet-18, which is over-parameterized). As the models become increasingly over-parameterized, the mod- els are getting con\ufb01dent about their predictions, as seen by the intensity of the color. When models are trained with clean labels, the model \ufb01ts all three points with high con- \ufb01dence by the time k= 4;and the decision boundaries change little beyond this point. The mechanism behind the error spike in the double de- scent curve is captured by the visualizations using label noise. In this case, the under-\ufb01tting behavior of the clas- sical regime is apparent at k= 4, as the model \ufb01ts only 1 out of 3 points correctly, and con\ufb01dence in predictions islow. When we reached k= 10 (the interpolation threshold), the model \ufb01ts most of the training data, including the three points in the visualization plane. As we cross this thresh- old, the decision regions become chaotic and fragmented. By the time we reach k= 20 , the fragmentation is reduced and class boundaries become smooth as we enter the over- parameterized regime. To re\ufb01ne our picture of double descent, we visualize the class boundaries at k= 10 for a range of different image triplets in Figure 8, both with and without label noise. We see that in the label noise case, where double descent is ob- served, there is a clear instability in the classi\ufb01cation behav- ior at the interpolation threshold. Let\u2019s now see what happens to the decision boundaries around mislabeled images. Figure 9 shows decision bound- aries around three points from the Automobile class, where one of the points is mislabeled in the training set. When k= 10 , we see chaotic boundaries. The mislabeled points are assigned their (incorrect) dataset label, but they are just barely interpolated in the sense that they lie very near the decision boundary. For k= 64 , the boundaries are seem- ingly regularized by inductive bias; the mislabeled points lie in the center of their respective regions, and boundaries are much more smooth. Having observed the qualitative behaviour of correctly labeled and mislabeled points in models with and without label noise at various capacities, we ask ", "meta_data": {"split_id": 2}, "response": "I'd be happy to evaluate the educational value of the provided extract.\n\nThe extract provides some basic information relevant to educational topics, such as decision boundaries, decision regions, and inductive bias. However, it also includes some technical jargon and complex concepts that may not be easily understandable for primary school or grade school students. The writing style is also quite formal and academic, which may not be engaging for younger students.\n\nI would award 2 points for this extract, as it provides some basic information on educational topics, but may not be suitable for primary school or grade school levels due to its technical complexity and formal tone.\n\nEducational score: 2", "prediction": 2}
