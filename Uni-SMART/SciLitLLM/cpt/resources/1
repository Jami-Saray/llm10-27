{mar Bhunia1Subhadeep Koley1,2Abdullah Faiz Ur Rahman Khilji*Aneeshan Sain1,2\nPinaki nath Chowdhury1,2Tao Xiang1,2Yi-Zhe Song1,2\n1SketchX, CVSSP, University of Surrey, United Kingdom.\n2iFlyTek-Surrey Joint Research Centre on Arti\ufb01cial Intelligence.\nfa.bhunia, s.koley, a.sain, p.chowdhury, t.xiang, y.song g@surrey.ac.uk\nAbstract\nSketching enables many exciting applications, notably,\nimage retrieval. The fear-to-sketch problem (i.e., \u201cI can\u2019t\nsketch\u201d) has however proven to be fatal for its widespread\nadoption. This paper tackles this \u201cfear\u201d head on, and for\nthe \ufb01rst time, proposes an auxiliary module for existing re-\ntrieval models that predominantly lets the users sketch with-\nout having to worry. We \ufb01rst conducted a pilot study that\nrevealed the secret lies in the existence of noisy strokes, but\nnot so much of the \u201cI can\u2019t sketch\u201d. We consequently design\na stroke subset selector that detects noisy strokes, leaving\nonly those which make a positive contribution towards suc-\ncessful retrieval. Our Reinforcement Learning based for-\nmulation quanti\ufb01es the importance of each stroke present\nin a given subset, based on the extent to which that stroke\ncontributes to retrieval. When combined with pre-trained\nretrieval models as a pre-processing module, we achieve a\nsigni\ufb01cant gain of 8%-10% over standard baselines and in\nturn report new state-of-the-art performance. Last but not\nleast, we demonstrate the selector once trained, can also be\nused in a plug-and-play manner to empower various sketch\napplications in ways that were not previously possible.\n1. Introduction\nThanks to the convenience of interactive touchscreen de-\nvices, sketch-based image retrieval (SBIR) [12, 13, 15, 39]\nhas emerged as a practical means of image research that\nis complementary to the conventional text-based retrieval\n[26]. Although initially developed for a category-level set-\nting [43, 37, 60], of late SBIR has undertaken a \ufb01ne-grained\nshift to better re\ufb02ect the inherent \ufb01ne-grained characteristics\n(pose, appearance detail, etc) of sketches [47, 57, 8].\nDespite great strides made [4, 34, 11], the fear-to-sketch\nhas proven to be fatal for its omnipresence \u2013 a \u201cI can\u2019t\nsketch\u201d reply is often the end of it. This \u201cfear\u201d is predom-\ninant for \ufb01ne-grained SBIR (FG-SBIR), where the system\ndictates users to produce even more faithful and diligent\n*Interned with SketchX\nFigure 1: (a) While the average ranking percentile increases\nas the sketching proceeds from starting towards completion,\nunwanted sudden drops have been noticed for many individ-\nual sketches due to noisy/irrelevant strokes drawn. (b) The\nsame thing is visualised with number of samples in the third\naxis to get an overall statistics on QMUL-Shoe-V2 dataset.\nsketches than that required for category-level retrieval [12].\nIn this paper, we tackle this \u201cfear\u201d head-on and pro-\npose for the \ufb01rst time a pre-processing module for FG-SBIR\nthat essentially let the users sketch without the worry of \u201cI\ncan\u2019t\u201d . We \ufb01rst experimentally show that, in most cases it\nis not about how bad a sketch is \u2013 most cansketch (even a\nrough outline) \u2013 the devil lies in the fact that users typically\ndraw irrelevant (noisy) strokes that are detrimental to the\noverall retrieval performance (see Section 3). This observa-\ntion has largely inspired us to alleviate the \u201ccan\u2019t sketch\u201d\nproblem by eliminating the noisy strokes through selecting\nan optimal subset that canlead to effective retrieval.\nThis problem might sound trivial enough \u2013 e.g., how\nabout considering all possible stroke subsets as training\nsamples to gain model invariance against noisy strokes? Al-\nbeit theoretically possible, the highly complex nature of this\nprocess (i.e.,O(2N)) quickly renders this naive solution\ninfeasible, especially when the number of strokes in free-\nhand sketches can range from an average of N= 9 to a\nmax ofN= 15 in \ufb01ne-grained SBIR datasets (QMUL-\nShoeV2/ChairV2 [57, 47]). Most importantly, augmenting\nthe training data by random stroke dropping would lead to\na noisy gradient during training. This is because out of all\npossible subsets, many of these augmented sketch subsetsarXiv:2203.14817v1  [cs.CV]  28 Mar 2022+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587\n\nare too coarse/incomplete to convey any meaningful infor-\nmation to represent the paired photo. Therefore, instead\nof naively learning the invariance, we advocate for \ufb01nding\nmeaningful subsets that can sustain ef\ufb01cient retrieval.\nOur solution generally rests with detecting noisy strokes\nand leaving only those that positively contribute to success-\nful retrieval. We achieve that by proposing a mechanism to\nquantify the importance of each stroke present in a given\nstroke-set, based on the extent to which that stroke is wor-\nthy for retrieval (i.e, makes a positive contribution). We\nwork on vector sketches[5] in order to utilise stroke-level\ninformation, and propose a sketch stroke subset selector\nthat learns to determine a binary action for every stroke \u2013\nwhether to include that particular stroke to the query stroke\nsubset, or not. The stroke subset selector is designed via\na hierarchical Recurrent Neural Network (RNN) that mod-\nels the compositional relationship among the strokes. Once\nthe stroke subset is obtained, it is \ufb01rst rasterized then passed\nthrough a pre-trained FG-SBIR model [57] to obtain a rank-\ning of target photos against the ground-truth photo. The\nmain objective is to select a particular subset that will rank\nthe paired ground-truth photo towards the top of the ranking\nlist. We use Reinforcement Learning (RL) based training\ndue to the non-differentiability of rasterization operation.\nAs explicit stroke-level ground-truth for the optimal subset\nis absent, we seek to train our stroke-subset selector with the\nhelp of pre-trained FG-SBIR for reward computation. In\nparticular, we use the actor-critic version of proximal policy\noptimisation (PPO) to train the stroke subset selector.\nApart from the main objective of noisy stroke elimina-\ntion, the proposed method also enables a few secondary\nsketch applications (Section 5) in a plug-and-play manner.\nFirst, we show that a pre-trained stroke selector can be used\nas astroke importance quanti\ufb01er to guide users to produce\na sketch \u201cjust\u201d enough for successful retrieval. Second, we\ndemonstrate that it can signi\ufb01cantly speed up existing works\non interactive \u201con-the-\ufb02y\u201d retrieval [8] removing the need\nfor incomplete rasterized sketch to be unnecessarily passed\nfor inference multiple times. Third, besides bene\ufb01ting FG-\nSBIR, our subset selector module can also act as a faithful\nsketch data augmenter over random stroke dropping with-\nout much computational overhead. That is, instead of costly\noperation like sketch deformation [59] or unfaithful approx-\nimation like edge/contour-map as soft ground-truths [10],\nusers can effortlessly generate nmost representative sub-\nsets to augment training for many downstream tasks.\nIn summary our contributions are, (a) We tackle the fear-\nto-sketch problem for sketch-based image retrieval for the\n\ufb01rst time, (b) We formulate the \u201ccan\u2019t sketch\u201d problem as\nstroke subset selection problem following detailed experi-\nmental analysis, (c) We propose a RL-based framework for\nstroke subset selection that learns through interacting with\na pre-trained retrieval model. (d) We demonstrate our pre-trained subset selector can empower other sketch applica-\ntions in a plug-and-plug manner.\n2. Related Works\nCategory-level SBIR: Category-level SBIR aims at re-\ntrieving category-speci\ufb01c photos from user given query\nsketches. Like any other retrieval system, Deep Neural\nNetworks have become a de-facto choice for any recent\nSBIR frameworks [15, 13, 37, 60, 12, 7] over early hand-\nengineered feature descriptors [50]. Overall, category level\nSBIR makes use of Siamese networks based on either CNN\n[12, 13], RNN [54], Transformer [37] or their combinations\n[12] along with a triplet-ranking objective to learn a joint\nembedding space. A distance metric is used to rank the\ngallery photos against the learned embedding space for a\ngiven query sketch for retrieval. Further efforts have been\nmade through zero-shot SBIR [13, 56] for cross-category\ngeneralisation, and employing binary hash-code embedding\n[29, 43] to reduce the computational complexity.\nFine-grained SBIR: Sketch holds a noteworthy advan-\ntage in its potential to depict \ufb01ne-grained properties of the\ntarget image, which are hard to describe via other query\nmediums [46] like text or attribute. Consequently, interest\nsurged in \ufb01ne-grained SBIR [57], which aims at instance-\nspeci\ufb01c matching for a user given query sketch. Initially\nstarting with graph-matching models [34], FG-SBIR re-\nsearch gained traction with the advent of various deep-\nlearning based approaches [57, 47, 8, 4]. Yu et al. [57] \ufb01rst\npioneered deep triplet-ranking based siamese networks for\nlearning a joint embedding space with instance-wise match-\ning criteria. This was further augmented via attention with\nhigher-order retrieval loss [47], cross-domain image gen-\neration [35], text tags [46], etc. Recent FG-SBIR works\ninclude advanced methods like hierarchical co-attention\n[40], reinforcement learning-based early retrieval [8], semi-\nsupervised generation-retrieval joint training [4], etc.\nWhile sketches are signi\ufb01cantly subjective to user\u2019s style\n[41] and vary considerably depending on the drawer\u2019s draw-\ning skill [8], these earlier works assumed the existing an-\nnotated \ufb01ne-grained dataset to be perfect . In other words, a\nrigid assumption is made that every annotated paired sketch\nis a perfect depiction of the paired photo. In this work, we\nargue that \u2018all sketches are sketchy\u2019 , which holds stronger\nsigni\ufb01cance for \ufb01ne-grained SBIR, as every stroke of an-\nnotated sketch [58] represents a speci\ufb01c part of the paired\nphoto, and the free-\ufb02ow nature of amateur sketching is\nlikely to introduce noise no matter how carefully it is drawn.\nModelling Partial Sketches: \u201cSketch\u201d being an inter-\nactive medium, is drawn sequentially in a stroke-by-stroke\nmanner. Moreover, due to its subjective nature, the same\nsketch might be perceived as partial or complete based on\nthe user\u2019s perception. Users can retrieve photos [8], cre-\nate [51] imaginative visual-art, or edit existing photos [22]+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587\n\nthrough repeated interactions with the AI agent. Therefore,\non-the-\ufb02y interaction with sketches requires sketch-based\nmodels to be capable of handling partial sketches. For in-\nstance, Sketch-RNN [17] can predict probable \ufb01nal sketch\nendings using a variational autoencoder trained on the vec-\ntor sketch coordinates. Furthermore, attempts have been\nmade to directly recognise partial sketches [28] and achieve\nsketch-to-photo generation [16] from incomplete sketch in-\nput, where both works involve a sketch-completion module\nbased on image-to-image translation. Recently, on-the-\ufb02y\nFG-SBIR [8] has been introduced to retrieve even from a\nfew elementary strokes as soon as the users start drawing.\nOverall, these works try to include random synthetic partial\nsketches during training to achieve their respective goals,\nbut here we aim to answer \u201cwhether a partial sketch has\nsuf\ufb01cient representative information/discriminative poten-\ntial to retrieve photos faithfully\u201d . Furthermore, we aim to\nquantify the instant at which a sequentially drawn sketch\nwould reach the optimum threshold point where it is repre-\nsentative enough for downstream tasks (e.g., retrieval). By\ndoing so, we can faithfully train models with suf\ufb01ciently\nrepresentative partial sketches instead of randomly drop-\nping strokes and ignoring instances where the synthetic par-\ntial sketch is too coarse to convey any meaning.\nReinforcement Learning in Vision: Reinforcement\nLearning (RL) [23] has been applied in different vision\nproblems [27, 52]. RL becomes handy when there exists a\nnon-differentiable way to quantify the goodness of the net-\nwork\u2019s state unlike differentiable loss function with hard-\nlabels. Instead, learning progresses via interactions [14, 19]\nwith the environment. Particularly in sketch community,\nRL has been leveraged for modelling sketch abstraction\n[32, 31], retrieval [8, 4], and designing competitive sketch-\ning agent [6]. Here, our objective is to engage an RL agent\nto get rid of noisy sketch strokes for better retrieval.\nLearning from noisy labels: Despite signi\ufb01cant\nprogress from the community-generated labelled data, ac-\ncurate labelling is challenging even for experienced do-\nmain experts [45]. Therefore, a separate topic of study\n[45, 61, 61] emerged, which aims at learning robust mod-\nels even from the noisy data distribution. While the exist-\ning works [18, 49] mainly consider having access to a large,\nnoisy dataset as well as a subset of carefully cleaned data for\nvalidation , our situation is even more dif\ufb01cult than usual.\nWe assume that every annotated sketch is not an absolutely\nperfect matching sketch of the paired photo. Therefore, we\naim to develop a noise-tolerant framework for FG-SBIR.\n3. Pilot Study: What\u2019s Wrong with FG-SBIR?\nBaseline FG-SBIR: Instead of complicated pre-training\n[36] or joint-training [4], we use a three branch state-\nof-the-art Siamese network [4] as our baseline retrieval\nmodel, which is considered to be a strong baseline tilldate. Each branch starts from ImageNet pre-trained VGG-\n16 [24], sharing equal weights. Given an input image\nI2RH\u0002W\u00023, we extract the convolutional feature-map\nF(I), which upon global average pooling followed by l2\nnormalisation generates a ddimensional feature embed-\nding. This model has been trained with an anchor sketch\n(a), a positive (p) photo, and a negative (n) photo triplets\nf\u0016a;\u0016p;\u0016ngusing triplet-loss [53]. Triplet-loss aims at in-\ncreasing the distance between anchor sketch and negative\nphoto\u000e\u0000=jjF(\u0016a)\u0000F(\u0016n)jj2, while simultaneously de-\ncreasing the same between anchor sketch and positive photo\n\u000e+=jjF(\u0016a)\u0000F(\u0016p)jj2. Therefore, the triplet-loss with\nmargin\u0016>0can be written as:\nLTriplet =maxf0;\u000e+\u0000\u000e\u0000+\u0016g (1)\nDual representation of sketch: Recent study has em-\nphasised on the dual representation [5] of sketch for self-\nsupervised feature learning. In rasterized pixel modality I,\nsketch can be represented as spatially extended image of\nsizeRH\u0002W\u00023. On the other side, in vector modality V, the\nsame sketch can be characterised by a sequence of strokes\n(s1;s2;\u0001\u0001\u0001;sK)where each stroke is a sequence of succes-\nsive pointssi= (vi\n1;vi\n2;\u0001\u0001\u0001;vi\nNi), and each point is rep-\nresented by an absolute 2D coordinate vi\nn= (xi\nn;yi\nn)in a\nH\u0002Wcanvas. Here, Kis number of strokes and Niis the\nnumber of points inside ithstroke. Individual strokes arise\ndue to pen up/down [17] movement. Although sketch vec-\ntors can easily be recorded through touch screen-devices,\ngeneration of the corresponding rasterized sketch image\nneeds a costly [55] rasterization operationR:V ! I .\nEither modality, raster or vector, has its own merits and de-\nmerits [5]. Apart from being more computationally ef\ufb01-\ncient [55] than raster domain, vector modality also contains\nthe stroke-by-stroke temporal information [17]. Nonethe-\nless, sketch vectors lack the spatial information [5] which\nis critical to model the \ufb01ne-grained details [4, 8]. Con-\nsequently, rasterized sketch image is the standard choice\n[36, 41, 40, 57] for FG-SBIR despite having a higher com-\nputational overhead and lacking temporal information.\nPreliminary analysis: The performance barrier due to ir-\nrelevant strokes gets noticed under on-the-\ufb02y FG-SBIR [8]\nsetup. Instead of only evaluating the complete sketch, we\nstart rendering at the end of every new kthstroke drawn\nas the rasterized sketch image SI\nk=R([s1;s2;\u0001\u0001\u0001;sk])\nwherek=f1;2;\u0001\u0001\u0001;Kg, and pass it through the pre-\ntrained baseline FG-SBIR model to get the feature repre-\nsentationF(SI\nk), followed by ranking the gallery images\nagainst it. We make these following observations on Shoe-\nV2 [57] dataset (Linear Limit) : (i) As the sketch proceeds\ntowards completion, the rank is supposed to be improved,\nhowever, we notice some unexpected dips in the perfor-\nmance in the later part of the drawing episode. This signi-\n\ufb01es that the later irrelevant strokes play a detrimental role,\nthereby degrading the retrieval performance (Fig. 1). (ii)+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587\n\nCompared to top@1(top@5) accuracy of 33:43%(67:81%)\non using complete sketch for retrieval, if we consider best\nrank achieved at any of the instant during the sketch draw-\ning episode as the retrieved result, top@1(top@5) accuracy\nextends to 42:54%(73:28%) . (iii) Further, we note that the\npercentage of instances where subsequently added strokes\ndrops the performance compared to the previous version SI\nk\nof the same sketch is 43:44%, which is a critical number.\nAblation on upper limit: Prior analysis unfolds the ne-\ncessity of dealing with irrelevant stroke, and we hypothe-\nsise that in many cases a subset of the strokes K0\u0014K\ncould better retrieve the paired photo by excluding the ir-\nrelevant ones . Different people follow varying stroke order\nfor sketching. Therefore, in order to simulate different pos-\nsible stroke orders and to estimate the upper limit that we\ncan achieve through the smart stroke-subset selector, we do\nthe following study. Given Kstrokes in a sketch, we form\n(2K\u00001)stroke subsets taking anynumber of strokes at a\ntime. Unlike the \u201con-the-\ufb02y\u201d [8] protocol, this setting does\nnot stick to a pre-recorded sequential order, rather it aims\nto \ufb01nd if there exists any subset that can retrieve the paired\nphoto better than the entire sketch set. Under this setting,\nwe achieve an exceptionally high top@1(top@5) accuracy\nof66:37%(88:31%) . However, evaluating with every pos-\nsible stroke combination during real-time inference is im-\npractical , and we do not have any explicit way to select\none \ufb01nal result. Therefore, in this work, we seek to build\nasmart stroke-subset selector as a pre-processing module\nwhich when plugged in before any pre-trained FG-SBIR\nmodel [57, 46], will aim to construct the most representative\nsubset to improve the overall accuracy.\n4. Noisy Stroke Tolerant FG-SBIR\nOverview: Our preliminary study motivates us to de-\nsign a stroke-subset selector to eliminate the noisy strokes\nfor FG-SBIR. While raster sketch image is essential [4] to\nmodel the \ufb01ne-grained correspondence, the stroke-level se-\nquential information is missing in raster modality. There-\nfore, taking advantage of the dual representation [5] of the\nsketch, we model the stroke subset selector on the sequen-\ntial vector space. In summary, our noise-tolerant FG-SBIR\nconsists of two following modules connected in cascade:\n(a)stroke-subset selector as pre-processing module work-\ning in vector space and (b) pretrained FG-SBIR Fthat uses\nrasterized version of predicted subset for \ufb01nal retrieval.\n4.1. Stroke Subset Selector\nModel: Given sketch-photo pair (S;P), the sketchScan\nbe represented as both raster image SIand stroke-level se-\nquential vector SV= (s1;s2;\u0001\u0001\u0001;sK). We design a stroke-\nsubset selectorX(\u0001)that takesSVas input, and aim to pre-\ndict an optimal subset SV=X(SV)withK0strokes where\nK0\u0014K. However, selecting the optimal subset of strokeis an ill-posed problem. Firstly, there is no explicit label\nwhich represents the optimal stroke subset. In fact, there\nmight be many sub-sets which can lead to successful re-\ntrieval. Furthermore, annotating the optimal stroke-subsets\nfor the whole training dataset via brute-force iteration is\ncomputationally impractical [6].\nIn our framework, we treat stroke subset selector as a\nbinary categorical classi\ufb01cation problem. In other words,\nfor a sketch of K strokes, we get an output of size RK\u00022,\nwhere every row is softmax normalised and it represents\na probability distribution p(aijsi)over two classes: a2\nfselect;ignoreg. However, we do not have any ex-\nplicit one-hot labels for this binary classi\ufb01cation task.\nTherefore, we let the stroke sub-set selector agent to inter-\nact with the pre-trained FG-SBIR model, and Xis learned\nusing a pre-trained FG-SBIR model Fas acritic which pro-\nvides the training signal to X.\nRasterization  Clipped Proximal\nPolicy GradientRewards\nFG-SBIRStroke Subset Selector\nLSTMLSTM\n   Residual Connection\nLocal LSTM Global LSTM\nFigure 2: Illustration of Noise Tolerant FG-SBIR frame-\nwork. Stroke Subset Selector X(\u0001)acts as a pre-processing\nmodule in the sketch vector space to eliminate the noisy\nstrokes. Selected stroke subset is then rasterized and fed\nthrough an existing pre-trained FG-SBIR model for reward\ncalculation, which is optimised by Proximal Policy Optimi-\nsation. For brevity, actor-only version is shown here.\nArchitecture: To design the architecture of stroke-level\nselector, we aim at preserving localised stroke-level infor-\nmation, as well as the compositional relationship [1] among\nthe strokes, which together conveys the overall semantic\nmeaning. Therefore, we employ a two-level hierarchical\nmodel comprising of a local stroke-embedding network ( E\u0012)\nand global relational network ( R\u0012) to enrich each stroke-\nlevel feature about the global semantics. In particular,\nwe feed individual stroke of size RNi\u00022havingNipoints\nthough a local stroke-embedding network E\u0012(e.g. RNN,\nLSTM or Transformer) whose weights of E\u0012are shared\nacross strokes. We take the \ufb01nal hidden-state feature as the\nlocalised representation fl\nsi2Rdsforithstroke. There-\nafter, feature representation of Ksuch strokes having size\nofRK\u0002dsare further fed to a global relational network ( R\u0012)\nwhose \ufb01nal hidden state fg2Rdscaptures the global se-\nmantic information of the whole sketch. Taking inspiration\nfrom residual learning [20], we fuse the global feature with+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587\n\nindividual stroke-level feature through a residual connec-\ntion with LayerNorm [3]. In concrete, every stroke feature\nenriched by global-local compositional hierarchy is repre-\nsented by ^fsi=LayerNorm (fl\nsi+fg)2Rd. We im-\nplement bothE\u0012andR\u0012through a one layer LSTM with\nhidden state size 128. Further, we apply a shared linear\nlayer (C\u0012) to getp(aijsi) =softmax (WX^fsi+bX), where\nWX2Rds\u00022andbX2R2. We group three modules\nfR\u0012;E\u0012;C\u0012gof stroke subset selector as X\u0012. See Fig. 2.\n4.2. Training Procedure\nNecessity of RL: Due to the unavailability of ground-\ntruth for optimum strokes, we rely on the pre-trained FG-\nSBIR model to learn the optimum stroke-subset selec-\ntion strategy. In particular, given probability distribution\np(aijsi)2R2forevery stroke overfselect;ignoreg,\nwe can sample from categorical distribution as ai\u0018\nCategorical ([p(aselectjsi);p(aignorejsi)]), and thereby\nwe will be getting a stroke subset as SVwithK0strokes,\nwhereK0\u0014K. In order to get the training signal from\npre-trained FG-SBIR model F, we need to feed the sub-\nset sketch through F. For that, we need to convert the se-\nquential sketch vector to raster sketch image through ras-\nterizationSI=R(SV), as \ufb01ne-grained SBIR model only\n[4, 8] works on raster image space. While subset sampling\ncould be relaxed by Gumbel-Softmax [21] operation for dif-\nferentiablity, non-differentiable rasterization operation R(\u0001)\nsqueeze us to use Policy-Gradient [48] from Reinforcement\nLearning (RL) literature [23].\nMDP Formulation: In particular, given an input sketch\nSV(initial state), the stroke-subset selector (X\u0012)acts a pol-\nicy network which takes action on selecting every stroke,\nand we get an updated state as subset-sketch SV(next\nstate). In order to train the policy network, we calculate\nreward usingFas a critic. Therefore, we can form the tu-\nple of four elements (initial state ,action ,reward ,\nnext state )that is typically required to train any RL\nmodel. In order to model the existence of multiple pos-\nsible successful subsets, we unroll this sequential Markov\nDecision Process (MDP) Ttimes starting from the com-\nplete sketch vector. In other words, for each sketch data, we\nsequentially sample the subset strokes Ttimes to learn the\nmulti-modal nature of true stroke subsets. Empirically we\nkeep episode length T= 5.\nReward Design: Our objective is to select the opti-\nmum set of stroke which can retrieve the paired photo\nwith minimum rank (e.g. best scenario: rank 1). In\nother words, pairwise-distance between the query sketch\nand paired photo embeddings should be lower than that of\nquery sketch and rest other photos of the gallery. As F\nis \ufb01xed, we can pre-compute the features of all Mgallery\nphotos asG2RM\u0002D\u2013 thus eliminating the burden of re-\npeatedly computing the photo features. During stroke sub-set selector training, we just need to calculate the feature\nembeddingF(SI)of rasterized version of predicted subset\nsketch, and we can calculate rank of paired photo using G\nand paired-photo index ef\ufb01ciently. We compute the reward\nboth in the ranking space as well as in the feature embed-\nding space using standard triplet loss on F(SI)following\nEqn. 1, which is found to give better stability and faster\ntraining convergence. In particular, we want to minimise\nthe rank of the paired photo and triplet loss simultaneously.\nFollowing the conventional norm of reward maximisation,\nwe de\ufb01ne the reward (R) as weighted summation of inverse\nof the rank and negative triplet loss as follows:\nR=!1\u00011\nrank+!2\u0001(\u0000LTriplet ) (2)\nActor Critic PPO: We make use of actor-critic version\nof Proximal Policy Optimisation (PPO) with clipped sur-\nrogate objective [42] to train our stroke-subset selector. In\nparticular, the very basic policy gradient [48] objective that\nis to be minimised could be written as:\nLPG(\u0012) =\u00001\nKKX\ni=1logp\u0012(aijsi)\u0001R (3)\nFor sampling ef\ufb01ciency, using the idea of Importance\nSampling [33], PPO maintains an older policy p0\n\u0012(aijsi),\nand thus Conservative Policy Iteration (CPI) objective be-\ncomesLCPI(\u0012) =\u00001\nKPK\ni=1ri(\u0012)\u0001R, whereri(\u0012) =\nlogp\u0012(aijsi)=logp0\n\u0012(aijsi). Further on, the clipped sur-\nrogate objective PPO can be written as LCLIP(\u0012) =\n\u00001\nKPK\ni=1clip (ri(\u0012);1\u0000\";1\u0000\")), which aims to pe-\nnalise too large policy update with hyperparameter \u000f= 0:2.\nWe take a minimum of the clipped and unclipped objective,\nso the \ufb01nal objective is a lower bound (i.e., a pessimistic\nbound) on the unclipped objective. The \ufb01nal actor only ver-\nsion PPO objective becomes:\nLA(\u0012) =\u00001\nKKX\ni=1min(LCPI;LCLIP) (4)\nTo reduce the variance, the actor-critic version of PPO\nmake use of a learned state-value function V(S)whereS\nis the sketch vector S= (s1;s2;\u0001\u0001\u0001;sK).V(S)shares pa-\nrameter with actor network X\u0012, where only the last linear\nlayer (C\u0012) is replaced by a new linear layer upon a single\nlatent vector (accumulated stroke-wise features by averag-\ning), predicting a scalar value that tries to approximate the\nreward value . Thus, the \ufb01nal loss function combines the\npolicy surrogate and value function error time together with\na entropy bonus ( En) to ensure suf\ufb01cient exploration is:\nLAC(\u0012) =\u00001\nKKX\ni=1(LA\u0000c1(V\u0012(S)\u0000R)2+c2En)(5)\nwhere,c1andc2are coef\ufb01cients. As we unroll the se-\nquential stroke-subset selection process for T= 5, for ev-\nery sample the loss accumulated over the MDP episode is\n1\nTPT\nt=1LAC\nt(\u0012).+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587\n\n5. Applications of Stroke-Subset Selector\nResistance against noisy strokes: Collected sketch la-\nbels, which are used to train the initial \ufb01ne-grained SBIR\nmodel are also noisy. The proposed stroke-subset selector\nnot only assists during inference by noisy-stroke elimina-\ntion, but also helps in cleaning training data , which in turn\ncan boost the performance to some extent. In particular,\nwe train the FG-SBIR model and Stroke-Subset Selector\nin stage-wise alternative manner, with the FG-SBIR model\nusing clean sketch labels produced by the trained stroke-\nsubset selector. Our method thus offers a plausible way to\nalleviate the latent/hidden noises of a FG-SBIR dataset [57].\nModelling ability to retrieve: As the critic network tries\nto approximate the scalar reward value which is a measure\nof retrieval performance, we can use the critic-network to\nquantify the retrieval ability at any instant of a sketching\nepisode. Higher scalar score from the critic signi\ufb01es bet-\nter retrieval ability. To wit, we ask the question whether a\npartial sketch is good enough for retrieval or not. Thus, in-\nstead of feeding rasterized partial sketch multiple times for\non-the-\ufb02y [8] retrieval, we can save signi\ufb01cant computation\ncost by feeding only after it gains a potential retrieval abil-\nity. Moreover, as both our actor and critic networks work in\nsketch vector modality, it adds less computational burden.\nOn-the-\ufb02y FG-SBIR: Training from Partial Sketches:\nState-of-the-art on-the-\ufb02y FG-SBIR [8] employs continu-\nous RL for training using ranking objective. A supervised\ntriplet-loss [59] based training, augmented with synthetic\npartial sketches obtained through random stroke-dropping\nis claimed to be sub-optimal, as randomly dropped strokes\nfrequently banish crucial details, resulting in the augmented\npartial sketch containing insuf\ufb01cient information to depict\nthe paired photo. In contrast, we use our stroke-subset se-\nlector to create several augmented partial versions of the\nsame sketch, each with suf\ufb01cient retrievability . While con-\ntinuous RL is time intensive to train and allegedly unstable\n[23], we can use simple triplet-loss based supervised learn-\ning with multiple meaningful augmented partial sketches .\n6. Experiments\nDatasets: Two publicly available FG-SBIR datasets\n[57, 34, 8] namely QMUL-Shoe-V2 and QMUL-Chair-V2\nare used in our experiments. Apart from having instance-\nwise paired sketch-photo, these datasets also contain the\nsketch coordinate information, and thus would enable us to\ntrain the stroke-subset selector using sketch vector modal-\nity. We use the standard training/testing split used by the ex-\nisting state-of-the-arts. In particular, out of 6;730(1;800)\nsketches and 2;000(400) photos from Shoe-V2 (Chair-V2)\ndataset, 6;051sketches ( 1;275) and 1;800(300) photos are\nused for training respectively, and the rest are for testing [8].\nImplementation: We have conducted all our experi-ments on an 11-GB Nvidia RTX 2080-Ti GPU with Py-\nTorch. For \ufb01ne-grained SBIR , we have used ImageNet [38]\npre-trained VGG-16 [44] backbone with feature embedding\ndimensiond= 512 . We train the FG-SBIR model using\nAdam optimiser [25] with a learning rate of 0.0001, batch\nsize 16, and margin value of 0.2 for triplet loss. For stroke\nsubset selector , we model local stroke embedding network\nand global relational network using one-layer LSTM with\nhidden state size 128for each. The critic network shares the\nsame weights with that of the actor, with only the last linear\nlayerC\u0012being replaced by a new one that predicts a single\nscalar value. We train it for 2000 epoch using Adam op-\ntimiser with initial learning rate 10\u00004till100epochs, then\nreducing to 10\u00005. We use a batch size of 16and keep an old\npolicy network for importance sampling [33] with episode\nlengthT= 5, and sampled instances are stored in a replay\nbuffer. We update the current policy network at every 20it-\neration using sampled instances from the replay buffer, and\nthe old policy network\u2019s weights are copied from the cur-\nrent one for subsequent sampling. We empirically set both\n!1,!2to1, and keepc1= 0:5,c2= 0:01,\u000f= 0:2.\nEvaluation Metric: (a) Standard FG-SBIR: Aligning\nto the existing state-of-the-art FG-SBIR frameworks [36,\n57], we use percentage of sketches having true-matched\nphoto in the top-1 (acc.@1) and top-5 (acc.@5) lists to as-\nsess the FG-SBIR performance. (b) On-the-\ufb02y FG-SBIR:\nFurthermore, to showcase the early retrieval performance\nfrom partial sketch, adhering to prior early-retrieval work\n[8] we employ two plots namely, (i) ranking-percentile and\n(ii)1\nrankvs.percentage of sketch . Higher area under these\ncurves indicate better early-retrieval potential. For the sake\nof simplicity, we call area under curves (i) and (ii) as r@A\nand r@B through the rest of the paper.\nCompetitors: To the best of our knowledge, no earlier\nworks have directly attempted to design a Noise-Tolerant\nFG-SBIR model in the SBIR literature. Therefore, we com-\npare with the existing standard FG-SBIR works appeared\nin the literature, as well as, we develop some self-designed\ncompetitive baselines under the assumption of \u2018all sketches\nare sketchy\u2019 \u2013 which explicitly intend to learn invariance\nagainst noisy strokes. (a) State-of-the-arts (SOTA): While\nTriplet-SN [57] uses Sketch-A-Net backbone along with\ntriplet loss, Triplet-Attn-HOLEF extends [57] with spatial\nattention and higher order ranking loss. Recent works in-\nclude: Jigsaw-Pretrain with self-supervised pre-training,\nTriplet-RL [8] employing RL-based \ufb01ne-tuning, Style-\nMeUP involving MAML training, Semi-Sup [4] incorpo-\nrating semi-supervised paradigm, and Cross-Hier [40] util-\nising cross-modal hierarchy with costly paired-embedding.\n(b)Self-designed Baselines (BL) : We create multiple ver-\nsion of the same sketch by randomly dropping strokes (en-\nsuring percentage of sketch vector length never drops be-\nlow 80%) or by synthetically adding random noisy stroke+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587\n\nTable 1: Results under Standard FG-SBIR setup.\nChair-V2 Shoe-V2\nAcc@1 Acc@5 Acc@1 Acc@5\nSOTATriplet-SN [57] 47.4% 71.4% 28.7% 63.5%\nTriplet-Attn-HOLEF [47] 50.7% 73.6% 31.2% 66.6%\nTriplet-RL [8] 51.2% 73.8% 30.8% 65.1%\nMixed-Jigsaw [34] 56.1% 75.3% 36.5% 68.9%\nSemi-Sup [4] 60.2% 78.1% 39.1% 69.9%\nStyleMeUp [41] 62.8% 79.6% 36.4% 68.1%\nCross-Hier [40] 62.4% 79.1% 36.2% 67.8%\nBL(B)aseline-Siamese 53.3% 74.3% 33.4% 67.8%\nAugmnt 54.1% 74.6% 33.9% 68.2%\nStyleMeUp+Augment 56.1% 76.9% 36.9% 69.9%\nContrastive+Augment 58.8% 77.1% 37.6% 70.1%\nLimitsUpper-Limit 78.6% 90.3% 66.3% 88.3%\nLinear-Limit 59.4% 77.3% 42.5% 73.2%\nProposed 64.8% 79.1% 43.7% 74.9%\npatches similar to [30]. Augment aims to learn the invari-\nance against noisy stroke by adding them inside training.\nThis is further advanced by StyleMeUp+Augment where\nsynthetic noisy/augmented sketches are mixed in the inner-\nloop of [41] to learn invariance by optimising outer-loop\non real sketches. Contrastive+Augment imposes an ad-\nditional contrastive loss [9] such that the distance between\ntwo augmented versions of same sketch should be lower\nthan that of with a random other sketch. Our pre-trained\nbaseline FG-SBIR model is termed as B-Siamese .\n6.1. Performance Analysis\nThe comparative analysis is shown in Table 1. Over-\nall, we observe a signi\ufb01cantly improved performance of our\nproposed Noise-Resistant \ufb01ne-grained SBIR employing a\nstroke-subset selector as a pre-processing neural agent com-\npared to the existing state-of-the-art. The early works tried\nto address different architectural modi\ufb01cations [46, 34], and\nlater on the \ufb01eld of \ufb01ne-grained SBIR witnessed successive\nimprovements through adaptation of different paradigms\nlikeself-supervised learning [36], meta-learning [41], semi-\nsupervised learning [4], etc. As opposed to these works,\nwe underpin an important phenomenon of noisy strokes,\nwhich is inherent to FG-SBIR. Most interestingly, our sim-\nple stroke-subset selector can improve the performance of\nbaseline B-Siamese model by an approximate margin of\n10:31% without any complicated joint-training of Semi-\nSup [4], costly hierarchical paired embedding of Cross-\nHier [40], or meta-learning cumbersome feature transfor-\nmation layer of StyleMeUp [41]. Furthermore, the per-\nformance of Augmnt baseline is slightly better than our\nbaseline pre-trained FG-SBIR as it learns some invariance\nfrom augmented/partial sketch. While we experienced dif-\n\ufb01culty in stable training for StyleMeUp+Augment ,Con-\ntrastive+Augment appears as a simple and straightforward\nway to learn the invariance against noisy strokes. Instead of\nmodelling invariance, we aim to eliminate the noisy strokes,\nthus giving a freedom of explainability through visualisa-\ntion. Despite using complicated architectures [40, 4], SOTAfails even to beat the accuracy of Linear-Limit (refer to sec-\ntion 3), while we can. Nevertheless, we suppress it by keep-\ning the simple baseline FG-SBIR untouched and prepend-\ning a simple stroke-selector agent \u2013 working on a cheaper\nvector modality for ef\ufb01cient deployment.\n6.2. Further Analysis and Insights\nAbility to retrieve/classify for partial sketches: The\nscalar value predicted by our learned state-value function\n(critic-network) [42] signi\ufb01es the retrieval ability of par-\ntial sketch with the notion of higher being the better. We\nhere train our model with a reward of1\nrankfor easy in-\nterpretability. Once the stroke-subset selector with actor-\ncritic version is trained, we feed the sketch to the critic net-\nwork (in vector space) at a progressive step of 5%comple-\ntion, and record the predicted scalar value at every instant.\nAt the same time, we rasterize every partial instance and\nfeed through pre-trained FG-SBIR to calculate the resul-\ntant ranking percentile of the paired photo. In Fig. 7, the\nhigh correlation demonstrates that the partial sketch with\na higher scalar score by the critic network tends to have a\nhigher average ranking percentile (ARP), while those with a\nlesser score result in lower ARP. Quantitatively, the top@5\naccuracy for partial sketches is 80:1%, which have a higher\npredicted scalar score than a threshold of1\n5. This validates\nthe potential of our critic network in quantifying if a par-\ntial sketch is suf\ufb01cient for retrieval. Suppose we repeat the\nsame with the negative of the classi\ufb01cation loss as a reward\nfor a pre-trained classi\ufb01cation network. In that case as well,\nwe observe a similar consistent behaviour for partial sketch\nclassi\ufb01cation, indicating our approach to be generic for var-\nious sketch-related downstream tasks. See \u00a7 supplementary.\nFigure 3: (a) Retrieval ability of partial sketch: correlation\nbetween critic network V(S) predicted score and ranking\npercentile (b) Performance at varying training data size with\nstroke-subset selector based data augmentation.\nData Augmentation: Our elementary study reveals that\nthere exists multiple possible subsets which can retrieve the\npaired photo faithfully. In particular, we use our policy net-\nwork to get stroke wise importance measure using p(aijsi)\ntowards the retrieval objectives. Through categorical sam-\npling ofp(aijsi), we can create multiple augmented ver-\nsions of the same sketch to increase the training data size.\nTo validate this, we compute the performance of baseline\nretrieval model at varying training data size with our sketch\naugmentation strategy in Fig. 7. While accuracy remains+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587\n\nmarginally better towards the high data regime, stroke sub-\nset selection based strategy excels the standard supervised\ncounter-part by a signi\ufb01cant margin, thus proving the ef\ufb01-\ncacy of our smart data-augmentation approach.\nOn-the-Fly Retrieval: Training a model with partial\nsketches generated by random stroke-dropping gives rise to\nnoisy gradient, and thus this naive baseline falls short com-\npared to RL-based \ufb01ne-tuning that consider the complete\nsketch drawing episode for training. In lieu of RL-based\n\ufb01ne-tuning [8], we train an on-the-\ufb02y retrieval model from\nmeaningful (holds ability to retrieve) partial sketches aug-\nmented through our critic network that have a higher scalar\nscore than1\n20. While training a continuous RL pipeline [8]\nis unstable and time-consuming, we achieve a competitive\non-the-\ufb02y r@A(r@B) performance of 85:78(21:1) with ba-\nsictriplet-loss based model trained with smartly augmented\npartial sketches compared to 85:38(21:24) as claimed in\n[8] on ShoeV2. From Fig. 4, we can see that at very early\nfew instances, RL-Based \ufb01ne-tuning [8] performs better,\nwhile ours achieve a signi\ufb01cantly better performance as the\ndrawing episode proceeds towards completion. While early\nsketch drawing episode is too coarse that hardly it can re-\ntrieve, through modelling the retrieval ability (with thresh-\nold of1\n10) of partial sketches, we can reduce the number\nof time we need to feed the rasterized sketch by 42:2%with\nvery little drop in performance (r@A(r@B): 85:07(20:98)).\nThus modelling partial sketches lead to signi\ufb01cant compu-\ntational edge under on-the-\ufb02y setting.\nFigure 4: Comparative results under on-the-\ufb02y setup (Shoe-\nV2), visualised through percentage of sketch. Higher area\nunder the plots indicates better early retrieval performance.\nResistance to Noisy Stroke: The signi\ufb01cance of stroke\nsubset selector is quantitatively shown in Table 1. While it\nvalidates our potential under inherent low-magnitude noise\nexisted in the dataset (shown in Fig. 5), we further aim to see\nhow our method works on extreme noisy situation. In par-\nticular, we augment the training sketches by synthetic noisy\npatches, and train our subset selector with a pre-trained re-\ntrieval model. During testing, we synthetically add noisy\nstrokes [30], and pass it through stroke-subset selector (pre-\nprocessing module) before feeding it to the retrieval model.\nWhile excluding the selector, the top@1 (top@5) drops to\n13:4%(44:9%) in presence of synthetic noises, our stroke\nsubset selector can improve them to 37:2%(68:2%) by elim-\ninating the synthetic noisy strokes (see Fig. 6).\n4\nPhoto Complete Subset\n6\n18\n31 1\n2 14 5\n14\n2 2\n13\n 7\n15\n2\n 1\nFigure 5: Examples showing selected subset performing\nbetter (rank in box) than complete sketch from ShoeV2.\nPhoto Synthetic\n Subset\n31\n351\n612\n218\n18\n17\n11 1\n213\n329\n474\n2\nFigure 6: Examples showing ability to perform (rank in\nbox) under synthetic noisy sketch input on ShoeV2.\nAblation on Design: (i) Instead of designing the stroke\nsubset selector through hierarchical LSTM, another straight\nforward way is to use one layer bidirectional LSTM, where\nevery coordinate point is being fed to each time step. How-\never, the top@1(top@5) lags behind by 4:9%(6:7%) than\nours, which veri\ufb01es the necessity of hierarchical modelling\nof sketch vectors to consider the compositional relation-\nship in our problem. Replacing LSTM by Transformer\nleads to no meaningful improvement in our case. (ii) Be-\ning a pre-processing step, we compare the extra time re-\nquired for selecting the optimal stroke set. In particular, it\nadds extra 22:4%multiply-add operations and 18:3%ex-\ntra CPU time compared standard baseline FG-SBIR. (iii)\nCompared to different RL methods [42], we get best results\nwith PPO actor-critic version with clipped surrogate objec-\ntive that beats its actor-only alternative by 1:7%top@1 ac-\ncuracy(ShoeV2). Importantly, training with critic network\nleads to one important byproduct of modelling retrieval\nability of partial sketches. (iv) Exploring different possi-\nble reward functions, we conclude that combining rewards\nfrom both ranking and feature embedding space through\ntriplet loss gives most optimum performance than ranking\nonly counterpart by extra 1:2%top@1 accuracy (ShoeV2).\nPlease refer to supplementary for more details.\n7. Conclusion\nIn this paper, we tackle the \u201cfear to sketch\u201d issue by\nproposing an intelligent stroke subset selector that automat-\nically selects the most representative stroke subset from the\nentire query stroke set. Our stroke subset selector can de-\ntect and eliminate irrelevant (noisy) strokes, thus boosting\nperformance of any off-the-shelf FG-SBIR framework. To\nthis end, we designed an RL-based framework, which learns\nto form an optimal stroke subset by interacting with a pre-\ntrained FG-SBIR model. We also show how the proposed\nselector can augment other sketch applications in a plug-\nand-play manner.+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587\n\nReferences\n[1] Emre Aksan, Thomas Deselaers, Andrea Tagliasacchi, and\nOtmar Hilliges. Cose: Compositional stroke embeddings. In\nNeurIPS , 2021. 4\n[2] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage,\nand Anil Anthony Bharath. Deep reinforcement learning: A\nbrief survey. IEEE Signal Processing Magazine , 2017. 11\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450 ,\n2016. 5\n[4] Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Aneeshan\nSain, Yongxin Yang, Tao Xiang, and Yi-Zhe Song. More\nphotos are all you need: Semi-supervised learning for \ufb01ne-\ngrained sketch based image retrieval. In CVPR , 2021. 1, 2,\n3, 4, 5, 6, 7, 11\n[5] Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Yongxin\nYang, Timothy M Hospedales, Tao Xiang, and Yi-Zhe Song.\nVectorization and rasterization: Self-supervised learning for\nsketch and handwriting. In CVPR , 2021. 2, 3, 4, 11\n[6] Ayan Kumar Bhunia, Ayan Das, Umar Riaz Muhammad,\nYongxin Yang, Timothy M Hospedales, Tao Xiang, Yulia\nGryaditskaya, and Yi-Zhe Song. Pixelor: A competitive\nsketching ai agent. so you think you can sketch? ACM-TOG ,\n2020. 3, 4\n[7] Ayan Kumar Bhunia, Viswanatha Reddy Gajjala, Subhadeep\nKoley, Rohit Kundu, Aneeshan Sain, Tao Xiang, and Yi-\nZhe Song. Doodle it yourself: Class incremental learning\nby drawing a few sketches. In CVPR , 2022. 2\n[8] Ayan Kumar Bhunia, Yongxin Yang, Timothy M\nHospedales, Tao Xiang, and Yi-Zhe Song. Sketch less\nfor more: On-the-\ufb02y \ufb01ne-grained sketch-based image\nretrieval. In CVPR , 2020. 1, 2, 3, 4, 5, 6, 7, 8, 11\n[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In ICML , 2020. 7\n[10] Wengling Chen and James Hays. Sketchygan: Towards di-\nverse and realistic sketch to image synthesis. In CVPR , 2018.\n2\n[11] Pinaki Nath Chowdhury, Ayan Kumar Bhunia,\nViswanatha Reddy Gajjala, Aneeshan Sain, Tao Xiang,\nand Yi-Zhe Song. Partially does it: Towards scene-level\nfg-sbir with partial input. In CVPR , 2022. 1\n[12] John Collomosse, Tu Bui, and Hailin Jin. Livesketch: Query\nperturbations for guided sketch-based visual search. In\nCVPR , 2019. 1, 2\n[13] Sounak Dey, Pau Riba, Anjan Dutta, Josep Llados, and Yi-\nZhe Song. Doodle to search: Practical zero-shot sketch-\nbased image retrieval. In CVPR , 2019. 1, 2\n[14] Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Nghia\nNguyen, Eric Patterson, Tien D Bui, and Ngan Le. Auto-\nmatic face aging in videos via deep reinforcement learning.\nInCVPR , 2019. 3\n[15] Anjan Dutta and Zeynep Akata. Semantically tied paired\ncycle consistency for zero-shot sketch-based image retrieval.\nInCVPR , 2019. 1, 2\n[16] Arnab Ghosh, Richard Zhang, Puneet K Dokania, Oliver\nWang, Alexei A Efros, Philip HS Torr, and Eli Shechtman.Interactive sketch & \ufb01ll: Multiclass sketch-to-image transla-\ntion. In ICCV , 2019. 3\n[17] David Ha and Douglas Eck. A neural representation of\nsketch drawings. In ICLR , 2018. 3\n[18] Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor\nTsang, Ya Zhang, and Masashi Sugiyama. Masking: A new\nperspective of noisy supervision. In NeurIPS , 2018. 3\n[19] Xiaoguang Han, Zhaoxuan Zhang, Dong Du, Mingdai Yang,\nJingming Yu, Pan Pan, Xin Yang, Ligang Liu, Zixiang\nXiong, and Shuguang Cui. Deep reinforcement learning\nof volume-guided progressive view inpainting for 3d point\nscene completion from a single depth image. In CVPR , 2019.\n3\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR ,\n2016. 4\n[21] Eric Jang, Shixiang Gu, and Ben Poole. Categorical repa-\nrameterization with gumbel-softmax. In ICLR , 2017. 5\n[22] Youngjoo Jo and Jongyoul Park. Sc-fegan: Face editing gen-\nerative adversarial network with user\u2019s sketch and color. In\nICCV , 2019. 2\n[23] Leslie Pack Kaelbling, Michael L Littman, and Andrew W\nMoore. Reinforcement learning: A survey. JAIR , 1996. 3, 5,\n6\n[24] Andrew Zisserman Karen Simonyan. Very deep convolu-\ntional networks for large-scale image recognition. In ICLR ,\n2015. 3\n[25] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR , 2014. 6\n[26] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xi-\naodong He. Stacked cross attention for image-text matching.\nInECCV , 2018. 1\n[27] Xiaodan Liang, Lisa Lee, and Eric P Xing. Deep variation-\nstructured reinforcement learning for visual relationship and\nattribute detection. In CVPR , 2017. 3\n[28] Fang Liu, Xiaoming Deng, Yu-Kun Lai, Yong-Jin Liu,\nCuixia Ma, and Hongan Wang. Sketchgan: Joint sketch com-\npletion and recognition with generative adversarial network.\nInCVPR , 2019. 3\n[29] Li Liu, Fumin Shen, Yuming Shen, Xianglong Liu, and Ling\nShao. Deep sketch hashing: Fast free-hand sketch-based im-\nage retrieval. In CVPR , 2017. 2\n[30] Runtao Liu, Qian Yu, and Stella X Yu. Unsupervised sketch\nto photo synthesis. In ECCV , 2020. 7, 8\n[31] Umar Riaz Muhammad, Yongxin Yang, Timothy M\nHospedales, Tao Xiang, and Yi-Zhe Song. Goal-driven se-\nquential data abstraction. In ICCV , 2019. 3\n[32] Umar Riaz Muhammad, Yongxin Yang, Yi-Zhe Song, Tao\nXiang, and Timothy M Hospedales. Learning deep sketch\nabstraction. In CVPR , 2018. 3\n[33] Radford M Neal. Annealed importance sampling. Statistics\nand Computing , 2001. 5, 6\n[34] Kaiyue Pang, Ke Li, Yongxin Yang, Honggang Zhang, Tim-\nothy M Hospedales, Tao Xiang, and Yi-Zhe Song. Gener-\nalising \ufb01ne-grained sketch-based image retrieval. In CVPR ,\n2019. 1, 2, 6, 7+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587\n\n[35] Kaiyue Pang, Yi-Zhe Song, Tony Xiang, and Timothy M\nHospedales. Cross-domain generative learning for \ufb01ne-\ngrained sketch-based image retrieval. In BMVC , 2017. 2\n[36] Kaiyue Pang, Yongxin Yang, Timothy M Hospedales, Tao\nXiang, and Yi-Zhe Song. Solving mixed-modal jigsaw puz-\nzle for \ufb01ne-grained sketch-based image retrieval. In CVPR ,\n2020. 3, 6, 7\n[37] Leo Sampaio Ferraz Ribeiro, Tu Bui, John Collomosse, and\nMoacir Ponti. Sketchformer: Transformer-based representa-\ntion for sketched structure. In CVPR , 2020. 1, 2\n[38] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. IJCV , 2015. 6\n[39] Aneeshan Sain, Ayan Kumar Bhunia, Vaishnav Potlapalli,\nPinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song.\nSketch3t: Test-time training for zero-shot sbir. In CVPR ,\n2022. 1\n[40] Aneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xi-\nang, and Yi-Zhe Song. Cross-modal hierarchical modelling\nfor \ufb01ne-grained sketch based image retrieval. In BMVC ,\n2020. 2, 3, 6, 7\n[41] Aneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xi-\nang, and Yi-Zhe Song. Stylemeup: Towards style-agnostic\nsketch-based image retrieval. In CVPR , 2021. 2, 3, 7\n[42] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\nford, and Oleg Klimov. Proximal policy optimization algo-\nrithms. arXiv preprint arXiv:1707.06347 , 2017. 5, 7, 8, 11\n[43] Yuming Shen, Li Liu, Fumin Shen, and Ling Shao. Zero-shot\nsketch-image hashing. In CVPR , 2018. 1, 2\n[44] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556 , 2014. 6\n[45] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin,\nand Jae-Gil Lee. Learning from noisy labels with deep neural\nnetworks: A survey. arXiv preprint arXiv:2007.08199 , 2021.\n3\n[46] Jifei Song, Yi-Zhe Song, Tony Xiang, and Timothy M\nHospedales. Fine-grained image retrieval: the text/sketch\ninput dilemma. In BMVC , 2017. 2, 4, 7\n[47] Jifei Song, Qian Yu, Yi-Zhe Song, Tao Xiang, and Timo-\nthy M Hospedales. Deep spatial-semantic attention for \ufb01ne-\ngrained sketch-based image retrieval. In CVPR , 2017. 1, 2,\n7, 13\n[48] Richard S Sutton, David A McAllester, Satinder P Singh, and\nYishay Mansour. Policy gradient methods for reinforcement\nlearning with function approximation. In NeurIPS , 2000. 5\n[49] Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan,\nDaniel C Alexander, and Nathan Silberman. Learning from\nnoisy labels by regularized estimation of annotator confu-\nsion. In CVPR , 2019. 3\n[50] Giorgos Tolias and Ondrej Chum. Asymmetric feature maps\nwith application to sketch based retrieval. In CVPR , 2017. 2\n[51] Sheng-Yu Wang, David Bau, and Jun-Yan Zhu. Sketch your\nown gan. In ICCV , 2021. 2\n[52] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao,\nDinghan Shen, Yuan-Fang Wang, William Yang Wang, andLei Zhang. Reinforced cross-modal matching and self-\nsupervised imitation learning for vision-language navigation.\nInCVPR , 2019. 3\n[53] Kilian Q Weinberger and Lawrence K Saul. Distance met-\nric learning for large margin nearest neighbor classi\ufb01cation.\nJMLR , 2009. 3\n[54] Peng Xu, Yongye Huang, Tongtong Yuan, Kaiyue Pang, Yi-\nZhe Song, Tao Xiang, Timothy M Hospedales, Zhanyu Ma,\nand Jun Guo. Sketchmate: Deep hashing for million-scale\nhuman sketch retrieval. In CVPR , 2018. 2\n[55] Peng Xu, Chaitanya K Joshi, and Xavier Bresson. Multi-\ngraph transformer for free-hand sketch recognition. IEEE\nT-NNLS , 2021. 3\n[56] Sasi Kiran Yelamarthi, Shiva Krishna Reddy, Ashish Mishra,\nand Anurag Mittal. A zero-shot framework for sketch based\nimage retrieval. In ECCV , 2018. 2\n[57] Qian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M\nHospedales, and Chen-Change Loy. Sketch me that shoe. In\nCVPR , 2016. 1, 2, 3, 4, 6, 7\n[58] Qian Yu, Jifei Song, Yi-Zhe Song, Tao Xiang, and Timo-\nthy M Hospedales. Fine-grained instance-level sketch-based\nimage retrieval. IJCV , 2021. 2\n[59] Qian Yu, Yongxin Yang, Feng Liu, Yi-Zhe Song, Tao Xiang,\nand Timothy M Hospedales. Sketch-a-net: A deep neural\nnetwork that beats humans. IJCV , 2017. 2, 6\n[60] Jingyi Zhang, Fumin Shen, Li Liu, Fan Zhu, Mengyang Yu,\nLing Shao, Heng Tao Shen, and Luc Van Gool. Generative\ndomain-migration hashing for sketch-to-image retrieval. In\nECCV , 2018. 1, 2\n[61] Tianyi Zhou, Shengjie Wang, and Jeff Bilmes. Robust cur-\nriculum learning: from clean label detection to noisy label\nself-correction. In ICLR , 2021. 3+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587\n\nSupplementary material for\nSketching without Worrying: Noise-Tolerant Sketch-Based\nImage Retrieval\nAyan Kumar Bhunia1Subhadeep Koley1,2Abdullah Faiz Ur Rahman KhiljiInterned with SketchX\nAneeshan Sain1,2Pinaki nath Chowdhury1,2Tao Xiang1,2Yi-Zhe Song1,2\n1SketchX, CVSSP, University of Surrey, United Kingdom.\n2iFlyTek-Surrey Joint Research Centre on Arti\ufb01cial Intelligence.\nfa.bhunia, s.koley, a.sain, p.chowdhury, t.xiang, y.song g@surrey.ac.uk\n8. Comparative Study with different RL methods\nWe compare with different RL methods [42, 2], starting from Vanilla Policy Gradient, Deep Q-Learning, TRPO, to variants\nof PPO. For our use-case we get best results (Table 2) with PPO actor-critic version with clipped surrogate objective, where\nthe critic network leads to one important byproduct of modelling retrieval ability of partial sketches.\nTable 2: Performance analysis using different RL approaches.\nChair-V2 Shoe-V2\nAcc@1 Acc@5 Acc@1 Acc@5\nVanilla Policy Gradient 61.4 % 78.6% 40.1% 71.9%\nDeep Q-Learning 61.9% 78.9% 40.7% 71.8%\nTRPO 60.8% 78.2% 39.8% 70.2%\nPPO Actor-Only KL 62.8% 78.5% 42.3% 72.8%\nPPO Actor-Only Clipped 63.9% 78.9% 43.1% 74.5%\nPPO Actor-Critic KL 63.8% 78.7% 42.1% 73.7%\nPPO Actor-Critic Clipped 64.8% 79.1% 43.7% 74.9%\n9. Comparative Study with different reward functions\nWe conducted experiments with different possible reward designs as shown in Table 3. Empirically, we found that com-\nbining rewards from both ranking and feature embedding space through triplet loss offers most optimum performance.\nTable 3: Performance analysis using different reward designs.\nChair-V2 Shoe-V2\nRewards Acc@1 Acc@5 Acc@1 Acc@5\n-rank 63.5 % 78.6% 42.6% 73.7%\n1\nrank64.2% 78.8% 43.2% 74.2%\n-Ltriplet 62.4% 78.1% 41.6% 72.7%\n1\nLtriplet +\u000f60.2% 77.3% 38.8% 68.6%\n1\nrank\u0000Ltriplet 64.8% 79.1% 43.7% 74.9%\n10. Classi\ufb01cation Ability and Data Augmentation for sketch classi\ufb01cation\nSimilar to \ufb01ne-grained retrieval [8, 4], we extend our RL-based stroke-subset selector framework for classi\ufb01cation task to\njudge if the critic network could be used to judge the recognition potential from partial sketch. To this end, we use negative of\ncross-entropy loss as the reward to train the stroke-subset selector under a pre-trained sketch classi\ufb01cation network (Resnet50)\non TU-Berlin dataset [5]. We obtain a similar correlation between critic network predicted score and classi\ufb01cation accuracy\nas shown in Fig. 7. In brief, the samples having higher scalar score predicted by the critic network tends to have a higher\nclassi\ufb01cation accuracy, thus proving the ef\ufb01ciency of modelling recognition ability of sketches through our framework.\nSimilarly, one can use the stroke-subset selector (policy network) to augment the sketches for classi\ufb01cation problem.\nPerformance at varying training data regime is shown in Fig. 7 on TU-Berlin dataset.\n11. Motivation on removing \u201cfear\u201d for sketching\nBy removing \u201cfear\u201d, we meant injecting that extra con\ufb01dence to the users, knowing that even if they can not sketch well,\nthe system will still be able to return favourable results.+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587\n\n-0.10-0.08-0.06-0.04-0.020\nCritic Scalar Score40%50%60%70%80%90%100%Classification Accuracy TU-Berlin\n11020 50 100\nTraining Data Size (%)30%45%60%75%Accuracy\nAug TU-Berlin\nNo-Aug TU-BerlinFigure 7: (a) Retrieval ability of partial sketch: correlation between critic network V(S) predicted score and ranking percentile\n(b) Performance at varying training data size with stroke-subset selector based data augmentation.\n12. What happens with extreme cases?\nThe extreme case of completely random junk can be handled by our critic network, which will assign a low retrieval ability\nscore, helping us to sidestep such unusable instances. On the other hand, critic network assigns progressively higher score\nfor sketches from professional artists, and achieves retrieval threshold much earlier. Fig. 8 offers examples of how the critic\nscore changes for a good/professional sketch (top) and a complete random one (bottom).\nRandom  \n SketchGood  \nSketch\nFigure 8: Critic score at progressive sketch drawing episode.\n13. Clarity of binary stroke selection scheme\nIn our framework, we have modelled the stroke selection through categorical distribution (softmax normalisation over\nR2). However, as the reviewer suggested, one could model using Bernoulli distribution where the stroke selector would\npredict a single sigmoid normalised scalar value R1. We tried both approaches and empirically found the use of categorical\ndistribution to be more stable with faster convergence and quantitatively better results (by 1:41% Acc@1 on ShoeV2). We\nwill speci\ufb01cally mention this in the supplementary with a thorough ablative study upon acceptance.\n14. Training-time comparison with baselines\nFor the baselines, we do not augment the sketches using all possible stroke-subset combinations (with cost O(2N)).\nTaking into account all possible stroke subsets not only slows down the training data-pipeline, but many of these augmented\nsketch subsets are too coarse/incomplete to convey any useful information about the paired photo. Some initial experiments\nindicated model collapse due to noisy gradients raised from such overly coarse/incomplete sketch-subsets. Therefore, in\norder to eliminate noisy gradients in the baselines, we drop strokes at random while ensuring that the percentage of sketch\nvector length never falls below a certain threshold \u2013 80% was empirically found to yield optimum performance.\nTo ensure fair comparisons, we also keep each model training until we \ufb01nd no further improvement in both the loss value\nand accuracy on the validation set for consecutive 20K iterations. Furthermore, under our experimental setup, the training\ntime for all baselines as well as our method lies between 12-14 hours, ensuring a largely uniform training time for all.+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587\n\n15. Clarity on Training dataset\nWe use 6051+1800 images to train both the retrieval model and stroke-subset selector. In particular, \ufb01rst, we pre-train the\nretrieval model on raster sketches. Next, we use the sketch-vector modality of the same set of sketches to train the stroke-\nsubset selector. It should be noted that while the retrieval model trained from raster sketches is unaware of stroke-speci\ufb01c\nimportance for retrieval, the stroke-subset selector intelligently manages to eliminate the noise/inconsistent sketch strokes.\nTesting is done on the remaining 679+200 images which are never used in either stage of the training.\n16. Comparison with soft-attention\nIn order to deal with partial sketches, one alternative is indeed to apply soft-spatial attention in raster-space, as used\nin Triplet-Attn-HOLEF [47]. Through fusing Triplet-Attn-HOLEF with our Augment baseline, we devise a new baseline\nTriplet-Attn-HOLEF+Augment, which is able to achieve Acc@1(Acc@5) of 34:6%(68:9%) on the ShoeV2 dataset. This is\nslightly better than the Augment baseline but signi\ufb01cantly falls behind our \ufb01nal results. This further veri\ufb01es the necessity of\nour stroke-subset selector to deal with the erroneous/noisy strokes that are inherent to the drawing process.\n17. Limitations\nCross-dataset generalisation for the stroke-subset selector in particular is an intriguing research direction, which we intend\nto cover in the future. Also, replacing the non-differentiable rasterization operation (sketch-vector to sketch-image) with a\ndifferentiable approximated one would be an interesting direction to explore too. This would make the whole pipeline end-\nto-end differentiable, so it can backpropagate the gradient calculated from triplet loss directly onto the stroke-subset selector\nwithout needing any RL-based formulation, ultimately increasing stability and pace of training.+v:mala2255\u83b7\u53d6\u66f4\u591a\u8bba\u6587\n", "meta_data": {"source": "Sketching without Worrying Noise-Tolerant Sketch-Based Image Retrieval.pdf"}}a negative 